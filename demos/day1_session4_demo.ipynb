{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Day 1, Session 4: End-to-End Invoice Processing System\n\n## Putting It All Together - Production Architecture\n\n### The Journey So Far\n\nWe've built the foundation pieces:\n- **Session 1**: HuggingFace pipelines for AI model integration\n- **Session 2**: ReAct agents for intelligent reasoning\n- **Session 3**: LangGraph workflows for complex orchestration\n\nNow we combine everything into a **production-ready system**.\n\n### System Architecture Overview\n\n```\n┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n│   Document  │ → │     AI      │ → │   Business  │\n│  Ingestion  │    │ Extraction  │    │    Rules    │\n└─────────────┘    └─────────────┘    └─────────────┘\n       │                   │                   │\n┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n│    OCR      │    │     NER     │    │ Validation  │\n│   Layout    │    │ QA Models   │    │   Engine    │\n│ Recognition │    │   Amounts   │    │ Thresholds  │\n└─────────────┘    └─────────────┘    └─────────────┘\n                            │\n                    ┌─────────────┐\n                    │   Decision  │\n                    │   Engine    │\n                    └─────────────┘\n```\n\n### Production vs Demo Systems\n\n**Demo System:**\n```python\n# Simple: Process one document\nresult = process_invoice(\"invoice.pdf\")\nif result.valid:\n    approve()\n```\n\n**Production System:**\n```python\n# Complex: Handle scale, errors, monitoring\nasync def process_batch(documents):\n    results = []\n    async with ProcessingCluster() as cluster:\n        for batch in chunk_documents(documents, size=100):\n            batch_results = await cluster.process_parallel(\n                batch,\n                retry_policy=exponential_backoff(),\n                circuit_breaker=external_services,\n                audit_logger=audit_trail\n            )\n            results.extend(batch_results)\n    return results\n```\n\n### Why This Matters for Business\n\n**Traditional Manual Processing:**\n- 15-30 minutes per invoice\n- 2-5% error rate\n- No audit trail\n- Cannot scale with volume\n\n**AI-Powered System:**\n- 3-5 seconds per invoice\n- <0.1% error rate\n- Complete audit trail\n- Scales horizontally\n\n**ROI Calculation:**\n```\nManual Cost: 1000 invoices × 20 minutes × $30/hour = $10,000/month\nAI System Cost: $500/month (infrastructure) + $200/month (processing)\nMonthly Savings: $9,300 (93% cost reduction)\n```\n\nLet's build this system!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "!pip install -q transformers torch pillow pytesseract pdf2image\n",
    "!pip install -q langgraph langchain langchain-community\n",
    "!apt-get install -qq tesseract-ocr poppler-utils\n",
    "\n",
    "# Configuration\n",
    "OLLAMA_URL = \"http://XX.XX.XX.XX\"  # Course server\n",
    "API_TOKEN = \"YOUR_TOKEN_HERE\"\n",
    "MODEL = \"qwen3:8b\"\n",
    "\n",
    "# For demo, we'll use local test images\n",
    "INVOICE_IMAGE_PATH = \"../images/invoices_1.png\"  # Generated earlier\n",
    "RECEIPT_IMAGE_PATH = \"../images/receipts_1.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Document Ingestion Layer - Handle the Real World\n\n### The Challenge of Document Variety\n\nProduction systems must handle diverse document formats:\n\n**Input Formats:**\n```python\n# Different file types\nformats = [\n    \"PDF documents (scanned and native)\",\n    \"Image files (PNG, JPEG, TIFF)\",\n    \"Email attachments with mixed content\",\n    \"Mobile phone photos of receipts\",\n    \"Faxed documents (poor quality)\",\n    \"Multi-page documents with tables\"\n]\n```\n\n**Quality Variations:**\n```python\n# Real-world quality issues\nquality_challenges = {\n    \"resolution\": \"72 DPI to 600 DPI scanned documents\",\n    \"skew\": \"Rotated or tilted documents\",\n    \"noise\": \"Background patterns, watermarks\",\n    \"lighting\": \"Shadows, reflections from phone photos\",\n    \"format_quality\": \"Compressed JPEGs with artifacts\"\n}\n```\n\n### OCR Engine Architecture\n\n**Multi-Engine Approach:**\n```python\n# Production systems use multiple OCR engines\nclass ProductionOCR:\n    def __init__(self):\n        self.engines = {\n            \"tesseract\": TesseractEngine(),      # Open source, good general purpose\n            \"cloud_vision\": GoogleVisionAPI(),   # Excellent for handwriting\n            \"azure_read\": AzureReadAPI(),        # Great for layout detection\n            \"aws_textract\": AWSTextractAPI()     # Best for forms and tables\n        }\n    \n    def extract_with_confidence(self, image):\n        results = []\n        for engine_name, engine in self.engines.items():\n            try:\n                result = engine.extract(image)\n                results.append({\n                    \"engine\": engine_name,\n                    \"text\": result.text,\n                    \"confidence\": result.confidence,\n                    \"layout\": result.layout_data\n                })\n            except Exception as e:\n                logger.warning(f\"Engine {engine_name} failed: {e}\")\n        \n        # Choose best result based on confidence\n        return max(results, key=lambda x: x[\"confidence\"])\n```\n\n### Document Preprocessing Pipeline\n\n**Image Enhancement:**\n```python\ndef preprocess_document(image):\n    \"\"\"Enhance image quality before OCR\"\"\"\n    \n    # 1. Deskew detection and correction\n    angle = detect_skew_angle(image)\n    if abs(angle) > 0.5:\n        image = rotate_image(image, -angle)\n    \n    # 2. Noise reduction\n    image = remove_noise(image, method=\"bilateral_filter\")\n    \n    # 3. Contrast enhancement\n    image = enhance_contrast(image, method=\"CLAHE\")\n    \n    # 4. Binarization for better OCR\n    if is_low_contrast(image):\n        image = adaptive_threshold(image)\n    \n    return image\n```\n\n**Layout Analysis:**\n```python\ndef analyze_document_layout(image):\n    \"\"\"Detect document structure before extraction\"\"\"\n    \n    layout = {\n        \"regions\": [],\n        \"text_blocks\": [],\n        \"tables\": [],\n        \"headers\": []\n    }\n    \n    # Detect text regions\n    text_regions = detect_text_regions(image)\n    \n    # Classify regions (header, body, table, footer)\n    for region in text_regions:\n        region_type = classify_region(region, image)\n        layout[\"regions\"].append({\n            \"bbox\": region.bbox,\n            \"type\": region_type,\n            \"confidence\": region.confidence\n        })\n    \n    return layout\n```\n\nLet's implement a robust ingestion system:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import os\n",
    "from typing import Union, List, Dict, Any\n",
    "\n",
    "class DocumentIngestion:\n",
    "    \"\"\"Handle various document formats and extract content\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_image(path: str) -> Image.Image:\n",
    "        \"\"\"Load image from file path\"\"\"\n",
    "        return Image.open(path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_pdf(path: str) -> List[Image.Image]:\n",
    "        \"\"\"Convert PDF to images\"\"\"\n",
    "        return convert_from_path(path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_ocr(image: Image.Image) -> str:\n",
    "        \"\"\"Extract text using OCR\"\"\"\n",
    "        return pytesseract.image_to_string(image)\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_layout_data(image: Image.Image) -> Dict:\n",
    "        \"\"\"Extract layout information (bounding boxes, confidence)\"\"\"\n",
    "        data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
    "        \n",
    "        # Group words into lines and blocks\n",
    "        layout = {\n",
    "            \"lines\": [],\n",
    "            \"confidence\": []\n",
    "        }\n",
    "        \n",
    "        current_line = []\n",
    "        last_top = 0\n",
    "        \n",
    "        for i, word in enumerate(data['text']):\n",
    "            if word.strip():\n",
    "                top = data['top'][i]\n",
    "                if abs(top - last_top) > 10 and current_line:\n",
    "                    layout[\"lines\"].append(' '.join(current_line))\n",
    "                    current_line = []\n",
    "                current_line.append(word)\n",
    "                layout[\"confidence\"].append(data['conf'][i])\n",
    "                last_top = top\n",
    "        \n",
    "        if current_line:\n",
    "            layout[\"lines\"].append(' '.join(current_line))\n",
    "        \n",
    "        return layout\n",
    "    \n",
    "    @classmethod\n",
    "    def process_document(cls, path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main entry point for document processing\"\"\"\n",
    "        result = {\n",
    "            \"path\": path,\n",
    "            \"type\": path.split('.')[-1].lower(),\n",
    "            \"text\": \"\",\n",
    "            \"layout\": {},\n",
    "            \"metadata\": {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            if result[\"type\"] == \"pdf\":\n",
    "                images = cls.load_pdf(path)\n",
    "                result[\"text\"] = \"\\n\\n\".join([cls.extract_text_ocr(img) for img in images])\n",
    "                result[\"metadata\"][\"pages\"] = len(images)\n",
    "            else:\n",
    "                image = cls.load_image(path)\n",
    "                result[\"text\"] = cls.extract_text_ocr(image)\n",
    "                result[\"layout\"] = cls.extract_layout_data(image)\n",
    "                result[\"metadata\"][\"dimensions\"] = image.size\n",
    "            \n",
    "            result[\"metadata\"][\"text_length\"] = len(result[\"text\"])\n",
    "            result[\"status\"] = \"success\"\n",
    "        except Exception as e:\n",
    "            result[\"status\"] = \"error\"\n",
    "            result[\"error\"] = str(e)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Test ingestion\n",
    "print(\"Testing Document Ingestion...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a test invoice image (simulate)\n",
    "test_image = Image.new('RGB', (800, 600), color='white')\n",
    "test_image.save('/tmp/test_invoice.png')\n",
    "\n",
    "ingestion = DocumentIngestion()\n",
    "test_result = ingestion.process_document('/tmp/test_invoice.png')\n",
    "\n",
    "print(f\"Document Type: {test_result['type']}\")\n",
    "print(f\"Status: {test_result['status']}\")\n",
    "print(f\"Metadata: {test_result['metadata']}\")\n",
    "print(\"✅ Ingestion layer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: AI Extraction Layer - Intelligence at Scale\n\n### Modern AI Pipeline Architecture\n\nProduction AI extraction uses specialized models for different tasks:\n\n**Model Selection Strategy:**\n```python\n# Different models for different tasks\nai_pipeline = {\n    \"layout_detection\": \"microsoft/layoutlm-base-uncased\",\n    \"entity_extraction\": \"dbmdz/bert-large-cased-finetuned-conll03-english\", \n    \"question_answering\": \"deepset/roberta-base-squad2\",\n    \"document_classification\": \"microsoft/DialoGPT-medium\",\n    \"amount_detection\": \"custom_regex_enhanced_model\",\n    \"date_parsing\": \"spacy_ner + custom_patterns\"\n}\n```\n\n**Performance vs Accuracy Trade-offs:**\n```python\n# Model size and speed considerations\nmodel_comparison = {\n    \"bert-base\": {\n        \"size\": \"110M parameters\",\n        \"inference_time\": \"50ms\",\n        \"accuracy\": \"95%\",\n        \"use_case\": \"Production real-time\"\n    },\n    \"bert-large\": {\n        \"size\": \"340M parameters\", \n        \"inference_time\": \"150ms\",\n        \"accuracy\": \"97%\",\n        \"use_case\": \"Batch processing\"\n    },\n    \"custom_distilled\": {\n        \"size\": \"66M parameters\",\n        \"inference_time\": \"30ms\", \n        \"accuracy\": \"93%\",\n        \"use_case\": \"Mobile/edge deployment\"\n    }\n}\n```\n\n### Advanced Extraction Techniques\n\n**Multi-Modal Processing:**\n```python\n# Combining text and visual information\ndef multimodal_extraction(image, text):\n    \"\"\"\n    Use both visual layout and text content for better extraction\n    \"\"\"\n    \n    # Visual features: table detection, logo recognition\n    visual_features = extract_visual_features(image)\n    \n    # Text features: NER, patterns, context\n    text_features = extract_text_features(text)\n    \n    # Combine both for enhanced accuracy\n    combined_features = {\n        \"vendor\": find_vendor_multimodal(visual_features, text_features),\n        \"amount\": find_amount_with_layout(visual_features, text_features),\n        \"line_items\": extract_table_data(visual_features, text_features)\n    }\n    \n    return combined_features\n```\n\n**Confidence Scoring:**\n```python\ndef calculate_extraction_confidence(extraction_result):\n    \"\"\"\n    Assign confidence scores to extracted fields\n    \"\"\"\n    confidence_factors = {\n        \"model_confidence\": 0.4,    # Model's internal confidence\n        \"pattern_match\": 0.3,       # Regex pattern strength  \n        \"context_validation\": 0.2,  # Surrounding text context\n        \"cross_validation\": 0.1     # Agreement between methods\n    }\n    \n    field_confidence = {}\n    for field, value in extraction_result.items():\n        scores = {\n            \"model\": value.get(\"model_score\", 0),\n            \"pattern\": validate_pattern(field, value[\"text\"]),\n            \"context\": validate_context(field, value[\"context\"]),\n            \"cross\": cross_validate_field(field, value)\n        }\n        \n        total_confidence = sum(\n            scores[factor] * weight \n            for factor, weight in confidence_factors.items()\n        )\n        \n        field_confidence[field] = min(total_confidence, 1.0)\n    \n    return field_confidence\n```\n\n### Error Handling and Fallbacks\n\n**Graceful Degradation:**\n```python\ndef robust_field_extraction(text, field_name):\n    \"\"\"\n    Multiple extraction strategies with fallbacks\n    \"\"\"\n    strategies = [\n        (\"transformer_model\", extract_with_transformer),\n        (\"regex_patterns\", extract_with_regex),\n        (\"keyword_proximity\", extract_with_keywords),\n        (\"manual_review\", flag_for_manual_review)\n    ]\n    \n    for strategy_name, strategy_func in strategies:\n        try:\n            result = strategy_func(text, field_name)\n            if validate_result(result, field_name):\n                return {\n                    \"value\": result,\n                    \"method\": strategy_name,\n                    \"confidence\": calculate_confidence(result, strategy_name)\n                }\n        except Exception as e:\n            logger.warning(f\"Strategy {strategy_name} failed: {e}\")\n            continue\n    \n    # All strategies failed\n    return {\n        \"value\": None,\n        \"method\": \"failed\",\n        \"confidence\": 0.0,\n        \"requires_manual_review\": True\n    }\n```\n\n**Data Quality Assessment:**\n```python\ndef assess_data_quality(extracted_data):\n    \"\"\"\n    Evaluate the quality of extracted data\n    \"\"\"\n    quality_metrics = {\n        \"completeness\": calculate_completeness(extracted_data),\n        \"consistency\": check_field_consistency(extracted_data),\n        \"plausibility\": validate_business_logic(extracted_data),\n        \"confidence\": calculate_overall_confidence(extracted_data)\n    }\n    \n    # Overall quality score\n    quality_score = (\n        quality_metrics[\"completeness\"] * 0.3 +\n        quality_metrics[\"consistency\"] * 0.3 +\n        quality_metrics[\"plausibility\"] * 0.2 +\n        quality_metrics[\"confidence\"] * 0.2\n    )\n    \n    return {\n        \"overall_score\": quality_score,\n        \"metrics\": quality_metrics,\n        \"recommendation\": get_processing_recommendation(quality_score)\n    }\n\ndef get_processing_recommendation(quality_score):\n    \"\"\"\n    Recommend processing path based on quality\n    \"\"\"\n    if quality_score >= 0.9:\n        return \"auto_approve\"\n    elif quality_score >= 0.7:\n        return \"supervisor_review\"  \n    elif quality_score >= 0.5:\n        return \"manual_review\"\n    else:\n        return \"reject_and_resubmit\"\n```\n\nLet's implement the enhanced AI extraction system:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "class AIExtraction:\n",
    "    \"\"\"Extract structured information using AI models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize pipelines\n",
    "        print(\"Loading AI models...\")\n",
    "        \n",
    "        # NER for entity extraction\n",
    "        self.ner_pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=\"dslim/bert-base-NER\",\n",
    "            aggregation_strategy=\"simple\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        \n",
    "        # QA for specific field extraction\n",
    "        self.qa_pipeline = pipeline(\n",
    "            \"question-answering\",\n",
    "            model=\"distilbert-base-cased-distilled-squad\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        \n",
    "        print(\"✅ AI models loaded\")\n",
    "    \n",
    "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extract named entities from text\"\"\"\n",
    "        entities = self.ner_pipeline(text[:512])  # Limit for speed\n",
    "        \n",
    "        result = {\n",
    "            \"organizations\": [],\n",
    "            \"persons\": [],\n",
    "            \"locations\": [],\n",
    "            \"misc\": []\n",
    "        }\n",
    "        \n",
    "        for entity in entities:\n",
    "            entity_type = entity['entity_group'].lower()\n",
    "            if entity_type == 'org':\n",
    "                result['organizations'].append(entity['word'])\n",
    "            elif entity_type == 'per':\n",
    "                result['persons'].append(entity['word'])\n",
    "            elif entity_type == 'loc':\n",
    "                result['locations'].append(entity['word'])\n",
    "            else:\n",
    "                result['misc'].append(entity['word'])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def extract_amounts(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract monetary amounts using regex and context\"\"\"\n",
    "        amounts = []\n",
    "        \n",
    "        # Pattern for currency amounts\n",
    "        patterns = [\n",
    "            r'\\$([0-9,]+\\.?[0-9]*)',  # $1,234.56\n",
    "            r'\\€([0-9,]+\\.?[0-9]*)',  # €1,234.56\n",
    "            r'([0-9,]+\\.?[0-9]*)\\s*(USD|EUR|GBP)',  # 1234.56 USD\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.finditer(pattern, text)\n",
    "            for match in matches:\n",
    "                amount_str = match.group(1).replace(',', '')\n",
    "                try:\n",
    "                    amount = float(amount_str)\n",
    "                    # Find context around amount\n",
    "                    start = max(0, match.start() - 50)\n",
    "                    end = min(len(text), match.end() + 50)\n",
    "                    context = text[start:end]\n",
    "                    \n",
    "                    amounts.append({\n",
    "                        \"value\": amount,\n",
    "                        \"raw\": match.group(0),\n",
    "                        \"context\": context,\n",
    "                        \"position\": match.start()\n",
    "                    })\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        \n",
    "        # Sort by value descending (likely total is largest)\n",
    "        amounts.sort(key=lambda x: x['value'], reverse=True)\n",
    "        return amounts\n",
    "    \n",
    "    def extract_dates(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract dates from text\"\"\"\n",
    "        dates = []\n",
    "        \n",
    "        # Common date patterns\n",
    "        patterns = [\n",
    "            (r'\\d{1,2}/\\d{1,2}/\\d{4}', '%m/%d/%Y'),\n",
    "            (r'\\d{4}-\\d{2}-\\d{2}', '%Y-%m-%d'),\n",
    "            (r'\\d{1,2}-\\w{3}-\\d{4}', '%d-%b-%Y'),\n",
    "            (r'\\w+ \\d{1,2}, \\d{4}', '%B %d, %Y'),\n",
    "        ]\n",
    "        \n",
    "        for pattern, date_format in patterns:\n",
    "            matches = re.finditer(pattern, text)\n",
    "            for match in matches:\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(match.group(0), date_format)\n",
    "                    \n",
    "                    # Find what type of date this might be\n",
    "                    context = text[max(0, match.start()-30):match.end()+30].lower()\n",
    "                    date_type = \"unknown\"\n",
    "                    if \"invoice\" in context:\n",
    "                        date_type = \"invoice_date\"\n",
    "                    elif \"due\" in context or \"payment\" in context:\n",
    "                        date_type = \"due_date\"\n",
    "                    elif \"ship\" in context or \"deliver\" in context:\n",
    "                        date_type = \"delivery_date\"\n",
    "                    \n",
    "                    dates.append({\n",
    "                        \"date\": date_obj.strftime('%Y-%m-%d'),\n",
    "                        \"raw\": match.group(0),\n",
    "                        \"type\": date_type,\n",
    "                        \"position\": match.start()\n",
    "                    })\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        \n",
    "        return dates\n",
    "    \n",
    "    def extract_invoice_fields(self, text: str) -> Dict:\n",
    "        \"\"\"Extract specific invoice fields using QA\"\"\"\n",
    "        fields = {}\n",
    "        \n",
    "        questions = {\n",
    "            \"invoice_number\": \"What is the invoice number?\",\n",
    "            \"vendor\": \"Who is the vendor or seller?\",\n",
    "            \"buyer\": \"Who is the buyer or bill to?\",\n",
    "            \"payment_terms\": \"What are the payment terms?\",\n",
    "            \"tax_rate\": \"What is the tax rate or VAT percentage?\"\n",
    "        }\n",
    "        \n",
    "        for field, question in questions.items():\n",
    "            try:\n",
    "                answer = self.qa_pipeline(\n",
    "                    question=question,\n",
    "                    context=text[:512]  # Limit context length\n",
    "                )\n",
    "                fields[field] = {\n",
    "                    \"value\": answer['answer'],\n",
    "                    \"confidence\": answer['score']\n",
    "                }\n",
    "            except Exception as e:\n",
    "                fields[field] = {\"value\": None, \"error\": str(e)}\n",
    "        \n",
    "        return fields\n",
    "    \n",
    "    def process(self, document_data: Dict) -> Dict:\n",
    "        \"\"\"Main processing function\"\"\"\n",
    "        text = document_data.get('text', '')\n",
    "        \n",
    "        if not text:\n",
    "            return {\"error\": \"No text to process\"}\n",
    "        \n",
    "        result = {\n",
    "            \"entities\": self.extract_entities(text),\n",
    "            \"amounts\": self.extract_amounts(text),\n",
    "            \"dates\": self.extract_dates(text),\n",
    "            \"fields\": self.extract_invoice_fields(text)\n",
    "        }\n",
    "        \n",
    "        # Determine most likely total amount\n",
    "        if result['amounts']:\n",
    "            # Look for \"total\" in context\n",
    "            for amount in result['amounts']:\n",
    "                if 'total' in amount['context'].lower():\n",
    "                    result['total_amount'] = amount['value']\n",
    "                    break\n",
    "            else:\n",
    "                # Default to largest amount\n",
    "                result['total_amount'] = result['amounts'][0]['value']\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Test AI extraction\n",
    "print(\"\\nTesting AI Extraction...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample invoice text\n",
    "sample_text = \"\"\"\n",
    "INVOICE #INV-2024-001\n",
    "Date: January 15, 2024\n",
    "Due Date: February 14, 2024\n",
    "\n",
    "From: TechSupplies Co.\n",
    "To: ABC Corporation\n",
    "\n",
    "Items:\n",
    "- Laptops (5 units): $10,000\n",
    "- Software Licenses: $5,000\n",
    "\n",
    "Subtotal: $15,000\n",
    "Tax (10%): $1,500\n",
    "Total Amount Due: $16,500\n",
    "\n",
    "Payment Terms: Net 30\n",
    "\"\"\"\n",
    "\n",
    "ai_extractor = AIExtraction()\n",
    "extraction_result = ai_extractor.process({\"text\": sample_text})\n",
    "\n",
    "print(\"\\n📊 Extraction Results:\")\n",
    "print(f\"Organizations found: {extraction_result['entities']['organizations']}\")\n",
    "print(f\"Total amount: ${extraction_result.get('total_amount', 'N/A')}\")\n",
    "print(f\"Dates found: {len(extraction_result['dates'])}\")\n",
    "print(f\"Invoice number: {extraction_result['fields']['invoice_number']['value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Business Rules Engine - Encoding Business Logic\n\n### The Challenge of Business Rules\n\nEvery organization has unique rules that must be encoded into the system:\n\n**Complexity of Real Rules:**\n```python\n# Simple rule\nif amount > 5000:\n    require_manager_approval()\n\n# Real-world rule  \nif (amount > approval_limits[user.department][user.level] and \n    vendor.risk_score > risk_thresholds[vendor.category] and\n    (invoice_date - last_invoice_date).days < duplicate_window and\n    payment_terms not in approved_terms[vendor.contract_type]):\n    escalate_to_risk_committee()\n```\n\n**Rule Categories:**\n```python\nrule_categories = {\n    \"financial_controls\": [\n        \"Amount thresholds by department/role\",\n        \"Budget availability checks\", \n        \"Currency and exchange rate rules\",\n        \"Tax compliance requirements\"\n    ],\n    \"vendor_management\": [\n        \"Approved vendor lists\",\n        \"Vendor risk scoring\",\n        \"Contract term validation\",\n        \"Performance history checks\"\n    ],\n    \"compliance\": [\n        \"Regulatory requirements (SOX, GDPR)\",\n        \"Audit trail requirements\",\n        \"Document retention policies\",\n        \"Segregation of duties\"\n    ],\n    \"operational\": [\n        \"Duplicate detection\",\n        \"Three-way matching (PO, Receipt, Invoice)\",\n        \"GL coding validation\", \n        \"Workflow routing rules\"\n    ]\n}\n```\n\n### Rule Engine Architecture\n\n**Declarative Rule Definition:**\n```python\n# Rules defined in configuration, not code\nrules_config = {\n    \"amount_approval_matrix\": {\n        \"type\": \"threshold_matrix\",\n        \"dimensions\": [\"department\", \"role\", \"vendor_category\"],\n        \"thresholds\": {\n            (\"finance\", \"analyst\", \"trusted\"): 10000,\n            (\"finance\", \"manager\", \"trusted\"): 50000,\n            (\"operations\", \"manager\", \"new\"): 1000\n        },\n        \"escalation_path\": [\"supervisor\", \"department_head\", \"cfo\"]\n    },\n    \"vendor_validation\": {\n        \"type\": \"multi_criteria\",\n        \"criteria\": [\n            {\"field\": \"vendor_status\", \"operator\": \"in\", \"values\": [\"active\", \"approved\"]},\n            {\"field\": \"risk_score\", \"operator\": \"<=\", \"value\": 0.7},\n            {\"field\": \"contract_valid\", \"operator\": \"==\", \"value\": True}\n        ],\n        \"action\": \"approve\",\n        \"failure_action\": \"manual_review\"\n    }\n}\n```\n\n**Dynamic Rule Evaluation:**\n```python\nclass RuleEvaluator:\n    def __init__(self, rules_config):\n        self.rules = self.compile_rules(rules_config)\n        self.context_providers = self.setup_context_providers()\n    \n    def evaluate_rule(self, rule, invoice_data, context):\n        \"\"\"\n        Evaluate a single rule against invoice data\n        \"\"\"\n        if rule.type == \"threshold_matrix\":\n            return self.evaluate_threshold_matrix(rule, invoice_data, context)\n        elif rule.type == \"multi_criteria\":\n            return self.evaluate_multi_criteria(rule, invoice_data, context)\n        elif rule.type == \"custom_function\":\n            return self.evaluate_custom_function(rule, invoice_data, context)\n        else:\n            raise ValueError(f\"Unknown rule type: {rule.type}\")\n    \n    def get_enriched_context(self, invoice_data):\n        \"\"\"\n        Gather additional context for rule evaluation\n        \"\"\"\n        context = {}\n        \n        # User context\n        context[\"user\"] = self.context_providers[\"user\"].get_user_info(\n            invoice_data.get(\"submitted_by\")\n        )\n        \n        # Vendor context\n        context[\"vendor\"] = self.context_providers[\"vendor\"].get_vendor_details(\n            invoice_data.get(\"vendor_name\")\n        )\n        \n        # Historical context\n        context[\"history\"] = self.context_providers[\"history\"].get_vendor_history(\n            invoice_data.get(\"vendor_name\"), \n            lookback_days=90\n        )\n        \n        # Budget context\n        context[\"budget\"] = self.context_providers[\"budget\"].check_budget_availability(\n            invoice_data.get(\"cost_center\"),\n            invoice_data.get(\"amount\")\n        )\n        \n        return context\n```\n\n### Advanced Validation Patterns\n\n**Three-Way Matching:**\n```python\ndef three_way_matching(invoice, purchase_order, receipt):\n    \"\"\"\n    Validate invoice against PO and receipt\n    \"\"\"\n    validation_results = {\n        \"po_match\": validate_po_match(invoice, purchase_order),\n        \"receipt_match\": validate_receipt_match(invoice, receipt),\n        \"amount_variance\": calculate_amount_variance(invoice, purchase_order),\n        \"quantity_variance\": calculate_quantity_variance(invoice, receipt)\n    }\n    \n    # Business rules for tolerance\n    tolerances = {\n        \"amount_variance_percent\": 5.0,      # 5% tolerance\n        \"quantity_variance_percent\": 2.0,     # 2% tolerance  \n        \"max_amount_variance\": 100.0         # $100 absolute tolerance\n    }\n    \n    issues = []\n    if validation_results[\"amount_variance\"][\"percent\"] > tolerances[\"amount_variance_percent\"]:\n        if validation_results[\"amount_variance\"][\"absolute\"] > tolerances[\"max_amount_variance\"]:\n            issues.append(\"Amount variance exceeds tolerance\")\n    \n    if validation_results[\"quantity_variance\"][\"percent\"] > tolerances[\"quantity_variance_percent\"]:\n        issues.append(\"Quantity variance exceeds tolerance\")\n    \n    return {\n        \"passed\": len(issues) == 0,\n        \"issues\": issues,\n        \"validation_details\": validation_results\n    }\n```\n\n**Duplicate Detection:**\n```python\ndef detect_duplicates(invoice_data, lookback_days=30):\n    \"\"\"\n    Sophisticated duplicate detection\n    \"\"\"\n    duplicate_criteria = [\n        {\n            \"name\": \"exact_amount_and_vendor\",\n            \"weight\": 0.9,\n            \"fields\": [\"vendor_name\", \"amount\", \"invoice_date\"],\n            \"tolerance\": {\"amount\": 0.01, \"date_days\": 3}\n        },\n        {\n            \"name\": \"similar_amount_and_number\",\n            \"weight\": 0.8, \n            \"fields\": [\"vendor_name\", \"invoice_number\", \"amount\"],\n            \"tolerance\": {\"amount\": 0.05}\n        },\n        {\n            \"name\": \"fuzzy_vendor_and_amount\",\n            \"weight\": 0.7,\n            \"fields\": [\"vendor_name_fuzzy\", \"amount\"],\n            \"tolerance\": {\"vendor_similarity\": 0.85, \"amount\": 0.02}\n        }\n    ]\n    \n    potential_duplicates = []\n    \n    for criterion in duplicate_criteria:\n        candidates = search_similar_invoices(\n            invoice_data, \n            criterion[\"fields\"],\n            criterion[\"tolerance\"],\n            lookback_days\n        )\n        \n        for candidate in candidates:\n            similarity_score = calculate_similarity(\n                invoice_data, \n                candidate, \n                criterion[\"fields\"]\n            )\n            \n            if similarity_score * criterion[\"weight\"] > 0.6:\n                potential_duplicates.append({\n                    \"candidate\": candidate,\n                    \"criterion\": criterion[\"name\"],\n                    \"similarity_score\": similarity_score,\n                    \"weighted_score\": similarity_score * criterion[\"weight\"]\n                })\n    \n    return potential_duplicates\n```\n\n### Rule Maintenance and Governance\n\n**Version Control for Rules:**\n```python\nclass RuleVersionControl:\n    def __init__(self):\n        self.rule_history = {}\n        self.approval_workflow = ApprovalWorkflow()\n    \n    def propose_rule_change(self, rule_id, changes, requester):\n        \"\"\"\n        Propose changes to business rules\n        \"\"\"\n        change_request = {\n            \"rule_id\": rule_id,\n            \"changes\": changes,\n            \"requester\": requester,\n            \"status\": \"pending_review\",\n            \"created_at\": datetime.now(),\n            \"impact_analysis\": self.analyze_rule_impact(rule_id, changes)\n        }\n        \n        # Route to appropriate approvers\n        approvers = self.get_required_approvers(rule_id, changes)\n        self.approval_workflow.submit_for_approval(change_request, approvers)\n        \n        return change_request\n    \n    def analyze_rule_impact(self, rule_id, changes):\n        \"\"\"\n        Analyze impact of rule changes on historical data\n        \"\"\"\n        # Test new rule against last 90 days of invoices\n        historical_invoices = get_historical_invoices(days=90)\n        \n        current_rule = self.get_current_rule(rule_id)\n        proposed_rule = self.apply_changes(current_rule, changes)\n        \n        impact_analysis = {\n            \"invoices_affected\": 0,\n            \"approval_changes\": [],\n            \"processing_time_impact\": 0\n        }\n        \n        for invoice in historical_invoices:\n            current_result = current_rule.evaluate(invoice)\n            proposed_result = proposed_rule.evaluate(invoice) \n            \n            if current_result != proposed_result:\n                impact_analysis[\"invoices_affected\"] += 1\n                impact_analysis[\"approval_changes\"].append({\n                    \"invoice_id\": invoice.id,\n                    \"current_decision\": current_result,\n                    \"proposed_decision\": proposed_result\n                })\n        \n        return impact_analysis\n```\n\nLet's implement the business rules engine:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "class RuleType(Enum):\n",
    "    THRESHOLD = \"threshold\"\n",
    "    REQUIRED_FIELD = \"required_field\"\n",
    "    VENDOR_CHECK = \"vendor_check\"\n",
    "    DATE_VALIDATION = \"date_validation\"\n",
    "    DUPLICATE_CHECK = \"duplicate_check\"\n",
    "\n",
    "@dataclass\n",
    "class ValidationRule:\n",
    "    name: str\n",
    "    rule_type: RuleType\n",
    "    parameters: Dict\n",
    "    severity: str  # 'error', 'warning', 'info'\n",
    "    message: str\n",
    "\n",
    "class BusinessRulesEngine:\n",
    "    \"\"\"Apply business rules to validate invoices\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rules = self._initialize_rules()\n",
    "        self.approved_vendors = [\"TechSupplies Co.\", \"CloudServices Inc.\", \"Office Depot\"]\n",
    "        self.threshold_limits = {\n",
    "            \"auto_approve\": 5000,\n",
    "            \"manager_approval\": 25000,\n",
    "            \"cfo_approval\": 100000\n",
    "        }\n",
    "    \n",
    "    def _initialize_rules(self) -> List[ValidationRule]:\n",
    "        \"\"\"Define business rules\"\"\"\n",
    "        return [\n",
    "            ValidationRule(\n",
    "                name=\"invoice_number_required\",\n",
    "                rule_type=RuleType.REQUIRED_FIELD,\n",
    "                parameters={\"field\": \"invoice_number\"},\n",
    "                severity=\"error\",\n",
    "                message=\"Invoice number is required\"\n",
    "            ),\n",
    "            ValidationRule(\n",
    "                name=\"vendor_required\",\n",
    "                rule_type=RuleType.REQUIRED_FIELD,\n",
    "                parameters={\"field\": \"vendor\"},\n",
    "                severity=\"error\",\n",
    "                message=\"Vendor information is required\"\n",
    "            ),\n",
    "            ValidationRule(\n",
    "                name=\"amount_threshold_check\",\n",
    "                rule_type=RuleType.THRESHOLD,\n",
    "                parameters={\"field\": \"total_amount\"},\n",
    "                severity=\"warning\",\n",
    "                message=\"Amount exceeds automatic approval threshold\"\n",
    "            ),\n",
    "            ValidationRule(\n",
    "                name=\"vendor_approval_check\",\n",
    "                rule_type=RuleType.VENDOR_CHECK,\n",
    "                parameters={\"field\": \"vendor\"},\n",
    "                severity=\"warning\",\n",
    "                message=\"Vendor not in approved list\"\n",
    "            ),\n",
    "            ValidationRule(\n",
    "                name=\"due_date_validation\",\n",
    "                rule_type=RuleType.DATE_VALIDATION,\n",
    "                parameters={\"field\": \"due_date\"},\n",
    "                severity=\"info\",\n",
    "                message=\"Check due date for payment scheduling\"\n",
    "            ),\n",
    "        ]\n",
    "    \n",
    "    def check_required_field(self, data: Dict, field: str) -> Optional[str]:\n",
    "        \"\"\"Check if required field exists and has value\"\"\"\n",
    "        if field in data.get('fields', {}):\n",
    "            field_data = data['fields'][field]\n",
    "            if field_data.get('value') and field_data['value'].strip():\n",
    "                return None\n",
    "        return f\"Missing required field: {field}\"\n",
    "    \n",
    "    def check_amount_threshold(self, data: Dict) -> Optional[str]:\n",
    "        \"\"\"Check if amount exceeds thresholds\"\"\"\n",
    "        amount = data.get('total_amount', 0)\n",
    "        \n",
    "        if amount > self.threshold_limits['cfo_approval']:\n",
    "            return f\"Amount ${amount:,.2f} requires CFO approval\"\n",
    "        elif amount > self.threshold_limits['manager_approval']:\n",
    "            return f\"Amount ${amount:,.2f} requires manager approval\"\n",
    "        elif amount > self.threshold_limits['auto_approve']:\n",
    "            return f\"Amount ${amount:,.2f} exceeds auto-approval limit\"\n",
    "        return None\n",
    "    \n",
    "    def check_vendor_approved(self, data: Dict) -> Optional[str]:\n",
    "        \"\"\"Check if vendor is in approved list\"\"\"\n",
    "        vendor_field = data.get('fields', {}).get('vendor', {})\n",
    "        vendor = vendor_field.get('value', '')\n",
    "        \n",
    "        # Also check entities\n",
    "        organizations = data.get('entities', {}).get('organizations', [])\n",
    "        \n",
    "        # Check if any known vendor matches\n",
    "        all_vendors = [vendor] + organizations\n",
    "        for v in all_vendors:\n",
    "            if v in self.approved_vendors:\n",
    "                return None\n",
    "        \n",
    "        return f\"Vendor '{vendor}' not in approved vendor list\"\n",
    "    \n",
    "    def check_date_validity(self, data: Dict) -> Optional[str]:\n",
    "        \"\"\"Check date logic and validity\"\"\"\n",
    "        dates = data.get('dates', [])\n",
    "        \n",
    "        invoice_date = None\n",
    "        due_date = None\n",
    "        \n",
    "        for date_info in dates:\n",
    "            if date_info['type'] == 'invoice_date':\n",
    "                invoice_date = datetime.strptime(date_info['date'], '%Y-%m-%d')\n",
    "            elif date_info['type'] == 'due_date':\n",
    "                due_date = datetime.strptime(date_info['date'], '%Y-%m-%d')\n",
    "        \n",
    "        if invoice_date and due_date:\n",
    "            if due_date < invoice_date:\n",
    "                return \"Due date is before invoice date\"\n",
    "            \n",
    "            days_to_pay = (due_date - invoice_date).days\n",
    "            if days_to_pay > 90:\n",
    "                return f\"Payment terms of {days_to_pay} days exceed maximum\"\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def validate(self, extracted_data: Dict) -> Dict:\n",
    "        \"\"\"Run all validation rules\"\"\"\n",
    "        results = {\n",
    "            \"passed\": [],\n",
    "            \"warnings\": [],\n",
    "            \"errors\": [],\n",
    "            \"approval_level\": \"auto\",\n",
    "            \"is_valid\": True\n",
    "        }\n",
    "        \n",
    "        for rule in self.rules:\n",
    "            violation = None\n",
    "            \n",
    "            if rule.rule_type == RuleType.REQUIRED_FIELD:\n",
    "                violation = self.check_required_field(\n",
    "                    extracted_data, \n",
    "                    rule.parameters['field']\n",
    "                )\n",
    "            elif rule.rule_type == RuleType.THRESHOLD:\n",
    "                violation = self.check_amount_threshold(extracted_data)\n",
    "            elif rule.rule_type == RuleType.VENDOR_CHECK:\n",
    "                violation = self.check_vendor_approved(extracted_data)\n",
    "            elif rule.rule_type == RuleType.DATE_VALIDATION:\n",
    "                violation = self.check_date_validity(extracted_data)\n",
    "            \n",
    "            if violation:\n",
    "                if rule.severity == \"error\":\n",
    "                    results[\"errors\"].append(violation)\n",
    "                    results[\"is_valid\"] = False\n",
    "                elif rule.severity == \"warning\":\n",
    "                    results[\"warnings\"].append(violation)\n",
    "            else:\n",
    "                results[\"passed\"].append(rule.name)\n",
    "        \n",
    "        # Determine approval level\n",
    "        amount = extracted_data.get('total_amount', 0)\n",
    "        if amount > self.threshold_limits['cfo_approval']:\n",
    "            results[\"approval_level\"] = \"cfo\"\n",
    "        elif amount > self.threshold_limits['manager_approval']:\n",
    "            results[\"approval_level\"] = \"manager\"\n",
    "        elif amount > self.threshold_limits['auto_approve']:\n",
    "            results[\"approval_level\"] = \"supervisor\"\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test business rules\n",
    "print(\"\\nTesting Business Rules Engine...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "rules_engine = BusinessRulesEngine()\n",
    "validation_result = rules_engine.validate(extraction_result)\n",
    "\n",
    "print(\"\\n📋 Validation Results:\")\n",
    "print(f\"Valid: {validation_result['is_valid']}\")\n",
    "print(f\"Approval Level: {validation_result['approval_level']}\")\n",
    "print(f\"Passed Rules: {len(validation_result['passed'])}\")\n",
    "print(f\"Warnings: {validation_result['warnings']}\")\n",
    "print(f\"Errors: {validation_result['errors']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Complete Processing Pipeline - Production Architecture\n\n### Enterprise Integration Patterns\n\nProduction systems must integrate with existing enterprise infrastructure:\n\n**System Integration Map:**\n```python\nenterprise_integrations = {\n    \"erp_systems\": {\n        \"sap\": {\"connector\": \"SAP_RFC\", \"endpoints\": [\"vendor_master\", \"gl_accounts\"]},\n        \"oracle\": {\"connector\": \"Oracle_API\", \"endpoints\": [\"ap_invoices\", \"budgets\"]},\n        \"netsuite\": {\"connector\": \"REST_API\", \"endpoints\": [\"transactions\", \"vendors\"]}\n    },\n    \"storage_systems\": {\n        \"document_store\": \"AWS_S3_bucket\",\n        \"database\": \"PostgreSQL_cluster\", \n        \"data_lake\": \"Snowflake_warehouse\",\n        \"backup\": \"Azure_blob_storage\"\n    },\n    \"notification_systems\": {\n        \"email\": \"Exchange_server\",\n        \"sms\": \"Twilio_API\",\n        \"chat\": \"Slack_webhooks\",\n        \"mobile\": \"Firebase_push\"\n    },\n    \"security_systems\": {\n        \"authentication\": \"Active_Directory\",\n        \"authorization\": \"RBAC_service\",\n        \"audit\": \"Splunk_logging\",\n        \"encryption\": \"HashiCorp_Vault\"\n    }\n}\n```\n\n**Event-Driven Architecture:**\n```python\n# Modern systems use event-driven patterns\nclass InvoiceProcessingEvents:\n    \"\"\"\n    Event-driven invoice processing with pub/sub\n    \"\"\"\n    \n    def __init__(self):\n        self.event_bus = EventBus()\n        self.setup_event_handlers()\n    \n    def setup_event_handlers(self):\n        \"\"\"Register event handlers for different stages\"\"\"\n        \n        # Document events\n        self.event_bus.subscribe(\"document.uploaded\", self.handle_document_upload)\n        self.event_bus.subscribe(\"ocr.completed\", self.handle_ocr_completion)\n        \n        # Processing events  \n        self.event_bus.subscribe(\"extraction.completed\", self.handle_extraction_completion)\n        self.event_bus.subscribe(\"validation.completed\", self.handle_validation_completion)\n        \n        # Decision events\n        self.event_bus.subscribe(\"approval.required\", self.handle_approval_request)\n        self.event_bus.subscribe(\"invoice.approved\", self.handle_invoice_approval)\n        self.event_bus.subscribe(\"invoice.rejected\", self.handle_invoice_rejection)\n        \n        # Integration events\n        self.event_bus.subscribe(\"erp.sync_required\", self.handle_erp_sync)\n        self.event_bus.subscribe(\"payment.scheduled\", self.handle_payment_scheduling)\n    \n    def handle_document_upload(self, event):\n        \"\"\"Process document upload event\"\"\"\n        document_id = event.data[\"document_id\"]\n        \n        # Trigger OCR processing\n        self.event_bus.publish(\"ocr.start\", {\n            \"document_id\": document_id,\n            \"priority\": event.data.get(\"priority\", \"normal\"),\n            \"callback_url\": f\"/api/ocr/callback/{document_id}\"\n        })\n    \n    def handle_invoice_approval(self, event):\n        \"\"\"Handle approved invoice\"\"\"\n        invoice_data = event.data\n        \n        # Multiple downstream actions triggered by single event\n        self.event_bus.publish(\"erp.create_payable\", invoice_data)\n        self.event_bus.publish(\"notification.send_approval\", invoice_data)\n        self.event_bus.publish(\"audit.log_approval\", invoice_data)\n        self.event_bus.publish(\"analytics.update_metrics\", invoice_data)\n```\n\n### Monitoring and Observability\n\n**Comprehensive Monitoring Stack:**\n```python\nclass MonitoringSystem:\n    \"\"\"\n    Enterprise-grade monitoring for invoice processing\n    \"\"\"\n    \n    def __init__(self):\n        self.metrics_collector = MetricsCollector()\n        self.alerting = AlertingSystem()\n        self.tracing = DistributedTracing()\n    \n    def track_processing_metrics(self):\n        \"\"\"Track key business and technical metrics\"\"\"\n        \n        business_metrics = {\n            \"invoices_processed_per_hour\": self.get_processing_rate(),\n            \"straight_through_processing_rate\": self.get_stp_rate(),\n            \"average_processing_time\": self.get_avg_processing_time(),\n            \"manual_review_rate\": self.get_manual_review_rate(),\n            \"approval_rates_by_amount\": self.get_approval_rates(),\n            \"vendor_performance_scores\": self.get_vendor_scores()\n        }\n        \n        technical_metrics = {\n            \"ocr_accuracy\": self.get_ocr_accuracy(),\n            \"extraction_confidence\": self.get_extraction_confidence(),\n            \"api_response_times\": self.get_api_response_times(),\n            \"error_rates_by_component\": self.get_error_rates(),\n            \"resource_utilization\": self.get_resource_utilization(),\n            \"queue_depths\": self.get_queue_depths()\n        }\n        \n        # Send to monitoring systems\n        self.metrics_collector.record_batch(business_metrics)\n        self.metrics_collector.record_batch(technical_metrics)\n    \n    def setup_alerts(self):\n        \"\"\"Configure alerting for critical issues\"\"\"\n        \n        alert_rules = [\n            {\n                \"name\": \"High Error Rate\",\n                \"condition\": \"error_rate > 5% for 5 minutes\",\n                \"severity\": \"critical\",\n                \"channels\": [\"pagerduty\", \"slack\"]\n            },\n            {\n                \"name\": \"Processing Backlog\",\n                \"condition\": \"queue_depth > 1000 for 10 minutes\", \n                \"severity\": \"warning\",\n                \"channels\": [\"email\", \"slack\"]\n            },\n            {\n                \"name\": \"Low STP Rate\",\n                \"condition\": \"stp_rate < 80% for 30 minutes\",\n                \"severity\": \"warning\", \n                \"channels\": [\"email\"]\n            },\n            {\n                \"name\": \"OCR Service Down\",\n                \"condition\": \"ocr_service_availability < 99%\",\n                \"severity\": \"critical\",\n                \"channels\": [\"pagerduty\", \"sms\"]\n            }\n        ]\n        \n        for rule in alert_rules:\n            self.alerting.create_alert_rule(rule)\n```\n\n### Scalability and Performance\n\n**Horizontal Scaling Architecture:**\n```python\nclass ScalableProcessingCluster:\n    \"\"\"\n    Auto-scaling invoice processing cluster\n    \"\"\"\n    \n    def __init__(self):\n        self.load_balancer = LoadBalancer()\n        self.worker_pool = WorkerPool()\n        self.auto_scaler = AutoScaler()\n        self.cache = DistributedCache()\n    \n    def process_batch(self, invoices, priority=\"normal\"):\n        \"\"\"\n        Process batch of invoices with auto-scaling\n        \"\"\"\n        \n        # Estimate resource requirements\n        estimated_processing_time = self.estimate_processing_time(invoices)\n        required_workers = self.calculate_worker_requirements(\n            len(invoices), \n            estimated_processing_time,\n            priority\n        )\n        \n        # Scale up if needed\n        current_workers = self.worker_pool.get_active_workers()\n        if required_workers > current_workers:\n            self.auto_scaler.scale_up(required_workers - current_workers)\n        \n        # Distribute work across workers\n        batches = self.create_optimal_batches(invoices, current_workers)\n        \n        results = []\n        async with TaskPool() as pool:\n            for batch in batches:\n                task = pool.submit(self.process_invoice_batch, batch)\n                results.append(task)\n            \n            # Wait for all batches to complete\n            completed_results = await pool.gather(*results)\n        \n        # Scale down if utilization is low\n        if self.get_utilization() < 0.3:\n            self.auto_scaler.scale_down()\n        \n        return self.merge_batch_results(completed_results)\n    \n    def optimize_performance(self):\n        \"\"\"\n        Continuous performance optimization\n        \"\"\"\n        \n        # Model caching strategy\n        self.cache.set_model_cache_policy({\n            \"extraction_models\": {\"ttl\": 3600, \"max_size\": \"2GB\"},\n            \"vendor_data\": {\"ttl\": 1800, \"max_size\": \"500MB\"},\n            \"business_rules\": {\"ttl\": 7200, \"max_size\": \"100MB\"}\n        })\n        \n        # Database query optimization\n        self.optimize_database_queries()\n        \n        # Batch processing optimization\n        self.optimize_batch_sizes()\n        \n        # Resource allocation optimization\n        self.optimize_resource_allocation()\n```\n\n### Security and Compliance\n\n**Enterprise Security Framework:**\n```python\nclass SecurityFramework:\n    \"\"\"\n    Comprehensive security for invoice processing\n    \"\"\"\n    \n    def __init__(self):\n        self.encryption = EncryptionService()\n        self.access_control = AccessControlService() \n        self.audit_logger = AuditLogger()\n        self.data_classifier = DataClassifier()\n    \n    def secure_document_processing(self, document, user_context):\n        \"\"\"\n        Secure processing pipeline for sensitive documents\n        \"\"\"\n        \n        # 1. Classify document sensitivity\n        classification = self.data_classifier.classify(document)\n        \n        # 2. Apply appropriate security controls\n        if classification.contains_pii:\n            document = self.anonymize_pii(document)\n        \n        if classification.sensitivity == \"confidential\":\n            document = self.encryption.encrypt_at_rest(document)\n        \n        # 3. Validate user permissions\n        if not self.access_control.can_process(user_context, classification):\n            raise PermissionDeniedError(\"Insufficient permissions\")\n        \n        # 4. Log all access\n        self.audit_logger.log_document_access({\n            \"user\": user_context.user_id,\n            \"document\": document.id,\n            \"classification\": classification.level,\n            \"action\": \"process\",\n            \"timestamp\": datetime.utcnow()\n        })\n        \n        return document\n    \n    def implement_data_governance(self):\n        \"\"\"\n        Data governance for compliance (GDPR, SOX, etc.)\n        \"\"\"\n        \n        governance_policies = {\n            \"data_retention\": {\n                \"invoices\": \"7_years\",\n                \"supporting_docs\": \"7_years\", \n                \"processing_logs\": \"3_years\",\n                \"pii_data\": \"deletion_on_request\"\n            },\n            \"data_residency\": {\n                \"eu_customers\": \"eu_west_1\",\n                \"us_customers\": \"us_east_1\",\n                \"sensitive_data\": \"on_premises\"\n            },\n            \"access_controls\": {\n                \"principle\": \"least_privilege\",\n                \"review_frequency\": \"quarterly\",\n                \"mfa_required\": True,\n                \"session_timeout\": \"4_hours\"\n            }\n        }\n        \n        return governance_policies\n```\n\nLet's implement the complete production pipeline:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Define complete state\n",
    "class InvoiceProcessingState(TypedDict):\n",
    "    # Input\n",
    "    document_path: str\n",
    "    \n",
    "    # Processing stages\n",
    "    raw_document: Optional[Dict]\n",
    "    extracted_data: Optional[Dict]\n",
    "    validation_results: Optional[Dict]\n",
    "    \n",
    "    # Decision\n",
    "    final_decision: Optional[str]\n",
    "    approval_level: Optional[str]\n",
    "    \n",
    "    # Audit\n",
    "    processing_log: List[Dict]\n",
    "    total_time: Optional[float]\n",
    "    timestamp: str\n",
    "\n",
    "class InvoiceProcessingPipeline:\n",
    "    \"\"\"Complete invoice processing system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ingestion = DocumentIngestion()\n",
    "        self.extractor = AIExtraction()\n",
    "        self.rules = BusinessRulesEngine()\n",
    "        self.workflow = self._build_workflow()\n",
    "    \n",
    "    def _build_workflow(self) -> StateGraph:\n",
    "        \"\"\"Build the processing workflow\"\"\"\n",
    "        workflow = StateGraph(InvoiceProcessingState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"ingest\", self._ingest_document)\n",
    "        workflow.add_node(\"extract\", self._extract_information)\n",
    "        workflow.add_node(\"validate\", self._validate_business_rules)\n",
    "        workflow.add_node(\"decide\", self._make_decision)\n",
    "        workflow.add_node(\"log\", self._log_results)\n",
    "        \n",
    "        # Add edges\n",
    "        workflow.set_entry_point(\"ingest\")\n",
    "        workflow.add_edge(\"ingest\", \"extract\")\n",
    "        workflow.add_edge(\"extract\", \"validate\")\n",
    "        workflow.add_edge(\"validate\", \"decide\")\n",
    "        workflow.add_edge(\"decide\", \"log\")\n",
    "        workflow.add_edge(\"log\", END)\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def _ingest_document(self, state: InvoiceProcessingState) -> InvoiceProcessingState:\n",
    "        \"\"\"Ingest and OCR document\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        state['raw_document'] = self.ingestion.process_document(state['document_path'])\n",
    "        \n",
    "        state['processing_log'].append({\n",
    "            \"stage\": \"ingestion\",\n",
    "            \"status\": state['raw_document']['status'],\n",
    "            \"duration\": time.time() - start\n",
    "        })\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _extract_information(self, state: InvoiceProcessingState) -> InvoiceProcessingState:\n",
    "        \"\"\"Extract structured data using AI\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        if state['raw_document']['status'] == 'success':\n",
    "            state['extracted_data'] = self.extractor.process(state['raw_document'])\n",
    "        else:\n",
    "            state['extracted_data'] = {\"error\": \"Document ingestion failed\"}\n",
    "        \n",
    "        state['processing_log'].append({\n",
    "            \"stage\": \"extraction\",\n",
    "            \"fields_extracted\": len(state['extracted_data'].get('fields', {})),\n",
    "            \"duration\": time.time() - start\n",
    "        })\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _validate_business_rules(self, state: InvoiceProcessingState) -> InvoiceProcessingState:\n",
    "        \"\"\"Apply business rules validation\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        if state['extracted_data'] and 'error' not in state['extracted_data']:\n",
    "            state['validation_results'] = self.rules.validate(state['extracted_data'])\n",
    "        else:\n",
    "            state['validation_results'] = {\n",
    "                \"is_valid\": False,\n",
    "                \"errors\": [\"Extraction failed\"],\n",
    "                \"approval_level\": \"manual\"\n",
    "            }\n",
    "        \n",
    "        state['processing_log'].append({\n",
    "            \"stage\": \"validation\",\n",
    "            \"is_valid\": state['validation_results']['is_valid'],\n",
    "            \"warnings\": len(state['validation_results'].get('warnings', [])),\n",
    "            \"errors\": len(state['validation_results'].get('errors', [])),\n",
    "            \"duration\": time.time() - start\n",
    "        })\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _make_decision(self, state: InvoiceProcessingState) -> InvoiceProcessingState:\n",
    "        \"\"\"Make final approval decision\"\"\"\n",
    "        validation = state['validation_results']\n",
    "        \n",
    "        if not validation['is_valid']:\n",
    "            state['final_decision'] = \"REJECTED\"\n",
    "            state['approval_level'] = \"N/A\"\n",
    "        elif validation['errors']:\n",
    "            state['final_decision'] = \"MANUAL_REVIEW\"\n",
    "            state['approval_level'] = validation['approval_level']\n",
    "        elif validation['warnings'] and validation['approval_level'] != 'auto':\n",
    "            state['final_decision'] = \"PENDING_APPROVAL\"\n",
    "            state['approval_level'] = validation['approval_level']\n",
    "        else:\n",
    "            state['final_decision'] = \"APPROVED\"\n",
    "            state['approval_level'] = validation['approval_level']\n",
    "        \n",
    "        state['processing_log'].append({\n",
    "            \"stage\": \"decision\",\n",
    "            \"final_decision\": state['final_decision'],\n",
    "            \"approval_level\": state['approval_level']\n",
    "        })\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _log_results(self, state: InvoiceProcessingState) -> InvoiceProcessingState:\n",
    "        \"\"\"Log results for audit trail\"\"\"\n",
    "        state['total_time'] = sum(\n",
    "            log.get('duration', 0) \n",
    "            for log in state['processing_log']\n",
    "        )\n",
    "        \n",
    "        # In production, would save to database\n",
    "        audit_log = {\n",
    "            \"timestamp\": state['timestamp'],\n",
    "            \"document\": state['document_path'],\n",
    "            \"decision\": state['final_decision'],\n",
    "            \"approval_level\": state['approval_level'],\n",
    "            \"total_time\": state['total_time'],\n",
    "            \"details\": {\n",
    "                \"amount\": state['extracted_data'].get('total_amount'),\n",
    "                \"vendor\": state['extracted_data'].get('fields', {}).get('vendor', {}).get('value'),\n",
    "                \"warnings\": state['validation_results'].get('warnings', []),\n",
    "                \"errors\": state['validation_results'].get('errors', [])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"\\n📝 AUDIT LOG ENTRY:\")\n",
    "        print(json.dumps(audit_log, indent=2, default=str))\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def process(self, document_path: str) -> Dict:\n",
    "        \"\"\"Process a document through the complete pipeline\"\"\"\n",
    "        initial_state = {\n",
    "            \"document_path\": document_path,\n",
    "            \"processing_log\": [],\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        result = self.workflow.invoke(initial_state)\n",
    "        return result\n",
    "\n",
    "# Create and test complete pipeline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPLETE PIPELINE TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pipeline = InvoiceProcessingPipeline()\n",
    "\n",
    "# Process test document\n",
    "print(\"\\n🚀 Processing document...\")\n",
    "result = pipeline.process('/tmp/test_invoice.png')\n",
    "\n",
    "print(\"\\n✅ PROCESSING COMPLETE!\")\n",
    "print(f\"Final Decision: {result['final_decision']}\")\n",
    "print(f\"Approval Level: {result['approval_level']}\")\n",
    "print(f\"Total Time: {result['total_time']:.2f} seconds\")\n",
    "\n",
    "# Show processing stages\n",
    "print(\"\\n📊 Processing Stages:\")\n",
    "for log_entry in result['processing_log']:\n",
    "    print(f\"  - {log_entry['stage']}: {log_entry.get('duration', 0):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Production Deployment Considerations\n",
    "\n",
    "Key considerations for deploying this system in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PRODUCTION DEPLOYMENT GUIDE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "deployment_guide = \"\"\"\n",
    "### 1. SCALABILITY\n",
    "- Use message queues (RabbitMQ, Kafka) for async processing\n",
    "- Deploy AI models on GPU clusters\n",
    "- Implement caching for frequently accessed data\n",
    "- Use load balancers for API endpoints\n",
    "\n",
    "### 2. RELIABILITY\n",
    "- Implement retry logic with exponential backoff\n",
    "- Add circuit breakers for external services\n",
    "- Create fallback mechanisms for AI model failures\n",
    "- Maintain audit logs for all decisions\n",
    "\n",
    "### 3. SECURITY\n",
    "- Encrypt documents at rest and in transit\n",
    "- Implement role-based access control (RBAC)\n",
    "- Add PII detection and masking\n",
    "- Regular security audits and penetration testing\n",
    "\n",
    "### 4. MONITORING\n",
    "- Track processing times and success rates\n",
    "- Monitor model accuracy and drift\n",
    "- Alert on anomalies and failures\n",
    "- Dashboard for business metrics\n",
    "\n",
    "### 5. INTEGRATION\n",
    "- REST API for document submission\n",
    "- Webhooks for status updates\n",
    "- Integration with ERP systems (SAP, Oracle)\n",
    "- Email notifications for approvals\n",
    "\n",
    "### 6. COST OPTIMIZATION\n",
    "- Use spot instances for batch processing\n",
    "- Implement model quantization for faster inference\n",
    "- Cache OCR results to avoid reprocessing\n",
    "- Auto-scale based on queue depth\n",
    "\"\"\"\n",
    "\n",
    "print(deployment_guide)\n",
    "\n",
    "# Performance metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPECTED PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metrics = {\n",
    "    \"Processing Time\": {\n",
    "        \"Simple Invoice (1 page)\": \"3-5 seconds\",\n",
    "        \"Complex Invoice (5+ pages)\": \"10-15 seconds\",\n",
    "        \"Batch (100 invoices)\": \"5-10 minutes\"\n",
    "    },\n",
    "    \"Accuracy\": {\n",
    "        \"Field Extraction\": \"95-98%\",\n",
    "        \"Amount Detection\": \"99%\",\n",
    "        \"Vendor Recognition\": \"92-95%\"\n",
    "    },\n",
    "    \"Throughput\": {\n",
    "        \"Single GPU\": \"500-1000 invoices/hour\",\n",
    "        \"GPU Cluster (4x)\": \"2000-4000 invoices/hour\"\n",
    "    },\n",
    "    \"Cost\": {\n",
    "        \"Per Invoice\": \"$0.02-0.05\",\n",
    "        \"Monthly (10K invoices)\": \"$200-500\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, values in metrics.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for metric, value in values.items():\n",
    "        print(f\"  - {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "### Complete System Architecture:\n",
    "\n",
    "1. **Multi-Layer Processing**\n",
    "   - Ingestion: Handle various document formats\n",
    "   - Extraction: AI-powered information extraction\n",
    "   - Validation: Business rules enforcement\n",
    "   - Decision: Automated approval logic\n",
    "   - Audit: Complete traceability\n",
    "\n",
    "2. **AI Model Integration**\n",
    "   - OCR for text extraction\n",
    "   - NER for entity recognition\n",
    "   - QA for specific field extraction\n",
    "   - Pattern matching for amounts and dates\n",
    "\n",
    "3. **Business Logic**\n",
    "   - Configurable rules engine\n",
    "   - Multi-level approval workflows\n",
    "   - Vendor validation\n",
    "   - Amount threshold checks\n",
    "\n",
    "4. **Production Readiness**\n",
    "   - Error handling at every stage\n",
    "   - Comprehensive logging\n",
    "   - Performance monitoring\n",
    "   - Scalable architecture\n",
    "\n",
    "### Real-World Impact:\n",
    "\n",
    "- **Efficiency**: 80-90% reduction in manual processing time\n",
    "- **Accuracy**: Fewer errors than manual data entry\n",
    "- **Compliance**: Automatic policy enforcement\n",
    "- **Visibility**: Real-time processing status\n",
    "- **Scalability**: Handle enterprise volumes\n",
    "\n",
    "### What's Next:\n",
    "\n",
    "In the final session, we'll explore:\n",
    "- Advanced optimization techniques\n",
    "- Custom model fine-tuning\n",
    "- Multi-language support\n",
    "- Complex document types (contracts, reports)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}