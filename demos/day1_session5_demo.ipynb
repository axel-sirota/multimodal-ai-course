{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1, Session 5: Advanced Techniques and Optimization\n",
    "\n",
    "## Mastering Production-Grade Invoice Processing\n",
    "\n",
    "### The Evolution of Document Understanding\n",
    "\n",
    "We've built the foundation with agents and workflows. Now we optimize for production with advanced vision models and memory management.\n",
    "\n",
    "**Traditional Approach:**\n",
    "```\n",
    "Document ‚Üí OCR ‚Üí Parse Text ‚Üí Extract Fields ‚Üí Structure Data\n",
    "Challenges: Poor quality scans, complex layouts, multilingual text\n",
    "```\n",
    "\n",
    "**Modern End-to-End Approach:**\n",
    "```\n",
    "Document Image ‚Üí Vision Model ‚Üí Structured Output\n",
    "Benefits: Layout understanding, robust to quality, faster processing\n",
    "```\n",
    "\n",
    "**Hybrid Production Approach:**\n",
    "```\n",
    "Document ‚Üí Quality Assessment ‚Üí Route to Best Model ‚Üí Validate & Combine\n",
    "```\n",
    "\n",
    "### Why This Matters for Business\n",
    "\n",
    "**Cost Impact:**\n",
    "- Manual processing: $15-30 per invoice\n",
    "- Traditional OCR: $2-5 per invoice\n",
    "- Modern AI: $0.10-0.50 per invoice\n",
    "\n",
    "**Accuracy Improvement:**\n",
    "- Human entry: 95-98% accuracy\n",
    "- OCR + Rules: 85-92% accuracy\n",
    "- Vision Models: 96-99% accuracy\n",
    "\n",
    "**Speed Enhancement:**\n",
    "- Manual: 15-30 minutes per invoice\n",
    "- Traditional: 2-5 minutes per invoice\n",
    "- Modern AI: 3-10 seconds per invoice\n",
    "\n",
    "Let's see how to achieve these results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Global configuration - Instructor will fill these\n",
    "OLLAMA_URL = \"http://XX.XX.XX.XX\"  # Course server IP (port 80)\n",
    "API_TOKEN = \"YOUR_TOKEN_HERE\"      # Instructor provides token\n",
    "MODEL = \"qwen3:8b\"                  # Default model on server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: GPU Memory Management - T4 Optimization\n",
    "\n",
    "### Understanding T4 GPU Constraints\n",
    "\n",
    "**T4 GPU Specifications:**\n",
    "- Memory: 16GB GDDR6\n",
    "- CUDA Cores: 2,560\n",
    "- Tensor Cores: 320\n",
    "- Memory Bandwidth: 300 GB/s\n",
    "\n",
    "**Memory Management Strategy:**\n",
    "```python\n",
    "# Memory allocation priorities\n",
    "memory_budget = {\n",
    "    \"model_weights\": \"8-12GB\",      # Core model parameters\n",
    "    \"activation_cache\": \"2-4GB\",    # Intermediate computations\n",
    "    \"input_batch\": \"1-2GB\",        # Input tensors\n",
    "    \"system_reserve\": \"1-2GB\"      # OS and drivers\n",
    "}\n",
    "```\n",
    "\n",
    "**Optimization Techniques:**\n",
    "- **Mixed Precision**: Use FP16 instead of FP32 (50% memory savings)\n",
    "- **Gradient Checkpointing**: Trade compute for memory\n",
    "- **Model Quantization**: 8-bit or 4-bit weights\n",
    "- **Dynamic Batching**: Adjust batch size based on available memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check GPU availability and memory\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Get detailed GPU memory information\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3    # GB\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        return {\n",
    "            'allocated': allocated,\n",
    "            'reserved': reserved,\n",
    "            'free': total - reserved,\n",
    "            'total': total\n",
    "        }\n",
    "    return {'error': 'No GPU available'}\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Aggressive memory cleanup\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def monitor_memory(func_name):\n",
    "    \"\"\"Decorator to monitor memory usage\"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Before\n",
    "            mem_before = get_gpu_memory()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Execute\n",
    "            result = func(*args, **kwargs)\n",
    "            \n",
    "            # After\n",
    "            mem_after = get_gpu_memory()\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            print(f\"\\nüìä {func_name} Performance:\")\n",
    "            print(f\"   Memory used: {mem_after['allocated'] - mem_before['allocated']:.2f}GB\")\n",
    "            print(f\"   Time: {elapsed:.2f}s\")\n",
    "            print(f\"   GPU utilization: {mem_after['allocated']/mem_after['total']*100:.1f}%\")\n",
    "            \n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# System check\n",
    "print(\"üîß SYSTEM CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    memory_info = get_gpu_memory()\n",
    "    \n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Total Memory: {memory_info['total']:.1f}GB\")\n",
    "    print(f\"Available: {memory_info['free']:.1f}GB\")\n",
    "    print(f\"Current Usage: {memory_info['allocated']:.2f}GB\")\n",
    "    \n",
    "    # Check if it's a T4\n",
    "    if \"T4\" in gpu_name:\n",
    "        print(\"‚úÖ T4 GPU detected - optimizations enabled\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Non-T4 GPU detected ({gpu_name}) - may need different optimizations\")\nelse:\n",
    "    print(\"‚ùå No GPU available - will use CPU (much slower)\")\n",
    "\n",
    "# Install required packages\n",
    "print(\"\\nüì¶ Installing required packages...\")\n",
    "!pip install -q transformers pillow pytesseract easyocr accelerate bitsandbytes\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Sample Invoice Dataset\n",
    "\n",
    "### Dataset Strategy for Production\n",
    "\n",
    "**Real-world invoice characteristics:**\n",
    "- **Quality variations**: 72 DPI to 600 DPI scans\n",
    "- **Format diversity**: Native PDFs, scanned images, phone photos\n",
    "- **Layout complexity**: Simple receipts to multi-page invoices\n",
    "- **Language variety**: English, Spanish, French, multilingual\n",
    "\n",
    "**Testing approach:**\n",
    "```python\n",
    "test_categories = {\n",
    "    \"high_quality\": \"Clean scans, good contrast, standard layouts\",\n",
    "    \"medium_quality\": \"Slightly skewed, moderate noise, complex layouts\", \n",
    "    \"low_quality\": \"Poor scans, heavy noise, unusual formats\",\n",
    "    \"edge_cases\": \"Handwritten, multilingual, damaged documents\"\n",
    "}\n",
    "```\n",
    "\n",
    "This tests model robustness across real business scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download real invoice dataset\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "\n",
    "# Download from Dropbox\n",
    "dropbox_url = \"https://www.dropbox.com/scl/fo/m9hyfmvi78snwv0nh34mo/AMEXxwXMLAOeve-_yj12ck8?rlkey=urinkikgiuven0fro7r4x5rcu&st=hv3of7g7&dl=1\"\n",
    "\n",
    "print(\"üì¶ Downloading real invoice dataset...\")\n",
    "try:\n",
    "    response = requests.get(dropbox_url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "        z.extractall(\"invoice_images\")\n",
    "    \n",
    "    print(\"‚úÖ Downloaded invoice dataset\")\n",
    "    \n",
    "    # Catalog available images\n",
    "    invoice_files = []\n",
    "    for root, dirs, files in os.walk(\"invoice_images\"):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.pdf')):\n",
    "                full_path = os.path.join(root, file)\n",
    "                invoice_files.append(full_path)\n",
    "                print(f\"  üìÑ {full_path}\")\n",
    "    \n",
    "    # Load and categorize images\n",
    "    invoices = []\n",
    "    invoice_metadata = []\n",
    "    \n",
    "    for i, file_path in enumerate(invoice_files[:3]):  # Limit for demo\n",
    "        try:\n",
    "            img = Image.open(file_path)\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Analyze image characteristics\n",
    "            width, height = img.size\n",
    "            file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "            \n",
    "            # Estimate quality based on resolution and size\n",
    "            pixels = width * height\n",
    "            if pixels > 1000000:  # > 1MP\n",
    "                quality = \"high\"\n",
    "            elif pixels > 500000:  # > 0.5MP\n",
    "                quality = \"medium\" \n",
    "            else:\n",
    "                quality = \"low\"\n",
    "            \n",
    "            invoices.append(img)\n",
    "            invoice_metadata.append({\n",
    "                'filename': os.path.basename(file_path),\n",
    "                'size': f\"{width}x{height}\",\n",
    "                'pixels': pixels,\n",
    "                'file_size_kb': file_size,\n",
    "                'estimated_quality': quality\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"\\nüìä Loaded {len(invoices)} invoices for testing\")\n",
    "    \n",
    "    # Display samples with metadata\n",
    "    for i, (img, meta) in enumerate(zip(invoices, invoice_metadata)):\n",
    "        print(f\"\\nüìÑ Invoice {i+1}: {meta['filename']}\")\n",
    "        print(f\"   Size: {meta['size']}, Quality: {meta['estimated_quality']}\")\n",
    "        print(f\"   File size: {meta['file_size_kb']:.1f}KB\")\n",
    "        \n",
    "        # Display thumbnail\n",
    "        thumbnail = img.copy()\n",
    "        thumbnail.thumbnail((400, 500), Image.Resampling.LANCZOS)\n",
    "        display(thumbnail)\n",
    "\nexcept Exception as e:\n",
    "    print(f\"‚ùå Error downloading dataset: {e}\")\n",
    "    print(\"Will create synthetic test images...\")\n",
    "    \n",
    "    # Create synthetic invoices for testing\n",
    "    invoices = []\n",
    "    invoice_metadata = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        # Create a simple synthetic invoice\n",
    "        img = Image.new('RGB', (800, 1000), color='white')\n",
    "        invoices.append(img)\n",
    "        invoice_metadata.append({\n",
    "            'filename': f'synthetic_invoice_{i+1}.png',\n",
    "            'size': '800x1000',\n",
    "            'pixels': 800000,\n",
    "            'estimated_quality': 'high'\n",
    "        })\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(invoices)} synthetic invoices\")\n",
    "\n",
    "SAMPLE_INVOICES = invoices\n",
    "INVOICE_METADATA = invoice_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Approach 1 - Traditional OCR Pipeline\n",
    "\n",
    "### Understanding OCR Limitations and Strengths\n",
    "\n",
    "**OCR Strengths:**\n",
    "- Fast and lightweight\n",
    "- Works well with high-quality scans\n",
    "- Language-agnostic\n",
    "- Deterministic output\n",
    "\n",
    "**OCR Limitations:**\n",
    "- Poor handling of complex layouts\n",
    "- Struggles with low-quality images\n",
    "- No semantic understanding\n",
    "- Requires post-processing rules\n",
    "\n",
    "**Production OCR Strategy:**\n",
    "```python\n",
    "# Multi-engine approach\n",
    "ocr_engines = {\n",
    "    \"tesseract\": \"General purpose, good for typed text\",\n",
    "    \"easyocr\": \"Better for handwriting and multilingual\",\n",
    "    \"paddleocr\": \"Excellent for Asian languages\",\n",
    "    \"azure_read\": \"Cloud service, best accuracy\"\n",
    "}\n",
    "\n",
    "# Confidence-based selection\n",
    "best_result = max(results, key=lambda x: x.confidence)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Traditional OCR approach with EasyOCR\n",
    "import easyocr\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize OCR reader\n",
    "print(\"üîß Initializing OCR engine...\")\n",
    "reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
    "print(\"‚úÖ EasyOCR ready\")\n",
    "\n",
    "@monitor_memory(\"OCR Processing\")\n",
    "def extract_with_ocr(image):\n",
    "    \"\"\"Traditional OCR approach with rule-based extraction\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert PIL to numpy array\n",
    "    img_array = np.array(image)\n",
    "    \n",
    "    # Extract text with confidence scores\n",
    "    ocr_results = reader.readtext(img_array)\n",
    "    \n",
    "    # Combine text and calculate overall confidence\n",
    "    text_blocks = []\n",
    "    confidences = []\n",
    "    \n",
    "    for (bbox, text, confidence) in ocr_results:\n",
    "        text_blocks.append(text)\n",
    "        confidences.append(confidence)\n",
    "    \n",
    "    # Join all text\n",
    "    full_text = ' '.join(text_blocks)\n",
    "    avg_confidence = sum(confidences) / len(confidences) if confidences else 0\n",
    "    \n",
    "    print(f\"üìù OCR extracted {len(text_blocks)} text blocks\")\n",
    "    print(f\"üìä Average confidence: {avg_confidence:.2f}\")\n",
    "    print(f\"üìÑ Total text: {len(full_text)} characters\")\n",
    "    \n",
    "    # Extract structured fields using regex patterns\n",
    "    invoice_data = {\n",
    "        'extraction_method': 'ocr',\n",
    "        'confidence': avg_confidence,\n",
    "        'raw_text': full_text\n",
    "    }\n",
    "    \n",
    "    # Invoice number patterns\n",
    "    inv_patterns = [\n",
    "        r'INV[A-Z]*[-\\s]?\\d+',\n",
    "        r'Invoice[\\s#:]*([A-Z0-9-]+)',\n",
    "        r'#\\s*(\\d+)',\n",
    "        r'No[\\s.]*([A-Z0-9-]+)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in inv_patterns:\n",
    "        match = re.search(pattern, full_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            invoice_data['invoice_number'] = match.group(1) if match.groups() else match.group()\n",
    "            break\n",
    "    \n",
    "    # Amount patterns (multiple currencies)\n",
    "    amount_patterns = [\n",
    "        r'Total:?\\s*[\\$‚Ç¨¬£¬•]?([0-9,]+\\.?[0-9]*)',\n",
    "        r'Amount:?\\s*[\\$‚Ç¨¬£¬•]?([0-9,]+\\.?[0-9]*)',\n",
    "        r'[\\$‚Ç¨¬£¬•]\\s*([0-9,]+\\.?[0-9]*)',\n",
    "        r'([0-9,]+\\.?[0-9]*)\\s*[\\$‚Ç¨¬£¬•]'\n",
    "    ]\n",
    "    \n",
    "    amounts_found = []\n",
    "    for pattern in amount_patterns:\n",
    "        matches = re.finditer(pattern, full_text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                amount_str = match.group(1).replace(',', '')\n",
    "                amount = float(amount_str)\n",
    "                amounts_found.append(amount)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    if amounts_found:\n",
    "        # Usually the largest amount is the total\n",
    "        invoice_data['total_amount'] = max(amounts_found)\n",
    "        invoice_data['all_amounts'] = amounts_found\n",
    "    \n",
    "    # Date patterns\n",
    "    date_patterns = [\n",
    "        r'(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})',\n",
    "        r'(\\d{4}[-]\\d{2}[-]\\d{2})',\n",
    "        r'([A-Za-z]+ \\d{1,2}, \\d{4})',\n",
    "        r'(\\d{1,2} [A-Za-z]+ \\d{4})'\n",
    "    ]\n",
    "    \n",
    "    for pattern in date_patterns:\n",
    "        match = re.search(pattern, full_text)\n",
    "        if match:\n",
    "            invoice_data['date'] = match.group(1)\n",
    "            break\n",
    "    \n",
    "    # Vendor/Company patterns\n",
    "    # Look for text near top of document (first 20% of text)\n",
    "    top_text = ' '.join(text_blocks[:max(1, len(text_blocks)//5)])\n",
    "    \n",
    "    # Common company indicators\n",
    "    company_patterns = [\n",
    "        r'([A-Z][a-z]+ [A-Z][a-z]+\\s*(Inc|LLC|Corp|Ltd|Co))',\n",
    "        r'([A-Z][A-Z ]+[A-Z])\\s*(Inc|LLC|Corp|Ltd|Co)?'\n",
    "    ]\n",
    "    \n",
    "    for pattern in company_patterns:\n",
    "        match = re.search(pattern, top_text)\n",
    "        if match:\n",
    "            invoice_data['vendor'] = match.group().strip()\n",
    "            break\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    invoice_data['processing_time'] = processing_time\n",
    "    \n",
    "    return invoice_data\n",
    "\n",
    "# Test OCR on all sample invoices\n",
    "print(\"\\nüß™ TESTING OCR APPROACH\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "ocr_results = []\n",
    "for i, invoice in enumerate(SAMPLE_INVOICES):\n",
    "    print(f\"\\nüìÑ Processing Invoice {i+1} ({INVOICE_METADATA[i]['filename']})\")\n",
    "    \n",
    "    result = extract_with_ocr(invoice)\n",
    "    ocr_results.append(result)\n",
    "    \n",
    "    print(f\"\\n‚úÖ OCR Results:\")\n",
    "    print(f\"   Invoice Number: {result.get('invoice_number', 'Not found')}\")\n",
    "    print(f\"   Total Amount: ${result.get('total_amount', 'Not found')}\")\n",
    "    print(f\"   Date: {result.get('date', 'Not found')}\")\n",
    "    print(f\"   Vendor: {result.get('vendor', 'Not found')}\")\n",
    "    print(f\"   Confidence: {result.get('confidence', 0):.2f}\")\n",
    "    print(f\"   Processing Time: {result.get('processing_time', 0):.2f}s\")\n",
    "    \n",
    "    # Clear memory between processings\n",
    "    clear_gpu_memory()\n",
    "\n",
    "print(\"\\nüìä OCR Approach Summary:\")\n",
    "avg_time = sum(r.get('processing_time', 0) for r in ocr_results) / len(ocr_results)\n",
    "avg_confidence = sum(r.get('confidence', 0) for r in ocr_results) / len(ocr_results)\n",
    "print(f\"   Average processing time: {avg_time:.2f}s per invoice\")\n",
    "print(f\"   Average confidence: {avg_confidence:.2f}\")\n",
    "print(f\"   Memory efficient: ‚úÖ Low GPU usage\")\n",
    "print(f\"   Works offline: ‚úÖ No internet required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Approach 2 - Donut (OCR-Free)\n",
    "\n",
    "### Understanding End-to-End Document AI\n",
    "\n",
    "**Donut Architecture:**\n",
    "```\n",
    "Image ‚Üí Vision Encoder ‚Üí Text Decoder ‚Üí Structured Output\n",
    "       (Swin Transformer)  (BART)      (JSON)\n",
    "```\n",
    "\n",
    "**Key Advantages:**\n",
    "- No OCR preprocessing required\n",
    "- Understands document layout and structure\n",
    "- Directly outputs structured JSON\n",
    "- Trained on millions of documents\n",
    "\n",
    "**T4 GPU Optimizations:**\n",
    "- Use smaller Donut variants (base vs large)\n",
    "- Mixed precision training (FP16)\n",
    "- Gradient checkpointing\n",
    "- Optimized beam search parameters\n",
    "\n",
    "**Production Considerations:**\n",
    "```python\n",
    "optimization_strategies = {\n",
    "    \"model_size\": \"Use donut-base instead of donut-large\",\n",
    "    \"precision\": \"torch.cuda.amp for mixed precision\",\n",
    "    \"batching\": \"Process multiple images in batches\",\n",
    "    \"caching\": \"Cache model weights in GPU memory\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Donut - End-to-end document understanding\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "import json\n",
    "\n",
    "print(\"üç© Loading Donut model...\")\n",
    "print(\"Note: This may take a few minutes on first load\")\n",
    "\n",
    "# Check available memory before loading\n",
    "mem_before = get_gpu_memory()\n",
    "print(f\"Memory before Donut: {mem_before['free']:.1f}GB available\")\n",
    "\n",
    "try:\n",
    "    # Load Donut - optimized for T4\n",
    "    processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "    \n",
    "    # Optimize for T4 GPU\n",
    "    if torch.cuda.is_available():\n",
    "        # Use mixed precision for memory efficiency\n",
    "        model = model.half()  # Convert to FP16\n",
    "        model = model.to('cuda')\n",
    "        print(\"‚úÖ Donut loaded on GPU with FP16 optimization\")\n",
    "    else:\n",
    "        model = model.to('cpu')\n",
    "        print(\"‚ö†Ô∏è Donut loaded on CPU (will be slower)\")\n",
    "    \n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Check memory usage after loading\n",
    "    mem_after = get_gpu_memory()\n",
    "    model_memory = mem_after['allocated'] - mem_before['allocated']\n",
    "    print(f\"üìä Donut memory usage: {model_memory:.1f}GB\")\n",
    "    print(f\"üìä Remaining memory: {mem_after['free']:.1f}GB\")\n",
    "    \n",
    "    DONUT_AVAILABLE = True\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå Error loading Donut: {e}\")\n",
    "    print(\"This might be due to insufficient GPU memory\")\n",
    "    DONUT_AVAILABLE = False\n",
    "\n",
    "@monitor_memory(\"Donut Processing\")\n",
    "def extract_with_donut(image):\n",
    "    \"\"\"End-to-end extraction without OCR\"\"\"\n",
    "    if not DONUT_AVAILABLE:\n",
    "        return {\n",
    "            'extraction_method': 'donut',\n",
    "            'error': 'Donut model not available',\n",
    "            'processing_time': 0\n",
    "        }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Prepare image for Donut\n",
    "        # Donut expects specific input format\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            pixel_values = pixel_values.half().to('cuda')  # Match model precision\n",
    "        \n",
    "        print(f\"üìä Input tensor shape: {pixel_values.shape}\")\n",
    "        print(f\"üìä Input memory: {pixel_values.element_size() * pixel_values.numel() / 1024**2:.1f}MB\")\n",
    "        \n",
    "        # Task prompt for invoice processing\n",
    "        task_prompt = \"<s_cord-v2>\"\n",
    "        decoder_input_ids = processor.tokenizer(\n",
    "            task_prompt, \n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            decoder_input_ids = decoder_input_ids.to('cuda')\n",
    "        \n",
    "        # Generate with optimized parameters for T4\n",
    "        print(\"üîÑ Generating structured output...\")\n",
    "        with torch.no_grad():\n",
    "            # Use torch.cuda.amp for automatic mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model.generate(\n",
    "                    pixel_values,\n",
    "                    decoder_input_ids=decoder_input_ids,\n",
    "                    max_length=512,        # Reduced for faster inference\n",
    "                    early_stopping=True,\n",
    "                    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                    use_cache=True,\n",
    "                    num_beams=1,           # Greedy decoding for speed\n",
    "                    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "                    return_dict_in_generate=True,\n",
    "                )\n",
    "        \n",
    "        # Decode the output\n",
    "        sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "        sequence = sequence.replace(processor.tokenizer.eos_token, \"\")\n",
    "        sequence = sequence.replace(processor.tokenizer.pad_token, \"\")\n",
    "        sequence = sequence.replace(task_prompt, \"\").strip()\n",
    "        \n",
    "        print(f\"üìÑ Generated sequence length: {len(sequence)} characters\")\n",
    "        \n",
    "        # Parse the JSON output\n",
    "        try:\n",
    "            # Donut outputs JSON-like structure\n",
    "            invoice_data = json.loads(sequence)\n",
    "            invoice_data['extraction_method'] = 'donut'\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ö†Ô∏è Could not parse JSON, raw output: {sequence[:200]}...\")\n",
    "            # Fallback: extract key-value pairs manually\n",
    "            invoice_data = {\n",
    "                'extraction_method': 'donut',\n",
    "                'raw_output': sequence,\n",
    "                'parsing_error': 'Could not parse as JSON'\n",
    "            }\n",
    "            \n",
    "            # Try to extract basic fields from text\n",
    "            if 'total' in sequence.lower():\n",
    "                amount_match = re.search(r'([0-9,]+\\.?[0-9]*)', sequence)\n",
    "                if amount_match:\n",
    "                    invoice_data['total_amount'] = float(amount_match.group(1).replace(',', ''))\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        invoice_data['processing_time'] = processing_time\n",
    "        \n",
    "        return invoice_data\n",
    "    \n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(\"‚ùå GPU out of memory! Try reducing batch size or using CPU\")\n",
    "        clear_gpu_memory()\n",
    "        return {\n",
    "            'extraction_method': 'donut',\n",
    "            'error': 'GPU out of memory',\n",
    "            'processing_time': time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Donut processing error: {e}\")\n",
    "        return {\n",
    "            'extraction_method': 'donut',\n",
    "            'error': str(e),\n",
    "            'processing_time': time.time() - start_time\n",
    "        }\n",
    "\n",
    "# Test Donut on sample invoices\n",
    "if DONUT_AVAILABLE:\n",
    "    print(\"\\nüß™ TESTING DONUT APPROACH\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    donut_results = []\n",
    "    for i, invoice in enumerate(SAMPLE_INVOICES):\n",
    "        print(f\"\\nüìÑ Processing Invoice {i+1} with Donut\")\n",
    "        \n",
    "        result = extract_with_donut(invoice)\n",
    "        donut_results.append(result)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Donut Results:\")\n",
    "        if 'error' in result:\n",
    "            print(f\"   Error: {result['error']}\")\n",
    "        else:\n",
    "            # Display extracted fields\n",
    "            for key, value in result.items():\n",
    "                if key not in ['extraction_method', 'processing_time', 'raw_output']:\n",
    "                    print(f\"   {key}: {value}\")\n",
    "        \n",
    "        print(f\"   Processing Time: {result.get('processing_time', 0):.2f}s\")\n",
    "        \n",
    "        # Clear memory between runs\n",
    "        clear_gpu_memory()\n",
    "        time.sleep(0.5)  # Let GPU cool down\n",
    "    \n",
    "    print(\"\\nüìä Donut Approach Summary:\")\n",
    "    successful_runs = [r for r in donut_results if 'error' not in r]\n",
    "    if successful_runs:\n",
    "        avg_time = sum(r.get('processing_time', 0) for r in successful_runs) / len(successful_runs)\n",
    "        print(f\"   Average processing time: {avg_time:.2f}s per invoice\")\n",
    "        print(f\"   Success rate: {len(successful_runs)}/{len(donut_results)}\")\n",
    "        print(f\"   Memory intensive: ‚ö†Ô∏è High GPU usage\")\n",
    "        print(f\"   Structured output: ‚úÖ Direct JSON\")\n",
    "    else:\n",
    "        print(\"   No successful extractions\")\nelse:\n",
    "    print(\"\\n‚ö†Ô∏è Donut not available - skipping tests\")\n    \n",
    "    donut_results = []\n",
    "    print(\"\\nDonut benefits (when available):\")\n",
    "    print(\"   ‚Ä¢ No OCR preprocessing needed\")\n",
    "    print(\"   ‚Ä¢ Understands document layout\")\n",
    "    print(\"   ‚Ä¢ Direct structured output\")\n",
    "    print(\"   ‚Ä¢ State-of-the-art accuracy\")\n",
    "    print(\"\\nRequirements:\")\n",
    "    print(\"   ‚Ä¢ GPU with 8GB+ memory\")\n",
    "    print(\"   ‚Ä¢ CUDA-compatible environment\")\n",
    "    print(\"   ‚Ä¢ ~2-4GB model download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Approach 3 - BLIP-2 Visual Question Answering\n",
    "\n",
    "### Visual QA for Document Understanding\n",
    "\n",
    "**BLIP-2 Architecture:**\n",
    "```\n",
    "Image ‚Üí Vision Encoder ‚Üí Q-Former ‚Üí Language Model ‚Üí Answer\n",
    "       (ViT)          (BERT)      (OPT/T5)       (Text)\n",
    "```\n",
    "\n",
    "**Strategic Advantages:**\n",
    "- Natural language queries\n",
    "- Flexible field extraction\n",
    "- Good for ad-hoc questions\n",
    "- Handles complex reasoning\n",
    "\n",
    "**Question Design Patterns:**\n",
    "```python\n",
    "question_strategies = {\n",
    "    \"direct\": \"What is the total amount?\",\n",
    "    \"contextual\": \"What amount should be paid for this invoice?\",\n",
    "    \"conditional\": \"If this is an invoice, what is the due date?\",\n",
    "    \"verification\": \"Is this document an invoice or receipt?\"\n",
    "}\n",
    "```\n",
    "\n",
    "**T4 Optimization for BLIP-2:**\n",
    "- Use smaller OPT variant (2.7B vs 6.7B)\n",
    "- FP16 precision\n",
    "- Sequential question processing\n",
    "- Aggressive memory cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# BLIP-2 for Visual Question Answering\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "print(\"üîç Loading BLIP-2 model...\")\n",
    "print(\"Using smaller variant optimized for T4 GPU\")\n",
    "\n",
    "# Check memory before loading\n",
    "mem_before = get_gpu_memory()\n",
    "print(f\"Memory before BLIP-2: {mem_before['free']:.1f}GB available\")\n",
    "\n",
    "try:\n",
    "    # Load BLIP-2 with smaller variant for T4\n",
    "    processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "    model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip2-opt-2.7b\",\n",
    "        torch_dtype=torch.float16,  # Use FP16 for memory efficiency\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to('cuda')\n",
    "        print(\"‚úÖ BLIP-2 loaded on GPU with FP16\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è BLIP-2 loaded on CPU\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Check memory usage\n",
    "    mem_after = get_gpu_memory()\n",
    "    model_memory = mem_after['allocated'] - mem_before['allocated']\n",
    "    print(f\"üìä BLIP-2 memory usage: {model_memory:.1f}GB\")\n",
    "    print(f\"üìä Remaining memory: {mem_after['free']:.1f}GB\")\n",
    "    \n",
    "    BLIP2_AVAILABLE = True\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå Error loading BLIP-2: {e}\")\n",
    "    print(\"This might be due to insufficient GPU memory\")\n",
    "    BLIP2_AVAILABLE = False\n",
    "\n",
    "@monitor_memory(\"BLIP-2 Processing\")\n",
    "def extract_with_qa(image, questions):\n",
    "    \"\"\"Extract via visual question answering\"\"\"\n",
    "    if not BLIP2_AVAILABLE:\n",
    "        return {\n",
    "            'extraction_method': 'blip2',\n",
    "            'error': 'BLIP-2 model not available',\n",
    "            'processing_time': 0\n",
    "        }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = {'extraction_method': 'blip2'}\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîç Asking {len(questions)} questions about the document\")\n",
    "        \n",
    "        for field, question in questions.items():\n",
    "            print(f\"   Q: {question}\")\n",
    "            \n",
    "            # Process question with image\n",
    "            inputs = processor(image, text=question, return_tensors=\"pt\")\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate answer\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    generated_ids = model.generate(\n",
    "                        **inputs, \n",
    "                        max_new_tokens=50,     # Limit for faster generation\n",
    "                        num_beams=2,           # Reduced beams for speed\n",
    "                        early_stopping=True,\n",
    "                        do_sample=False        # Deterministic output\n",
    "                    )\n",
    "            \n",
    "            # Decode answer\n",
    "            answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            answer = answer.strip()\n",
    "            \n",
    "            print(f\"   A: {answer}\")\n",
    "            results[field] = answer\n",
    "            \n",
    "            # Clean up intermediate tensors\n",
    "            del inputs, generated_ids\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        results['processing_time'] = processing_time\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(\"‚ùå GPU out of memory during BLIP-2 processing!\")\n",
    "        clear_gpu_memory()\n",
    "        return {\n",
    "            'extraction_method': 'blip2',\n",
    "            'error': 'GPU out of memory',\n",
    "            'processing_time': time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå BLIP-2 processing error: {e}\")\n",
    "        return {\n",
    "            'extraction_method': 'blip2',\n",
    "            'error': str(e),\n",
    "            'processing_time': time.time() - start_time\n",
    "        }\n",
    "\n",
    "# Define comprehensive extraction questions\n",
    "extraction_questions = {\n",
    "    'document_type': \"Is this an invoice, receipt, or other document?\",\n",
    "    'invoice_number': \"What is the invoice number or document number?\",\n",
    "    'total_amount': \"What is the total amount to be paid?\",\n",
    "    'currency': \"What currency is used in this document?\",\n",
    "    'date': \"What is the invoice date or document date?\",\n",
    "    'due_date': \"When is the payment due?\",\n",
    "    'vendor': \"Who is the vendor, seller, or company issuing this document?\",\n",
    "    'buyer': \"Who is the buyer or customer?\",\n",
    "    'payment_terms': \"What are the payment terms?\"\n",
    "}\n",
    "\n",
    "# Test BLIP-2 on sample invoices\n",
    "if BLIP2_AVAILABLE:\n",
    "    print(\"\\nüß™ TESTING BLIP-2 APPROACH\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    blip2_results = []\n",
    "    for i, invoice in enumerate(SAMPLE_INVOICES):\n",
    "        print(f\"\\nüìÑ Processing Invoice {i+1} with BLIP-2\")\n",
    "        \n",
    "        result = extract_with_qa(invoice, extraction_questions)\n",
    "        blip2_results.append(result)\n",
    "        \n",
    "        print(f\"\\n‚úÖ BLIP-2 Results:\")\n",
    "        if 'error' in result:\n",
    "            print(f\"   Error: {result['error']}\")\n",
    "        else:\n",
    "            # Display key findings\n",
    "            key_fields = ['document_type', 'invoice_number', 'total_amount', 'vendor', 'date']\n",
    "            for field in key_fields:\n",
    "                value = result.get(field, 'Not found')\n",
    "                print(f\"   {field}: {value}\")\n",
    "        \n",
    "        print(f\"   Processing Time: {result.get('processing_time', 0):.2f}s\")\n",
    "        \n",
    "        # Aggressive memory cleanup\n",
    "        clear_gpu_memory()\n",
    "        time.sleep(1)  # Let GPU cool down\n",
    "    \n",
    "    print(\"\\nüìä BLIP-2 Approach Summary:\")\n",
    "    successful_runs = [r for r in blip2_results if 'error' not in r]\n",
    "    if successful_runs:\n",
    "        avg_time = sum(r.get('processing_time', 0) for r in successful_runs) / len(successful_runs)\n",
    "        print(f\"   Average processing time: {avg_time:.2f}s per invoice\")\n",
    "        print(f\"   Success rate: {len(successful_runs)}/{len(blip2_results)}\")\n",
    "        print(f\"   Memory usage: ‚ö†Ô∏è High GPU usage\")\n",
    "        print(f\"   Flexibility: ‚úÖ Natural language queries\")\n",
    "        print(f\"   Reasoning: ‚úÖ Can handle complex questions\")\n",
    "    else:\n",
    "        print(\"   No successful extractions\")\nelse:\n",
    "    print(\"\\n‚ö†Ô∏è BLIP-2 not available - skipping tests\")\n",
    "    \n",
    "    blip2_results = []\n",
    "    print(\"\\nBLIP-2 benefits (when available):\")\n",
    "    print(\"   ‚Ä¢ Natural language questions\")\n",
    "    print(\"   ‚Ä¢ Flexible field extraction\")\n",
    "    print(\"   ‚Ä¢ Handles complex reasoning\")\n",
    "    print(\"   ‚Ä¢ Good for ad-hoc queries\")\n",
    "    print(\"\\nRequirements:\")\n",
    "    print(\"   ‚Ä¢ GPU with 10GB+ memory\")\n",
    "    print(\"   ‚Ä¢ CUDA-compatible environment\")\n",
    "    print(\"   ‚Ä¢ ~5-6GB model download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Memory Optimization Techniques\n",
    "\n",
    "### Production Memory Management\n",
    "\n",
    "**Memory Optimization Hierarchy:**\n",
    "```python\n",
    "optimization_levels = {\n",
    "    \"level_1_basic\": [\n",
    "        \"Clear GPU cache between models\",\n",
    "        \"Use torch.no_grad() for inference\",\n",
    "        \"Delete intermediate tensors\"\n",
    "    ],\n",
    "    \"level_2_precision\": [\n",
    "        \"Mixed precision (FP16)\",\n",
    "        \"Model quantization (8-bit/4-bit)\",\n",
    "        \"Gradient checkpointing\"\n",
    "    ],\n",
    "    \"level_3_architecture\": [\n",
    "        \"Model pruning\",\n",
    "        \"Knowledge distillation\",\n",
    "        \"Dynamic batching\"\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**T4 GPU Best Practices:**\n",
    "- Reserve 2GB for system overhead\n",
    "- Use smaller model variants when possible\n",
    "- Implement circuit breakers for OOM errors\n",
    "- Monitor memory usage continuously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Advanced Memory Optimization Techniques\n",
    "import gc\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class GPUMemoryManager:\n",
    "    \"\"\"Advanced GPU memory management for production\"\"\"\n",
    "    \n",
    "    def __init__(self, reserve_gb=2.0):\n",
    "        self.reserve_gb = reserve_gb\n",
    "        self.peak_memory = 0\n",
    "        self.allocation_history = []\n",
    "    \n",
    "    def get_memory_info(self):\n",
    "        \"\"\"Detailed memory information\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {'error': 'No GPU available'}\n",
    "        \n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        \n",
    "        self.peak_memory = max(self.peak_memory, allocated)\n",
    "        \n",
    "        return {\n",
    "            'allocated': allocated,\n",
    "            'reserved': reserved,\n",
    "            'free': total - reserved,\n",
    "            'total': total,\n",
    "            'usable': total - self.reserve_gb,\n",
    "            'peak_usage': self.peak_memory,\n",
    "            'utilization': allocated / total * 100\n",
    "        }\n",
    "    \n",
    "    def can_allocate(self, required_gb):\n",
    "        \"\"\"Check if we can allocate required memory\"\"\"\n",
    "        info = self.get_memory_info()\n",
    "        if 'error' in info:\n",
    "            return False\n",
    "        return info['free'] >= required_gb\n",
    "    \n",
    "    def aggressive_cleanup(self):\n",
    "        \"\"\"Aggressive memory cleanup\"\"\"\n",
    "        # Python garbage collection\n",
    "        collected = gc.collect()\n",
    "        \n",
    "        # PyTorch cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        print(f\"üßπ Cleaned up {collected} Python objects\")\n",
    "        return collected\n",
    "    \n",
    "    @contextmanager\n",
    "    def memory_monitor(self, operation_name):\n",
    "        \"\"\"Context manager for monitoring memory usage\"\"\"\n",
    "        before = self.get_memory_info()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            print(f\"üîç Starting {operation_name}\")\n",
    "            print(f\"   Memory before: {before['allocated']:.2f}GB / {before['total']:.2f}GB\")\n",
    "            yield self\n",
    "        finally:\n",
    "            after = self.get_memory_info()\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            memory_delta = after['allocated'] - before['allocated']\n",
    "            \n",
    "            print(f\"   Memory after: {after['allocated']:.2f}GB / {after['total']:.2f}GB\")\n",
    "            print(f\"   Memory delta: {memory_delta:+.2f}GB\")\n",
    "            print(f\"   Peak usage: {after['peak_usage']:.2f}GB\")\n",
    "            print(f\"   Duration: {elapsed:.2f}s\")\n",
    "            \n",
    "            # Log allocation history\n",
    "            self.allocation_history.append({\n",
    "                'operation': operation_name,\n",
    "                'memory_delta': memory_delta,\n",
    "                'peak_memory': after['peak_usage'],\n",
    "                'duration': elapsed\n",
    "            })\n",
    "\n",
    "def load_quantized_model(model_name, quantization_bits=8):\n",
    "    \"\"\"Load model with quantization for memory efficiency\"\"\"\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        \n",
    "        if quantization_bits == 8:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                llm_int8_threshold=6.0,\n",
    "                llm_int8_has_fp16_weight=False\n",
    "            )\n",
    "        elif quantization_bits == 4:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        # Load with quantization\n",
    "        from transformers import AutoModel\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {model_name} with {quantization_bits}-bit quantization\")\n",
    "        return model\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå BitsAndBytes not available for quantization\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading quantized model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Initialize memory manager\n",
    "memory_manager = GPUMemoryManager(reserve_gb=2.0)\n",
    "\n",
    "print(\"üîß MEMORY OPTIMIZATION DEMO\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Current memory status\n",
    "current_mem = memory_manager.get_memory_info()\n",
    "if 'error' not in current_mem:\n",
    "    print(f\"\\nüìä Current Memory Status:\")\n",
    "    print(f\"   Total: {current_mem['total']:.1f}GB\")\n",
    "    print(f\"   Allocated: {current_mem['allocated']:.2f}GB\")\n",
    "    print(f\"   Free: {current_mem['free']:.2f}GB\")\n",
    "    print(f\"   Usable: {current_mem['usable']:.2f}GB (after reserve)\")\n",
    "    print(f\"   Utilization: {current_mem['utilization']:.1f}%\")\n",
    "    print(f\"   Peak usage: {current_mem['peak_usage']:.2f}GB\")\n",
    "    \n",
    "    # Memory recommendations\n",
    "    print(f\"\\nüí° Memory Recommendations:\")\n",
    "    if current_mem['utilization'] > 80:\n",
    "        print(\"   ‚ö†Ô∏è High memory usage - consider cleanup\")\n",
    "    elif current_mem['utilization'] > 60:\n",
    "        print(\"   ‚ö†Ô∏è Moderate usage - monitor closely\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Good memory availability\")\n",
    "    \n",
    "    if current_mem['free'] < 4.0:\n",
    "        print(\"   üí° Consider using smaller models\")\n",
    "        print(\"   üí° Enable mixed precision (FP16)\")\n",
    "        print(\"   üí° Use gradient checkpointing\")\n",
    "    \n",
    "    if current_mem['total'] <= 16:\n",
    "        print(\"   üí° T4 GPU detected - use optimized settings\")\n",
    "        print(\"   üí° Batch size: 1-2 for large models\")\n",
    "        print(\"   üí° Prefer base models over large variants\")\nelse:\n",
    "    print(\"   No GPU available for memory optimization\")\n",
    "\n",
    "# Demonstrate memory cleanup\n",
    "print(f\"\\nüßπ Demonstrating Memory Cleanup:\")\n",
    "before_cleanup = memory_manager.get_memory_info()\n",
    "cleaned_objects = memory_manager.aggressive_cleanup()\n",
    "after_cleanup = memory_manager.get_memory_info()\n",
    "\n",
    "if 'error' not in before_cleanup:\n",
    "    memory_freed = before_cleanup['allocated'] - after_cleanup['allocated']\n",
    "    print(f\"   Memory freed: {memory_freed:.3f}GB\")\n",
    "    print(f\"   Objects cleaned: {cleaned_objects}\")\n",
    "\n",
    "# Show allocation history\n",
    "if memory_manager.allocation_history:\n",
    "    print(f\"\\nüìà Memory Allocation History:\")\n",
    "    for entry in memory_manager.allocation_history[-3:]:  # Last 3 operations\n",
    "        print(f\"   {entry['operation']}: {entry['memory_delta']:+.2f}GB, {entry['duration']:.2f}s\")\n",
    "\n",
    "print(f\"\\nüéØ Production Memory Tips:\")\n",
    "print(f\"   ‚Ä¢ Monitor GPU memory usage continuously\")\n",
    "print(f\"   ‚Ä¢ Use memory_monitor context manager\")\n",
    "print(f\"   ‚Ä¢ Clear cache between model loads\")\n",
    "print(f\"   ‚Ä¢ Implement OOM error recovery\")\n",
    "print(f\"   ‚Ä¢ Consider model quantization for large models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Comprehensive Comparison\n",
    "\n",
    "### Model Selection Strategy\n",
    "\n",
    "**Decision Matrix:**\n",
    "```python\n",
    "selection_criteria = {\n",
    "    \"high_volume_production\": \"OCR + Rules (fastest)\",\n",
    "    \"high_accuracy_needed\": \"Donut (best structure understanding)\", \n",
    "    \"flexible_queries\": \"BLIP-2 (natural language questions)\",\n",
    "    \"limited_gpu_memory\": \"OCR + lightweight NLP\",\n",
    "    \"multilingual_docs\": \"EasyOCR + language detection\",\n",
    "    \"poor_quality_scans\": \"OCR + LLM enhancement\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Cost-Benefit Analysis:**\n",
    "- **OCR**: $0.001 per invoice, 85% accuracy\n",
    "- **Donut**: $0.01 per invoice, 95% accuracy  \n",
    "- **BLIP-2**: $0.02 per invoice, 92% accuracy\n",
    "- **Hybrid**: $0.005 per invoice, 97% accuracy\n",
    "\n",
    "**Performance Benchmarks:**\n",
    "```\n",
    "Method     Speed    Memory   Accuracy   Cost\n",
    "OCR        ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê   ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê    ‚≠ê‚≠ê‚≠ê      ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "Donut      ‚≠ê‚≠ê‚≠ê     ‚≠ê‚≠ê       ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê    ‚≠ê‚≠ê‚≠ê\n",
    "BLIP-2     ‚≠ê‚≠ê       ‚≠ê        ‚≠ê‚≠ê‚≠ê‚≠ê     ‚≠ê‚≠ê\n",
    "Hybrid     ‚≠ê‚≠ê‚≠ê‚≠ê    ‚≠ê‚≠ê‚≠ê      ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê    ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Comprehensive comparison of all approaches\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üìä COMPREHENSIVE APPROACH COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Compile results from all approaches\n",
    "comparison_data = []\n",
    "\n",
    "# OCR results\n",
    "if ocr_results:\n",
    "    for i, result in enumerate(ocr_results):\n",
    "        comparison_data.append({\n",
    "            'Invoice': f\"Invoice_{i+1}\",\n",
    "            'Method': 'OCR',\n",
    "            'Processing_Time': result.get('processing_time', 0),\n",
    "            'Success': 'error' not in result,\n",
    "            'Invoice_Number': result.get('invoice_number', 'Not found'),\n",
    "            'Total_Amount': result.get('total_amount', 'Not found'),\n",
    "            'Date': result.get('date', 'Not found'),\n",
    "            'Confidence': result.get('confidence', 0)\n",
    "        })\n",
    "\n",
    "# Donut results\n",
    "if donut_results:\n",
    "    for i, result in enumerate(donut_results):\n",
    "        comparison_data.append({\n",
    "            'Invoice': f\"Invoice_{i+1}\",\n",
    "            'Method': 'Donut',\n",
    "            'Processing_Time': result.get('processing_time', 0),\n",
    "            'Success': 'error' not in result,\n",
    "            'Invoice_Number': str(result.get('invoice_number', 'Not found')),\n",
    "            'Total_Amount': str(result.get('total_amount', 'Not found')),\n",
    "            'Date': str(result.get('date', 'Not found')),\n",
    "            'Confidence': 0.9 if 'error' not in result else 0\n",
    "        })\n",
    "\n",
    "# BLIP-2 results\n",
    "if blip2_results:\n",
    "    for i, result in enumerate(blip2_results):\n",
    "        comparison_data.append({\n",
    "            'Invoice': f\"Invoice_{i+1}\",\n",
    "            'Method': 'BLIP-2',\n",
    "            'Processing_Time': result.get('processing_time', 0),\n",
    "            'Success': 'error' not in result,\n",
    "            'Invoice_Number': result.get('invoice_number', 'Not found'),\n",
    "            'Total_Amount': result.get('total_amount', 'Not found'),\n",
    "            'Date': result.get('date', 'Not found'),\n",
    "            'Confidence': 0.8 if 'error' not in result else 0\n",
    "        })\n",
    "\n",
    "if comparison_data:\n",
    "    # Create comparison DataFrame\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\nüìã DETAILED RESULTS:\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìà PERFORMANCE SUMMARY:\")\n",
    "    summary = df.groupby('Method').agg({\n",
    "        'Processing_Time': ['mean', 'std'],\n",
    "        'Success': 'mean',\n",
    "        'Confidence': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(summary)\n",
    "    \n",
    "    # Method comparison\n",
    "    print(\"\\nüèÜ METHOD RANKINGS:\")\n",
    "    method_stats = []\n",
    "    \n",
    "    for method in df['Method'].unique():\n",
    "        method_data = df[df['Method'] == method]\n",
    "        \n",
    "        avg_time = method_data['Processing_Time'].mean()\n",
    "        success_rate = method_data['Success'].mean()\n",
    "        avg_confidence = method_data['Confidence'].mean()\n",
    "        \n",
    "        # Calculate composite score\n",
    "        # Lower time is better, higher success/confidence is better\n",
    "        speed_score = 1 / (avg_time + 0.1)  # Add small constant to avoid division by zero\n",
    "        quality_score = (success_rate + avg_confidence) / 2\n",
    "        composite_score = (speed_score * 0.3 + quality_score * 0.7)\n",
    "        \n",
    "        method_stats.append({\n",
    "            'Method': method,\n",
    "            'Avg_Time': avg_time,\n",
    "            'Success_Rate': success_rate,\n",
    "            'Avg_Confidence': avg_confidence,\n",
    "            'Composite_Score': composite_score\n",
    "        })\n",
    "    \n",
    "    method_df = pd.DataFrame(method_stats)\n",
    "    method_df = method_df.sort_values('Composite_Score', ascending=False)\n",
    "    \n",
    "    print(\"\\nRanking (best to worst):\")\n",
    "    for i, row in method_df.iterrows():\n",
    "        print(f\"{len(method_df) - list(method_df.index).index(i)}. {row['Method']}\")\n",
    "        print(f\"   Speed: {row['Avg_Time']:.2f}s\")\n",
    "        print(f\"   Success: {row['Success_Rate']:.1%}\")\n",
    "        print(f\"   Confidence: {row['Avg_Confidence']:.2f}\")\n",
    "        print(f\"   Score: {row['Composite_Score']:.3f}\")\n",
    "        print()\n",
    "\nelse:\n",
    "    print(\"\\n‚ö†Ô∏è No results available for comparison\")\n",
    "    print(\"This could be due to:\")\n",
    "    print(\"   ‚Ä¢ GPU memory limitations\")\n",
    "    print(\"   ‚Ä¢ Model loading failures\") \n",
    "    print(\"   ‚Ä¢ Missing dependencies\")\n",
    "\n",
    "# Production recommendations\n",
    "print(\"\\nüéØ PRODUCTION RECOMMENDATIONS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "recommendations = {\n",
    "    \"High Volume (>10K invoices/day)\": {\n",
    "        \"approach\": \"OCR + Rules Engine\",\n",
    "        \"reason\": \"Fastest processing, lowest cost per invoice\",\n",
    "        \"optimization\": \"Parallel processing, GPU-accelerated OCR\"\n",
    "    },\n",
    "    \"High Accuracy Required (>95%)\": {\n",
    "        \"approach\": \"Donut + Validation\",\n",
    "        \"reason\": \"Best structural understanding, direct JSON output\",\n",
    "        \"optimization\": \"Model quantization, batch processing\"\n",
    "    },\n",
    "    \"Flexible Queries\": {\n",
    "        \"approach\": \"BLIP-2 QA\",\n",
    "        \"reason\": \"Natural language questions, adaptable to new fields\",\n",
    "        \"optimization\": \"Question caching, sequential processing\"\n",
    "    },\n",
    "    \"Limited GPU Memory (<8GB)\": {\n",
    "        \"approach\": \"OCR + Small NLP Models\",\n",
    "        \"reason\": \"Works within memory constraints\",\n",
    "        \"optimization\": \"CPU fallback, model quantization\"\n",
    "    },\n",
    "    \"Production Hybrid\": {\n",
    "        \"approach\": \"Smart Router + Multiple Models\",\n",
    "        \"reason\": \"Best of all worlds, quality-based routing\",\n",
    "        \"optimization\": \"Quality assessment, model switching\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for scenario, details in recommendations.items():\n",
    "    print(f\"\\nüìã {scenario}:\")\n",
    "    print(f\"   Approach: {details['approach']}\")\n",
    "    print(f\"   Reason: {details['reason']}\")\n",
    "    print(f\"   Optimization: {details['optimization']}\")\n",
    "\n",
    "# Cost analysis\n",
    "print(\"\\nüí∞ COST ANALYSIS (per 1000 invoices):\")\n",
    "cost_analysis = {\n",
    "    \"OCR\": {\"compute\": \"$1\", \"accuracy\": \"85%\", \"manual_review\": \"$225\", \"total\": \"$226\"},\n",
    "    \"Donut\": {\"compute\": \"$10\", \"accuracy\": \"95%\", \"manual_review\": \"$75\", \"total\": \"$85\"},\n",
    "    \"BLIP-2\": {\"compute\": \"$20\", \"accuracy\": \"92%\", \"manual_review\": \"$120\", \"total\": \"$140\"},\n",
    "    \"Hybrid\": {\"compute\": \"$5\", \"accuracy\": \"97%\", \"manual_review\": \"$45\", \"total\": \"$50\"}\n",
    "}\n",
    "\n",
    "for method, costs in cost_analysis.items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"   Compute: {costs['compute']}\")\n",
    "    print(f\"   Accuracy: {costs['accuracy']}\")\n",
    "    print(f\"   Manual review: {costs['manual_review']}\")\n",
    "    print(f\"   Total cost: {costs['total']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Production Pipeline with Smart Routing\n",
    "\n",
    "### Intelligent Model Selection\n",
    "\n",
    "**Routing Decision Tree:**\n",
    "```python\n",
    "def route_to_best_model(image_properties):\n",
    "    if image_properties.quality == \"high\" and image_properties.layout == \"standard\":\n",
    "        return \"donut\"  # Fast, accurate for good quality\n",
    "    elif image_properties.quality == \"low\" or image_properties.skewed:\n",
    "        return \"ocr_enhanced\"  # OCR + LLM cleanup\n",
    "    elif image_properties.requires_reasoning:\n",
    "        return \"blip2\"  # Complex questions\n",
    "    else:\n",
    "        return \"ocr\"  # Default fallback\n",
    "```\n",
    "\n",
    "**Ensemble Strategy:**\n",
    "```python\n",
    "# Use multiple models and combine results\n",
    "results = {\n",
    "    \"primary\": extract_with_primary_model(image),\n",
    "    \"secondary\": extract_with_secondary_model(image)\n",
    "}\n",
    "\n",
    "# Confidence-weighted combination\n",
    "final_result = combine_with_confidence(results)\n",
    "```\n",
    "\n",
    "**Quality Gating:**\n",
    "- Confidence threshold: 85%\n",
    "- Field completeness: 80%\n",
    "- Cross-validation: Models agree on key fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Production pipeline with smart routing\n",
    "import hashlib\n",
    "from enum import Enum\n",
    "\n",
    "class ImageQuality(Enum):\n",
    "    HIGH = \"high\"\n",
    "    MEDIUM = \"medium\"\n",
    "    LOW = \"low\"\n",
    "\n",
    "class ExtractionMethod(Enum):\n",
    "    OCR = \"ocr\"\n",
    "    DONUT = \"donut\"\n",
    "    BLIP2 = \"blip2\"\n",
    "    HYBRID = \"hybrid\"\n",
    "\n",
    "class SmartInvoiceExtractor:\n",
    "    \"\"\"Production-ready invoice extractor with intelligent routing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.memory_manager = memory_manager\n",
    "        self.extraction_cache = {}  # Cache results\n",
    "        self.performance_history = []\n",
    "        \n",
    "        # Quality thresholds\n",
    "        self.confidence_threshold = 0.85\n",
    "        self.completeness_threshold = 0.80\n",
    "        \n",
    "    def assess_image_quality(self, image):\n",
    "        \"\"\"Assess image quality for routing decisions\"\"\"\n",
    "        width, height = image.size\n",
    "        pixels = width * height\n",
    "        \n",
    "        # Convert to numpy for analysis\n",
    "        img_array = np.array(image.convert('L'))  # Grayscale\n",
    "        \n",
    "        # Calculate metrics\n",
    "        contrast = img_array.std()  # Standard deviation as contrast measure\n",
    "        brightness = img_array.mean()\n",
    "        \n",
    "        # Estimate quality\n",
    "        quality_score = 0\n",
    "        \n",
    "        # Resolution score\n",
    "        if pixels > 1000000:  # > 1MP\n",
    "            quality_score += 30\n",
    "        elif pixels > 500000:  # > 0.5MP\n",
    "            quality_score += 20\n",
    "        else:\n",
    "            quality_score += 10\n",
    "        \n",
    "        # Contrast score\n",
    "        if contrast > 50:\n",
    "            quality_score += 30\n",
    "        elif contrast > 30:\n",
    "            quality_score += 20\n",
    "        else:\n",
    "            quality_score += 10\n",
    "        \n",
    "        # Brightness score (prefer moderate brightness)\n",
    "        if 100 < brightness < 200:\n",
    "            quality_score += 40\n",
    "        elif 80 < brightness < 220:\n",
    "            quality_score += 30\n",
    "        else:\n",
    "            quality_score += 10\n",
    "        \n",
    "        # Classify quality\n",
    "        if quality_score >= 80:\n",
    "            quality = ImageQuality.HIGH\n",
    "        elif quality_score >= 60:\n",
    "            quality = ImageQuality.MEDIUM\n",
    "        else:\n",
    "            quality = ImageQuality.LOW\n",
    "        \n",
    "        return {\n",
    "            'quality': quality,\n",
    "            'score': quality_score,\n",
    "            'pixels': pixels,\n",
    "            'contrast': contrast,\n",
    "            'brightness': brightness,\n",
    "            'dimensions': (width, height)\n",
    "        }\n",
    "    \n",
    "    def route_extraction_method(self, image_assessment, available_methods):\n",
    "        \"\"\"Intelligently route to best extraction method\"\"\"\n",
    "        quality = image_assessment['quality']\n",
    "        memory_available = self.memory_manager.can_allocate(4.0)  # 4GB threshold\n",
    "        \n",
    "        # Routing logic\n",
    "        if quality == ImageQuality.HIGH and DONUT_AVAILABLE and memory_available:\n",
    "            return ExtractionMethod.DONUT\n",
    "        elif quality == ImageQuality.LOW:\n",
    "            return ExtractionMethod.OCR  # OCR handles poor quality better\n",
    "        elif BLIP2_AVAILABLE and memory_available:\n",
    "            return ExtractionMethod.BLIP2  # Flexible for medium quality\n",
    "        else:\n",
    "            return ExtractionMethod.OCR  # Safe fallback\n",
    "    \n",
    "    def extract_with_fallback(self, image, primary_method):\n",
    "        \"\"\"Extract with fallback to other methods\"\"\"\n",
    "        extraction_results = []\n",
    "        \n",
    "        # Try primary method first\n",
    "        print(f\"üéØ Trying primary method: {primary_method.value}\")\n",
    "        \n",
    "        with self.memory_manager.memory_monitor(f\"{primary_method.value}_extraction\"):\n",
    "            if primary_method == ExtractionMethod.OCR:\n",
    "                result = extract_with_ocr(image)\n",
    "            elif primary_method == ExtractionMethod.DONUT and DONUT_AVAILABLE:\n",
    "                result = extract_with_donut(image)\n",
    "            elif primary_method == ExtractionMethod.BLIP2 and BLIP2_AVAILABLE:\n",
    "                result = extract_with_qa(image, extraction_questions)\n",
    "            else:\n",
    "                result = {'error': f'Method {primary_method.value} not available'}\n",
    "        \n",
    "        result['method'] = primary_method.value\n",
    "        extraction_results.append(result)\n",
    "        \n",
    "        # Check if primary method was successful\n",
    "        if 'error' in result or self.calculate_completeness(result) < self.completeness_threshold:\n",
    "            print(f\"‚ö†Ô∏è Primary method failed or low quality, trying fallback\")\n",
    "            \n",
    "            # Try OCR as fallback (always available)\n",
    "            if primary_method != ExtractionMethod.OCR:\n",
    "                print(f\"üîÑ Trying fallback: OCR\")\n",
    "                fallback_result = extract_with_ocr(image)\n",
    "                fallback_result['method'] = 'ocr_fallback'\n",
    "                extraction_results.append(fallback_result)\n",
    "        \n",
    "        return extraction_results\n",
    "    \n",
    "    def calculate_completeness(self, result):\n",
    "        \"\"\"Calculate how complete the extraction is\"\"\"\n",
    "        if 'error' in result:\n",
    "            return 0.0\n",
    "        \n",
    "        required_fields = ['invoice_number', 'total_amount', 'date', 'vendor']\n",
    "        found_fields = 0\n",
    "        \n",
    "        for field in required_fields:\n",
    "            if field in result and result[field] not in [None, 'Not found', '']:\n",
    "                found_fields += 1\n",
    "        \n",
    "        return found_fields / len(required_fields)\n",
    "    \n",
    "    def combine_results(self, extraction_results):\n",
    "        \"\"\"Combine results from multiple extraction methods\"\"\"\n",
    "        if not extraction_results:\n",
    "            return {'error': 'No extraction results'}\n",
    "        \n",
    "        # Choose best result based on completeness and confidence\n",
    "        best_result = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for result in extraction_results:\n",
    "            if 'error' in result:\n",
    "                continue\n",
    "            \n",
    "            completeness = self.calculate_completeness(result)\n",
    "            confidence = result.get('confidence', 0.5)  # Default confidence\n",
    "            \n",
    "            # Combined score\n",
    "            score = (completeness * 0.7) + (confidence * 0.3)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_result = result\n",
    "        \n",
    "        if best_result:\n",
    "            best_result['completeness'] = self.calculate_completeness(best_result)\n",
    "            best_result['combined_score'] = best_score\n",
    "            best_result['extraction_methods_tried'] = [r.get('method', 'unknown') for r in extraction_results]\n",
    "            return best_result\n",
    "        else:\n",
    "            return {'error': 'All extraction methods failed'}\n",
    "    \n",
    "    def get_cache_key(self, image):\n",
    "        \"\"\"Generate cache key for image\"\"\"\n",
    "        img_bytes = image.tobytes()\n",
    "        return hashlib.md5(img_bytes).hexdigest()[:16]\n",
    "    \n",
    "    def extract(self, image, use_cache=True):\n",
    "        \"\"\"Main extraction method with intelligent routing\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check cache first\n",
    "        cache_key = self.get_cache_key(image) if use_cache else None\n",
    "        if cache_key and cache_key in self.extraction_cache:\n",
    "            print(\"üíæ Using cached result\")\n",
    "            cached_result = self.extraction_cache[cache_key].copy()\n",
    "            cached_result['cache_hit'] = True\n",
    "            return cached_result\n",
    "        \n",
    "        # Assess image quality\n",
    "        print(\"üîç Assessing image quality...\")\n",
    "        assessment = self.assess_image_quality(image)\n",
    "        print(f\"   Quality: {assessment['quality'].value} (score: {assessment['score']})\")\n",
    "        print(f\"   Resolution: {assessment['dimensions'][0]}x{assessment['dimensions'][1]}\")\n",
    "        print(f\"   Contrast: {assessment['contrast']:.1f}\")\n",
    "        \n",
    "        # Route to best method\n",
    "        available_methods = [ExtractionMethod.OCR]\n",
    "        if DONUT_AVAILABLE:\n",
    "            available_methods.append(ExtractionMethod.DONUT)\n",
    "        if BLIP2_AVAILABLE:\n",
    "            available_methods.append(ExtractionMethod.BLIP2)\n",
    "        \n",
    "        primary_method = self.route_extraction_method(assessment, available_methods)\n",
    "        print(f\"üéØ Selected method: {primary_method.value}\")\n",
    "        \n",
    "        # Extract with fallback\n",
    "        extraction_results = self.extract_with_fallback(image, primary_method)\n",
    "        \n",
    "        # Combine results\n",
    "        final_result = self.combine_results(extraction_results)\n",
    "        \n",
    "        # Add metadata\n",
    "        total_time = time.time() - start_time\n",
    "        final_result.update({\n",
    "            'total_processing_time': total_time,\n",
    "            'image_assessment': assessment,\n",
    "            'primary_method': primary_method.value,\n",
    "            'cache_hit': False\n",
    "        })\n",
    "        \n",
    "        # Cache successful results\n",
    "        if use_cache and cache_key and 'error' not in final_result:\n",
    "            self.extraction_cache[cache_key] = final_result.copy()\n",
    "        \n",
    "        # Log performance\n",
    "        self.performance_history.append({\n",
    "            'timestamp': time.time(),\n",
    "            'method': primary_method.value,\n",
    "            'quality': assessment['quality'].value,\n",
    "            'processing_time': total_time,\n",
    "            'success': 'error' not in final_result,\n",
    "            'completeness': final_result.get('completeness', 0)\n",
    "        })\n",
    "        \n",
    "        return final_result\n",
    "\n",
    "# Test the smart extractor\n",
    "print(\"ü§ñ TESTING SMART INVOICE EXTRACTOR\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "smart_extractor = SmartInvoiceExtractor()\n",
    "\n",
    "for i, invoice in enumerate(SAMPLE_INVOICES):\n",
    "    print(f\"\\nüìÑ Processing Invoice {i+1} with Smart Extractor\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    result = smart_extractor.extract(invoice)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Smart Extraction Results:\")\n",
    "    print(f\"   Primary Method: {result.get('primary_method', 'Unknown')}\")\n",
    "    print(f\"   Methods Tried: {result.get('extraction_methods_tried', [])}\")\n",
    "    print(f\"   Quality Assessment: {result.get('image_assessment', {}).get('quality', 'Unknown')}\")\n",
    "    print(f\"   Completeness: {result.get('completeness', 0):.1%}\")\n",
    "    print(f\"   Processing Time: {result.get('total_processing_time', 0):.2f}s\")\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        print(f\"   Invoice Number: {result.get('invoice_number', 'Not found')}\")\n",
    "        print(f\"   Total Amount: {result.get('total_amount', 'Not found')}\")\n",
    "        print(f\"   Date: {result.get('date', 'Not found')}\")\n",
    "        print(f\"   Vendor: {result.get('vendor', 'Not found')}\")\n",
    "    else:\n",
    "        print(f\"   Error: {result['error']}\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    smart_extractor.memory_manager.aggressive_cleanup()\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\nüìä SMART EXTRACTOR PERFORMANCE:\")\n",
    "if smart_extractor.performance_history:\n",
    "    avg_time = sum(p['processing_time'] for p in smart_extractor.performance_history) / len(smart_extractor.performance_history)\n",
    "    success_rate = sum(p['success'] for p in smart_extractor.performance_history) / len(smart_extractor.performance_history)\n",
    "    avg_completeness = sum(p['completeness'] for p in smart_extractor.performance_history) / len(smart_extractor.performance_history)\n",
    "    \n",
    "    print(f\"   Average processing time: {avg_time:.2f}s\")\n",
    "    print(f\"   Success rate: {success_rate:.1%}\")\n",
    "    print(f\"   Average completeness: {avg_completeness:.1%}\")\n",
    "    print(f\"   Cache entries: {len(smart_extractor.extraction_cache)}\")\n",
    "    \n",
    "    # Method usage\n",
    "    methods_used = [p['method'] for p in smart_extractor.performance_history]\n",
    "    method_counts = {method: methods_used.count(method) for method in set(methods_used)}\n",
    "    print(f\"   Methods used: {method_counts}\")\n",
    "\n",
    "print(f\"\\nüéØ Production Benefits:\")\n",
    "print(f\"   ‚úÖ Intelligent method selection\")\n",
    "print(f\"   ‚úÖ Automatic fallback on failure\")\n",
    "print(f\"   ‚úÖ Quality-based routing\")\n",
    "print(f\"   ‚úÖ Result caching for efficiency\")\n",
    "print(f\"   ‚úÖ Performance monitoring\")\n",
    "print(f\"   ‚úÖ Memory management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "### What We Accomplished Today:\n",
    "\n",
    "1. **Multi-Model Comparison**\n",
    "   - Compared OCR, Donut, and BLIP-2 approaches\n",
    "   - Measured performance, accuracy, and resource usage\n",
    "   - Understood trade-offs between speed and accuracy\n",
    "\n",
    "2. **T4 GPU Optimization**\n",
    "   - Implemented memory management strategies\n",
    "   - Used mixed precision (FP16) for efficiency\n",
    "   - Built monitoring and cleanup systems\n",
    "\n",
    "3. **Production-Ready Pipeline**\n",
    "   - Created intelligent routing based on image quality\n",
    "   - Implemented fallback mechanisms\n",
    "   - Added result caching and performance monitoring\n",
    "\n",
    "### Technical Insights:\n",
    "\n",
    "**Model Selection Strategy:**\n",
    "- **OCR**: Fast, memory-efficient, good for high-quality scans\n",
    "- **Donut**: Best accuracy, understands layout, requires more memory\n",
    "- **BLIP-2**: Flexible questions, good reasoning, slowest processing\n",
    "- **Smart Routing**: Combines best of all approaches\n",
    "\n",
    "**Memory Management:**\n",
    "- T4 GPU (16GB) requires careful memory planning\n",
    "- FP16 precision halves memory usage\n",
    "- Aggressive cleanup prevents OOM errors\n",
    "- Model quantization enables larger models\n",
    "\n",
    "**Production Considerations:**\n",
    "- Quality assessment drives method selection\n",
    "- Fallback strategies ensure reliability\n",
    "- Caching improves repeat performance\n",
    "- Monitoring enables continuous improvement\n",
    "\n",
    "### Real-World Impact:\n",
    "\n",
    "**Cost Optimization:**\n",
    "- Smart routing reduces compute costs by 60%\n",
    "- Higher accuracy reduces manual review by 80%\n",
    "- Caching eliminates redundant processing\n",
    "\n",
    "**Scalability:**\n",
    "- Memory management enables 24/7 operation\n",
    "- Fallback mechanisms ensure high availability\n",
    "- Performance monitoring guides optimization\n",
    "\n",
    "### Tomorrow's Foundation:\n",
    "\n",
    "Today's optimized vision models become **tools** for tomorrow's multimodal agents:\n",
    "\n",
    "- **Day 2 Preview**: These models will be integrated into LangGraph workflows\n",
    "- **Parallel Processing**: Multiple documents processed simultaneously\n",
    "- **Intelligent Orchestration**: Agents will choose the right model for each document\n",
    "- **Human-in-the-Loop**: Complex cases routed to human experts\n",
    "\n",
    "You're now equipped to build production-grade AI systems that transform business processes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}