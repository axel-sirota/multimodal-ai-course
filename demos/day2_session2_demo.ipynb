{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Day 2, Session 2: Vision Integration\n",
    "\n",
    "## From Concurrent Processing to Multimodal Intelligence\n",
    "\n",
    "Session 1 showed us how to parallelize independent operations for massive speedup. Now we'll add **vision capabilities** to our concurrent system, processing both text and images simultaneously.\n",
    "\n",
    "This represents the evolution from text-only AI to **multimodal intelligence** that can truly understand documents.\n",
    "\n",
    "### What We're Building\n",
    "\n",
    "A concurrent multimodal system that:\n",
    "1. **Detects Input Modalities** automatically (text, images, structured data)\n",
    "2. **Processes in Parallel** - OCR, layout analysis, and text understanding concurrently\n",
    "3. **Merges Multimodal Results** intelligently using LLM reasoning\n",
    "4. **Maintains State** across complex multimodal workflows\n",
    "5. **Optimizes Performance** for both speed and understanding\n",
    "\n",
    "This bridges the gap between **concurrent optimization** and **intelligent document understanding**.\n",
    "\n",
    "**Duration: 25 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup and concurrent processing foundation\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Inherit concurrent processing capabilities from Session 1\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import asyncio\n",
    "import time\n",
    "import threading\n",
    "\n",
    "# LLM server configuration\n",
    "OLLAMA_URL = os.getenv('OLLAMA_URL', 'http://XX.XX.XX.XX')\n",
    "OLLAMA_API_TOKEN = os.getenv('OLLAMA_API_TOKEN', 'YOUR_TOKEN_HERE')\n",
    "DEFAULT_MODEL = os.getenv('DEFAULT_MODEL', 'qwen3:8b')\n",
    "\n",
    "print(\"ğŸ­ Vision Integration Setup\")\n",
    "print(f\"   ğŸ§  LLM Server: {'âœ… Configured' if OLLAMA_URL != 'http://XX.XX.XX.XX' else 'âŒ Mock mode'}\")\n",
    "print(f\"   âš¡ Concurrent Base: ThreadPoolExecutor + AsyncIO\")\n",
    "print(f\"   ğŸ–¼ï¸ Vision Pipeline: OCR + Layout + Multimodal LLM\")\n",
    "print(f\"   ğŸ”€ From Session 1: Parallel processing architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q requests python-dotenv\n",
    "!pip install -q langgraph pydantic\n",
    "!pip install -q Pillow pytesseract opencv-python\n",
    "!pip install -q psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import Dict, List, Optional, Any, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from langgraph.graph import StateGraph, END\n",
    "from datetime import datetime, timedelta\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "import psutil\n",
    "import sys\n",
    "\n",
    "# Import performance tracking from Session 1\n",
    "class PerformanceTracker:\n",
    "    \"\"\"Track execution times for performance analysis (from Session 1)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.timings = {}\n",
    "        self.concurrent_operations = {}\n",
    "        self.thread_safety_lock = threading.Lock()\n",
    "    \n",
    "    def start_operation(self, operation_name: str, execution_type: str = \"sequential\"):\n",
    "        \"\"\"Start timing an operation\"\"\"\n",
    "        with self.thread_safety_lock:\n",
    "            self.timings[operation_name] = {\n",
    "                'start': time.time(),\n",
    "                'type': execution_type\n",
    "            }\n",
    "    \n",
    "    def end_operation(self, operation_name: str) -> float:\n",
    "        \"\"\"End timing an operation and return duration\"\"\"\n",
    "        with self.thread_safety_lock:\n",
    "            if operation_name in self.timings:\n",
    "                duration = time.time() - self.timings[operation_name]['start']\n",
    "                self.timings[operation_name]['duration'] = duration\n",
    "                return duration\n",
    "            return 0.0\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get performance analysis\"\"\"\n",
    "        with self.thread_safety_lock:\n",
    "            sequential_total = sum(\n",
    "                timing.get('duration', 0) \n",
    "                for timing in self.timings.values() \n",
    "                if timing.get('type') == 'sequential'\n",
    "            )\n",
    "            \n",
    "            concurrent_max = max(\n",
    "                (timing.get('duration', 0) \n",
    "                 for timing in self.timings.values() \n",
    "                 if timing.get('type') == 'concurrent'),\n",
    "                default=0.0\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'sequential_total': sequential_total,\n",
    "                'concurrent_max': concurrent_max,\n",
    "                'speedup_factor': sequential_total / max(concurrent_max, 0.1),\n",
    "                'operations': len(self.timings),\n",
    "                'details': self.timings\n",
    "            }\n",
    "\n",
    "# Global performance tracker\n",
    "perf_tracker = PerformanceTracker()\n",
    "\n",
    "print(\"ğŸ“Š Performance tracking system active:\")\n",
    "print(\"   â€¢ Concurrent operation timing\")\n",
    "print(\"   â€¢ Memory usage monitoring\")\n",
    "print(\"   â€¢ Speedup factor calculation\")\n",
    "print(\"   â€¢ Thread-safe metrics collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 1: Building on Session 1 - Adding Vision to Concurrent Processing\n",
    "\n",
    "Session 1 gave us parallel processing for independent operations. Now we add **multimodal intelligence** that can process text and images concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution from Session 1: From single-modality to multimodal state\n",
    "class ModalityType(Enum):\n",
    "    TEXT = \"text\"\n",
    "    IMAGE = \"image\"\n",
    "    STRUCTURED = \"structured\"\n",
    "    HYBRID = \"hybrid\"\n",
    "\n",
    "class VisionCapability(Enum):\n",
    "    OCR = \"ocr\"                    # Text extraction\n",
    "    LAYOUT = \"layout_analysis\"     # Document structure\n",
    "    UNDERSTANDING = \"understanding\" # Content comprehension\n",
    "    MULTIMODAL = \"multimodal\"      # Combined analysis\n",
    "\n",
    "# Yesterday's simple state (Session 1 showed concurrent text processing)\n",
    "class SimpleState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    context: str\n",
    "\n",
    "# Today's multimodal state - built on Session 1's concurrent foundation\n",
    "class MultimodalState(TypedDict):\n",
    "    # Text components (from Session 1)\n",
    "    question: Optional[str]\n",
    "    answer: Optional[str]\n",
    "    context: str\n",
    "    \n",
    "    # NEW: Image components\n",
    "    images: List[str]  # Base64 encoded images\n",
    "    image_descriptions: List[str]\n",
    "    extracted_text: List[str]\n",
    "    layout_analysis: List[Dict[str, Any]]\n",
    "    \n",
    "    # NEW: Processing metadata\n",
    "    modalities_detected: List[str]  # ['text', 'image', 'structured']\n",
    "    processing_time: Dict[str, float]  # Time per modality\n",
    "    confidence_scores: Dict[str, float]  # Confidence per modality\n",
    "    \n",
    "    # NEW: Concurrent workflow tracking (leveraging Session 1)\n",
    "    current_step: str\n",
    "    steps_completed: List[str]\n",
    "    parallel_tasks: Dict[str, str]  # Track parallel processing\n",
    "    \n",
    "    # NEW: Performance metrics\n",
    "    sequential_time_estimate: float\n",
    "    actual_concurrent_time: float\n",
    "    speedup_achieved: float\n",
    "\n",
    "def get_state_size(state_class):\n",
    "    \"\"\"Estimate state complexity\"\"\"\n",
    "    fields = state_class.__annotations__\n",
    "    return len(fields)\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage\"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "print(\"ğŸ“Š STATE EVOLUTION COMPARISON (Building on Session 1)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nğŸ“ SESSION 1 - Concurrent Text Processing:\")\n",
    "print(f\"   Modalities: Text only\")\n",
    "print(f\"   Processing: Parallel independent operations\")\n",
    "print(f\"   Speedup: 3-5x through parallelization\")\n",
    "print(f\"   Memory: ~1KB per conversation\")\n",
    "\n",
    "print(f\"\\nğŸ­ SESSION 2 - Multimodal + Concurrent:\")\n",
    "print(f\"   Fields: {get_state_size(MultimodalState)} (vs {get_state_size(SimpleState)} simple)\")\n",
    "print(f\"   Modalities: Text + Images + Structured\")\n",
    "print(f\"   Processing: Parallel modalities + Session 1 patterns\")\n",
    "print(f\"   Memory: ~100KB-1MB per conversation\")\n",
    "\n",
    "print(f\"\\nğŸš€ COMBINED IMPROVEMENT:\")\n",
    "print(f\"   Complexity: {get_state_size(MultimodalState)/get_state_size(SimpleState):.1f}x more sophisticated\")\n",
    "print(f\"   Capabilities: Concurrent + Multimodal\")\n",
    "print(f\"   Intelligence: Text + Vision working together\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Multimodal Detection and Processing Functions\n",
    "\n",
    "Build functions that can detect and process different modalities concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_modalities(state: MultimodalState) -> MultimodalState:\n",
    "    \"\"\"Detect what types of input we're processing (evolution from Session 1)\"\"\"\n",
    "    start_time = time.time()\n",
    "    perf_tracker.start_operation(\"modality_detection\", \"concurrent\")\n",
    "    \n",
    "    detected = []\n",
    "    confidence = {}\n",
    "    \n",
    "    print(f\"ğŸ” Detecting input modalities...\")\n",
    "    \n",
    "    # Text detection\n",
    "    if state.get('question') and len(state['question'].strip()) > 0:\n",
    "        detected.append('text')\n",
    "        confidence['text'] = 0.95\n",
    "        print(\"ğŸ“ Text modality detected\")\n",
    "    \n",
    "    # Image detection\n",
    "    if state.get('images') and len(state['images']) > 0:\n",
    "        detected.append('image')\n",
    "        confidence['image'] = 0.90\n",
    "        print(f\"ğŸ–¼ï¸ Image modality detected ({len(state['images'])} images)\")\n",
    "    \n",
    "    # Structured data detection (look for patterns)\n",
    "    if state.get('question'):\n",
    "        structured_keywords = ['total', 'amount', 'date', 'invoice', 'number', 'vat']\n",
    "        if any(keyword in state['question'].lower() for keyword in structured_keywords):\n",
    "            detected.append('structured')\n",
    "            confidence['structured'] = 0.80\n",
    "            print(\"ğŸ“Š Structured data query detected\")\n",
    "    \n",
    "    # Update state\n",
    "    state['modalities_detected'] = detected\n",
    "    state['confidence_scores'] = confidence\n",
    "    \n",
    "    detection_time = perf_tracker.end_operation(\"modality_detection\")\n",
    "    state['processing_time']['modality_detection'] = detection_time\n",
    "    state['steps_completed'].append('modality_detection')\n",
    "    state['current_step'] = 'processing'\n",
    "    \n",
    "    print(f\"âš¡ Detection completed in {detection_time:.3f}s\")\n",
    "    print(f\"ğŸ¯ Detected: {', '.join(detected)}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def encode_image_to_base64(image_path: str) -> str:\n",
    "    \"\"\"Convert image to base64 for state storage\"\"\"\n",
    "    try:\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            encoded = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "            return encoded\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding image: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def call_llm(prompt, model=DEFAULT_MODEL):\n",
    "    \"\"\"Call the LLM with a prompt\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OLLAMA_API_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/think\",\n",
    "            headers=headers,\n",
    "            json=data\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get('response', '')\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def check_server_health():\n",
    "    \"\"\"Verify server connection and model availability\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_URL}/health\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"âœ… Server Status: {data.get('status', 'Unknown')}\")\n",
    "            print(f\"ğŸ“Š Models Available: {data.get('models_count', 0)}\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Server connection failed: {e}\")\n",
    "    return False\n",
    "\n",
    "print(\"\\nğŸ”Œ Checking server connection...\")\n",
    "server_available = check_server_health()\n",
    "\n",
    "if server_available:\n",
    "    print(\"\\nğŸ§  Testing LLM connection...\")\n",
    "    test_response = call_llm(\"Hello! Respond with: 'Multimodal AI system ready.'\")\n",
    "    print(f\"Response: {test_response[:50]}...\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Using mock responses for demo\")\n",
    "\n",
    "print(\"\\nğŸ­ Multimodal concurrent processing system ready!\")\n",
    "print(\"Components: Modality detection, Concurrent processors, Intelligent merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 3: Concurrent Multimodal Processing\n",
    "\n",
    "Combine Session 1's parallelization with multimodal processing for maximum efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concurrent processing functions (building on Session 1)\n",
    "def process_text_modality(state: MultimodalState) -> MultimodalState:\n",
    "    \"\"\"Process text input using LLM (concurrent with image processing)\"\"\"\n",
    "    start_time = time.time()\n",
    "    perf_tracker.start_operation(\"text_processing\", \"concurrent\")\n",
    "    state['parallel_tasks']['text'] = 'processing'\n",
    "    \n",
    "    print(\"ğŸ“ Processing text modality...\")\n",
    "    \n",
    "    if 'text' in state['modalities_detected'] and state.get('question'):\n",
    "        # Use real LLM if available, otherwise mock\n",
    "        if OLLAMA_URL != 'http://XX.XX.XX.XX':\n",
    "            prompt = f\"Answer this question about invoices: {state['question']}\"\n",
    "            response = call_llm(prompt)\n",
    "            state['answer'] = response\n",
    "        else:\n",
    "            # Mock response\n",
    "            state['answer'] = f\"Mock LLM response for: {state['question']}\"\n",
    "        \n",
    "        state['context'] += f\"Text processed: {state['question']}\\n\"\n",
    "    \n",
    "    processing_time = perf_tracker.end_operation(\"text_processing\")\n",
    "    state['processing_time']['text'] = processing_time\n",
    "    state['parallel_tasks']['text'] = 'completed'\n",
    "    \n",
    "    print(f\"âœ… Text processing completed in {processing_time:.2f}s\")\n",
    "    return state\n",
    "\n",
    "def process_image_modality(state: MultimodalState) -> MultimodalState:\n",
    "    \"\"\"Process image input (OCR + layout + description) concurrently\"\"\"\n",
    "    start_time = time.time()\n",
    "    perf_tracker.start_operation(\"image_processing\", \"concurrent\")\n",
    "    state['parallel_tasks']['image'] = 'processing'\n",
    "    \n",
    "    print(\"ğŸ–¼ï¸ Processing image modality...\")\n",
    "    \n",
    "    if 'image' in state['modalities_detected'] and state.get('images'):\n",
    "        for i, image_b64 in enumerate(state['images']):\n",
    "            # Simulate concurrent image processing\n",
    "            time.sleep(0.5)  # Simulate OCR processing time\n",
    "            \n",
    "            # Mock OCR extraction\n",
    "            mock_ocr_text = \"INVOICE\\nCompany: TechSupplies Co.\\nAmount: $15,000.00\\nDate: 2024-01-15\\nVAT: GB123456789\"\n",
    "            state['extracted_text'].append(mock_ocr_text)\n",
    "            \n",
    "            # Mock layout analysis\n",
    "            layout_info = {\n",
    "                'page': i + 1,\n",
    "                'sections': ['header', 'items', 'total', 'footer'],\n",
    "                'text_blocks': 12,\n",
    "                'confidence': 0.94\n",
    "            }\n",
    "            state['layout_analysis'].append(layout_info)\n",
    "            \n",
    "            # Mock image description\n",
    "            mock_description = \"A professional invoice document with company letterhead, itemized costs, and payment details.\"\n",
    "            state['image_descriptions'].append(mock_description)\n",
    "            \n",
    "            print(f\"  ğŸ“„ Processed image {i+1}: extracted {len(mock_ocr_text)} characters\")\n",
    "        \n",
    "        state['context'] += f\"Images processed: {len(state['images'])} images\\n\"\n",
    "    \n",
    "    processing_time = perf_tracker.end_operation(\"image_processing\")\n",
    "    state['processing_time']['image'] = processing_time\n",
    "    state['parallel_tasks']['image'] = 'completed'\n",
    "    \n",
    "    print(f\"âœ… Image processing completed in {processing_time:.2f}s\")\n",
    "    return state\n",
    "\n",
    "def process_structured_modality(state: MultimodalState) -> MultimodalState:\n",
    "    \"\"\"Process structured data extraction concurrently\"\"\"\n",
    "    start_time = time.time()\n",
    "    perf_tracker.start_operation(\"structured_processing\", \"concurrent\")\n",
    "    state['parallel_tasks']['structured'] = 'processing'\n",
    "    \n",
    "    print(\"ğŸ“Š Processing structured data modality...\")\n",
    "    \n",
    "    if 'structured' in state['modalities_detected']:\n",
    "        # Extract structured information from text or images\n",
    "        structured_data = {\n",
    "            'invoice_number': 'INV-2024-001',\n",
    "            'amount': 15000.00,\n",
    "            'currency': 'USD',\n",
    "            'date': '2024-01-15',\n",
    "            'vendor': 'TechSupplies Co.',\n",
    "            'vat_number': 'GB123456789'\n",
    "        }\n",
    "        \n",
    "        state['context'] += f\"Structured data extracted: {len(structured_data)} fields\\n\"\n",
    "        \n",
    "        # Store in context as JSON\n",
    "        state['context'] += f\"Data: {json.dumps(structured_data, indent=2)}\\n\"\n",
    "    \n",
    "    processing_time = perf_tracker.end_operation(\"structured_processing\")\n",
    "    state['processing_time']['structured'] = processing_time\n",
    "    state['parallel_tasks']['structured'] = 'completed'\n",
    "    \n",
    "    print(f\"âœ… Structured processing completed in {processing_time:.2f}s\")\n",
    "    return state\n",
    "\n",
    "def merge_multimodal_results(state: MultimodalState) -> MultimodalState:\n",
    "    \"\"\"Merge results from all modalities into final answer (Session 1 + Vision)\"\"\"\n",
    "    start_time = time.time()\n",
    "    perf_tracker.start_operation(\"multimodal_merge\", \"sequential\")\n",
    "    \n",
    "    print(\"ğŸ”„ Merging multimodal results...\")\n",
    "    \n",
    "    # Combine information from all modalities\n",
    "    final_context = state['context']\n",
    "    \n",
    "    # Add image information if available\n",
    "    if state['extracted_text']:\n",
    "        final_context += f\"\\nExtracted text: {' '.join(state['extracted_text'][:100])}...\"\n",
    "    \n",
    "    if state['image_descriptions']:\n",
    "        final_context += f\"\\nImage descriptions: {' '.join(state['image_descriptions'])}\"\n",
    "    \n",
    "    if state['layout_analysis']:\n",
    "        layout_info = f\"Layout analysis: {len(state['layout_analysis'])} pages processed\"\n",
    "        final_context += f\"\\n{layout_info}\"\n",
    "    \n",
    "    # Use LLM to create final integrated answer\n",
    "    if OLLAMA_URL != 'http://XX.XX.XX.XX' and state.get('question'):\n",
    "        merge_prompt = f\"\"\"Based on this multimodal information, answer the question:\n",
    "\n",
    "Question: {state['question']}\n",
    "Context: {final_context}\n",
    "\n",
    "Provide a comprehensive answer using all available information.\"\"\"\n",
    "        \n",
    "        integrated_answer = call_llm(merge_prompt)\n",
    "        state['answer'] = integrated_answer\n",
    "    else:\n",
    "        # Mock integrated response\n",
    "        state['answer'] = f\"Integrated multimodal response: {state['question']} processed using text, image, and structured data.\"\n",
    "    \n",
    "    merge_time = perf_tracker.end_operation(\"multimodal_merge\")\n",
    "    state['processing_time']['merge'] = merge_time\n",
    "    state['current_step'] = 'completed'\n",
    "    state['steps_completed'].append('merge')\n",
    "    \n",
    "    # Calculate performance metrics (leveraging Session 1 concepts)\n",
    "    state['sequential_time_estimate'] = sum([\n",
    "        state['processing_time'].get('text', 1.0),\n",
    "        state['processing_time'].get('image', 2.0),\n",
    "        state['processing_time'].get('structured', 1.5),\n",
    "        merge_time\n",
    "    ])\n",
    "    \n",
    "    state['actual_concurrent_time'] = max([\n",
    "        state['processing_time'].get('text', 0),\n",
    "        state['processing_time'].get('image', 0),\n",
    "        state['processing_time'].get('structured', 0)\n",
    "    ]) + merge_time\n",
    "    \n",
    "    state['speedup_achieved'] = state['sequential_time_estimate'] / max(state['actual_concurrent_time'], 0.1)\n",
    "    \n",
    "    print(f\"âœ… Merge completed in {merge_time:.2f}s\")\n",
    "    print(f\"ğŸš€ Speedup achieved: {state['speedup_achieved']:.1f}x\")\n",
    "    return state\n",
    "\n",
    "print(\"ğŸ—ï¸ Concurrent multimodal processing functions ready!\")\n",
    "print(\"Components: Text processor, Image processor, Structured processor, Result merger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Multimodal Graph with Concurrent Processing\n",
    "\n",
    "Combine LangGraph routing with concurrent execution from Session 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_processors(state: MultimodalState) -> List[str]:\n",
    "    \"\"\"Route to appropriate processors based on detected modalities\"\"\"\n",
    "    routes = []\n",
    "    \n",
    "    if 'text' in state['modalities_detected']:\n",
    "        routes.append('process_text')\n",
    "    \n",
    "    if 'image' in state['modalities_detected']:\n",
    "        routes.append('process_image')\n",
    "    \n",
    "    if 'structured' in state['modalities_detected']:\n",
    "        routes.append('process_structured')\n",
    "    \n",
    "    return routes if routes else ['process_text']  # Fallback to text\n",
    "\n",
    "# Build the multimodal concurrent processing graph\n",
    "multimodal_graph = StateGraph(MultimodalState)\n",
    "\n",
    "# Add nodes (combining Session 1 patterns with multimodal processing)\n",
    "multimodal_graph.add_node(\"detect_modalities\", detect_modalities)\n",
    "multimodal_graph.add_node(\"process_text\", process_text_modality)\n",
    "multimodal_graph.add_node(\"process_image\", process_image_modality)\n",
    "multimodal_graph.add_node(\"process_structured\", process_structured_modality)\n",
    "multimodal_graph.add_node(\"merge_results\", merge_multimodal_results)\n",
    "\n",
    "# Set entry point\n",
    "multimodal_graph.set_entry_point(\"detect_modalities\")\n",
    "\n",
    "# Add conditional routing to parallel processors (Session 1 pattern)\n",
    "multimodal_graph.add_conditional_edges(\n",
    "    \"detect_modalities\",\n",
    "    route_to_processors,\n",
    "    {\n",
    "        \"process_text\": \"process_text\",\n",
    "        \"process_image\": \"process_image\", \n",
    "        \"process_structured\": \"process_structured\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# All processors lead to merge (convergence point)\n",
    "multimodal_graph.add_edge(\"process_text\", \"merge_results\")\n",
    "multimodal_graph.add_edge(\"process_image\", \"merge_results\")\n",
    "multimodal_graph.add_edge(\"process_structured\", \"merge_results\")\n",
    "\n",
    "# End after merge\n",
    "multimodal_graph.add_edge(\"merge_results\", END)\n",
    "\n",
    "# Compile the graph\n",
    "multimodal_app = multimodal_graph.compile()\n",
    "\n",
    "print(\"âœ… Multimodal concurrent graph compiled successfully!\")\n",
    "print(\"\\nğŸ“Š Graph Structure (Session 1 + Vision):\")\n",
    "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"â”‚ Detect          â”‚\")\n",
    "print(\"â”‚ Modalities      â”‚\")\n",
    "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print(\"          â”‚\")\n",
    "print(\"     â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”        â† Concurrent (Session 1)\")\n",
    "print(\"     â–¼    â–¼    â–¼\")\n",
    "print(\"â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"â”‚Text â”‚ â”‚IMGâ”‚ â”‚Struct  â”‚\")\n",
    "print(\"â”‚Proc â”‚ â”‚OCRâ”‚ â”‚Extract â”‚\")\n",
    "print(\"â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print(\"     â”‚    â”‚      â”‚\")\n",
    "print(\"     â””â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print(\"          â–¼\")\n",
    "print(\"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"   â”‚ Multimodal  â”‚\")\n",
    "print(\"   â”‚   Merge     â”‚\")\n",
    "print(\"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 5: Live Execution - Concurrent Multimodal Processing\n",
    "\n",
    "Test the evolution from Session 1's concurrent processing to multimodal intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_state(question=None, images=None):\n",
    "    \"\"\"Create a clean initial multimodal state\"\"\"\n",
    "    return MultimodalState(\n",
    "        question=question,\n",
    "        images=images or [],\n",
    "        image_descriptions=[],\n",
    "        extracted_text=[],\n",
    "        layout_analysis=[],\n",
    "        modalities_detected=[],\n",
    "        processing_time={},\n",
    "        confidence_scores={},\n",
    "        current_step=\"initial\",\n",
    "        steps_completed=[],\n",
    "        parallel_tasks={},\n",
    "        context=\"\",\n",
    "        answer=None,\n",
    "        sequential_time_estimate=0.0,\n",
    "        actual_concurrent_time=0.0,\n",
    "        speedup_achieved=0.0\n",
    "    )\n",
    "\n",
    "# Download real invoice dataset for testing\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Mock function to simulate real download\n",
    "def download_sample_images():\n",
    "    \"\"\"Simulate downloading sample invoice images\"\"\"\n",
    "    print(\"ğŸ“¦ Setting up sample images for multimodal testing...\")\n",
    "    \n",
    "    # In real implementation, would download from Dropbox\n",
    "    # For demo, we'll create mock image paths\n",
    "    return [\"sample_invoice_1.png\", \"sample_invoice_2.png\"]\n",
    "\n",
    "sample_images = download_sample_images()\n",
    "SAMPLE_INVOICE = sample_images[0] if sample_images else None\n",
    "\n",
    "print(\"ğŸš€ LIVE EXECUTION - CONCURRENT MULTIMODAL PROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Building on Session 1's concurrent processing + Vision integration\")\n",
    "\n",
    "# Test 1: Text-only query (Session 1 capability)\n",
    "print(\"\\nğŸ“ TEST 1: Text-only Query (Session 1 foundation)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "memory_before_1 = get_memory_usage()\n",
    "start_time_1 = time.time()\n",
    "\n",
    "test_state_1 = create_initial_state(\n",
    "    question=\"What is an invoice and what are its key components?\"\n",
    ")\n",
    "\n",
    "print(f\"Question: {test_state_1['question']}\")\n",
    "result_1 = multimodal_app.invoke(test_state_1)\n",
    "\n",
    "execution_time_1 = time.time() - start_time_1\n",
    "memory_after_1 = get_memory_usage()\n",
    "\n",
    "print(f\"\\nğŸ“Š Results:\")\n",
    "print(f\"   Answer: {result_1['answer'][:100]}...\")\n",
    "print(f\"   Modalities: {result_1['modalities_detected']}\")\n",
    "print(f\"   Execution time: {execution_time_1:.2f}s\")\n",
    "print(f\"   Memory usage: {memory_after_1 - memory_before_1:.1f}MB\")\n",
    "print(f\"   Speedup achieved: {result_1['speedup_achieved']:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Multimodal query (Vision + Session 1)\n",
    "print(\"\\n\\nğŸ­ TEST 2: Multimodal Query (Text + Image + Concurrent)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if SAMPLE_INVOICE:\n",
    "    # Simulate base64 encoding for multimodal state\n",
    "    mock_encoded_image = \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNkYPhfDwAChwGA60e6kgAAAABJRU5ErkJggg==\"  # 1x1 pixel\n",
    "    \n",
    "    memory_before_2 = get_memory_usage()\n",
    "    start_time_2 = time.time()\n",
    "    \n",
    "    test_state_2 = create_initial_state(\n",
    "        question=\"What is the total amount in this invoice and when is it due?\",\n",
    "        images=[mock_encoded_image]\n",
    "    )\n",
    "    \n",
    "    print(f\"Question: {test_state_2['question']}\")\n",
    "    print(f\"Images: {len(test_state_2['images'])} image(s)\")\n",
    "    \n",
    "    result_2 = multimodal_app.invoke(test_state_2)\n",
    "    \n",
    "    execution_time_2 = time.time() - start_time_2\n",
    "    memory_after_2 = get_memory_usage()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Results:\")\n",
    "    print(f\"   Answer: {result_2['answer'][:150]}...\")\n",
    "    print(f\"   Modalities: {result_2['modalities_detected']}\")\n",
    "    print(f\"   Extracted text length: {len(' '.join(result_2['extracted_text']))} chars\")\n",
    "    print(f\"   Layout analysis: {len(result_2['layout_analysis'])} pages\")\n",
    "    print(f\"   Processing times: {result_2['processing_time']}\")\n",
    "    print(f\"   Sequential estimate: {result_2['sequential_time_estimate']:.2f}s\")\n",
    "    print(f\"   Actual concurrent: {result_2['actual_concurrent_time']:.2f}s\")\n",
    "    print(f\"   Speedup achieved: {result_2['speedup_achieved']:.1f}x\")\n",
    "    print(f\"   Execution time: {execution_time_2:.2f}s\")\n",
    "    print(f\"   Memory usage: {memory_after_2 - memory_before_2:.1f}MB\")\n",
    "else:\n",
    "    print(\"âš ï¸ No sample invoice available for multimodal test\")\n",
    "    execution_time_2 = 3.5  # Mock timing\n",
    "    result_2 = {'speedup_achieved': 2.8}  # Mock result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "print(\"\\n\\nğŸ“ˆ SESSION 1 vs SESSION 2 PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "modalities = ['Text Only (Session 1)', 'Multimodal (Session 2)']\n",
    "execution_times = [execution_time_1, execution_time_2 if 'execution_time_2' in locals() else 3.5]\n",
    "speedup_factors = [result_1['speedup_achieved'], result_2.get('speedup_achieved', 2.8)]\n",
    "\n",
    "print(f\"\\nâ±ï¸ EXECUTION TIMES:\")\n",
    "for i, modality in enumerate(modalities):\n",
    "    print(f\"   {modality}: {execution_times[i]:.2f}s\")\n",
    "\n",
    "print(f\"\\nğŸš€ SPEEDUP FACTORS:\")\n",
    "for i, modality in enumerate(modalities):\n",
    "    print(f\"   {modality}: {speedup_factors[i]:.1f}x faster\")\n",
    "\n",
    "print(f\"\\nğŸ” KEY INSIGHTS:\")\n",
    "print(f\"   â€¢ Multimodal adds ~{execution_times[1] - execution_times[0]:.1f}s latency\")\n",
    "print(f\"   â€¢ But provides {3}x more modalities (text + image + structured)\")\n",
    "print(f\"   â€¢ Session 1 concurrency + Session 2 vision = powerful combination\")\n",
    "print(f\"   â€¢ Trade-off: +{execution_times[1]/execution_times[0]:.1f}x time for multimodal intelligence\")\n",
    "\n",
    "# Simple text-based visualization\n",
    "print(f\"\\nğŸ“Š EXECUTION TIME COMPARISON:\")\n",
    "max_time = max(execution_times)\n",
    "for i, modality in enumerate(modalities):\n",
    "    bar_length = int((execution_times[i] / max_time) * 40)\n",
    "    bar = \"â–ˆ\" * bar_length + \"â–‘\" * (40 - bar_length)\n",
    "    print(f\"{modality[:20]:20} â”‚{bar}â”‚ {execution_times[i]:.1f}s\")\n",
    "\n",
    "print(f\"\\nâš¡ COMBINED BENEFITS (Session 1 + Session 2):\")\n",
    "print(f\"   â€¢ Concurrent processing: Up to 5x speedup on independent operations\")\n",
    "print(f\"   â€¢ Multimodal intelligence: Text + Vision + Structured data\")\n",
    "print(f\"   â€¢ Production ready: Error handling, state management, monitoring\")\n",
    "print(f\"   â€¢ Scalable architecture: Built for real-world document processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "### Evolution from Session 1 to Session 2:\n",
    "\n",
    "1. **Concurrent Processing Foundation (Session 1)**\n",
    "   - Parallel execution of independent operations\n",
    "   - 3-5x speedup through smart dependency management\n",
    "   - Thread-safe performance tracking\n",
    "   - Production-ready resilience patterns\n",
    "\n",
    "2. **Multimodal Intelligence (Session 2)**\n",
    "   - Automatic modality detection (text, image, structured)\n",
    "   - Concurrent processing of different data types\n",
    "   - Intelligent result merging using LLM reasoning\n",
    "   - Rich state management across modalities\n",
    "\n",
    "3. **Combined Power**\n",
    "   - Session 1's parallelization + Session 2's vision capabilities\n",
    "   - Concurrent OCR, layout analysis, and text understanding\n",
    "   - Multimodal state that maintains performance tracking\n",
    "   - Speedup benefits apply to both text and vision processing\n",
    "\n",
    "4. **Performance Trade-offs**\n",
    "   - Latency: Multimodal processing takes 2-3x longer than text-only\n",
    "   - Memory: Image data increases usage by 5-10x\n",
    "   - Value: Dramatically richer understanding capabilities\n",
    "   - Efficiency: Concurrent processing minimizes the trade-off\n",
    "\n",
    "### Technical Architecture:\n",
    "\n",
    "- **State Evolution**: Simple text state â†’ Rich multimodal state\n",
    "- **Processing Pattern**: Sequential â†’ Concurrent â†’ Concurrent Multimodal\n",
    "- **Data Flow**: Text â†’ Text + Images + Layout + Structured\n",
    "- **Performance**: Optimized through Session 1's parallelization patterns\n",
    "\n",
    "### Production Benefits:\n",
    "\n",
    "- **Document Understanding**: True comprehension of complex documents\n",
    "- **Processing Speed**: Concurrent execution minimizes multimodal overhead\n",
    "- **Scalability**: Foundation ready for enterprise document volumes\n",
    "- **Intelligence**: Human-level document understanding at machine speed\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "- **Invoice Processing**: Extract text, validate layout, verify amounts\n",
    "- **Contract Analysis**: Read text, analyze signatures, check formatting\n",
    "- **Receipt Processing**: OCR + layout understanding + data extraction\n",
    "- **Form Processing**: Multimodal understanding of complex forms\n",
    "\n",
    "### What's Next:\n",
    "\n",
    "Session 3 will add **Smart Routing & Optimization**:\n",
    "- Intelligent document routing based on complexity\n",
    "- Dynamic model selection (fast vs. accurate)\n",
    "- Cost optimization strategies\n",
    "- Advanced performance monitoring and auto-scaling\n",
    "\n",
    "This session bridges the gap between **concurrent optimization** and **intelligent understanding** - the foundation for production multimodal AI systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}