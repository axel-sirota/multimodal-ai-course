{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2, Session 3: Multi-Document Batch Processor\n",
    "\n",
    "## Scaling AI with Parallel Processing\n",
    "\n",
    "Today we move beyond single-document processing to handle enterprise-scale batch workloads. When you need to process hundreds or thousands of invoices, serial processing becomes a bottleneck. We'll use LangGraph's Send API to create dynamic parallel workflows that scale with your workload.\n",
    "\n",
    "### The Performance Challenge\n",
    "\n",
    "**Serial Processing:** 10 invoices × 3 seconds each = 30 seconds  \n",
    "**Parallel Processing:** 10 invoices ÷ 4 workers = ~8 seconds  \n",
    "**Enterprise Scale:** 1000 invoices in minutes instead of hours\n",
    "\n",
    "### What We're Building\n",
    "\n",
    "A dynamic batch processor that:\n",
    "- Processes multiple invoices simultaneously\n",
    "- Allocates workers based on current load\n",
    "- Handles partial failures gracefully\n",
    "- Provides real-time progress monitoring\n",
    "- Scales memory usage efficiently\n",
    "\n",
    "Let's see parallel AI in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration - Instructor will fill these\n",
    "OLLAMA_URL = \"http://XX.XX.XX.XX\"  # Course server IP (port 80)\n",
    "API_TOKEN = \"YOUR_TOKEN_HERE\"      # Instructor provides token\n",
    "MODEL = \"qwen3:8b\"                  # Default model on server\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any, TypedDict, Annotated\n",
    "from dataclasses import dataclass, field\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import psutil\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q langgraph langchain-core\n",
    "\n",
    "from langgraph.graph import StateGraph, END, Send\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Health check\n",
    "def check_server_health():\n",
    "    \"\"\"Verify server connection\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_URL}/health\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"✅ Server Status: {data.get('status', 'Unknown')}\")\n",
    "            print(f\"📊 Models Available: {data.get('models_count', 0)}\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Server connection failed: {e}\")\n",
    "    return False\n",
    "\n",
    "# LLM calling function\n",
    "def call_llm(prompt, model=MODEL):\n",
    "    \"\"\"Call the LLM with a prompt\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/think\",\n",
    "            headers=headers,\n",
    "            json=data\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get('response', '')\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage\"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "def get_server_metrics():\n",
    "    \"\"\"Get server performance metrics\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_URL}/metrics\")\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "    except:\n",
    "        pass\n",
    "    return {\"status\": \"unavailable\"}\n",
    "\n",
    "print(\"🚀 Parallel Processing Demo Setup\")\nprint(\"🔌 Connecting to course server...\")\nserver_available = check_server_health()\n\nprint(f\"\\n💾 Initial Memory Usage: {get_memory_usage():.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download invoice dataset\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "dropbox_url = \"https://www.dropbox.com/scl/fo/m9hyfmvi78snwv0nh34mo/AMEXxwXMLAOeve-_yj12ck8?rlkey=urinkikgiuven0fro7r4x5rcu&st=hv3of7g7&dl=1\"\n",
    "\n",
    "print(\"📦 Downloading invoice dataset...\")\n",
    "try:\n",
    "    response = requests.get(dropbox_url)\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "        z.extractall(\"invoice_images\")\n",
    "    print(\"✅ Downloaded invoice dataset\")\n",
    "    \n",
    "    # Create sample invoice batch for testing\n",
    "    SAMPLE_INVOICES = [\n",
    "        {\n",
    "            \"invoice_id\": f\"INV-2024-{str(i+1).zfill(3)}\",\n",
    "            \"vendor\": f\"Company {chr(65+i)}\",\n",
    "            \"amount\": 1000 + (i * 500),\n",
    "            \"currency\": [\"USD\", \"EUR\", \"GBP\"][i % 3],\n",
    "            \"complexity\": [\"simple\", \"medium\", \"complex\"][i % 3],\n",
    "            \"processing_time_estimate\": 2 + (i % 3)  # 2-4 seconds\n",
    "        }\n",
    "        for i in range(10)\n",
    "    ]\n",
    "    \n",
    "    print(f\"📊 Created batch of {len(SAMPLE_INVOICES)} test invoices\")\n",
    "    \n",
    "    # Show sample\n",
    "    for i, invoice in enumerate(SAMPLE_INVOICES[:3]):\n",
    "        print(f\"  {i+1}. {invoice['invoice_id']}: {invoice['vendor']} - ${invoice['amount']} {invoice['currency']}\")\n",
    "    print(f\"  ... and {len(SAMPLE_INVOICES)-3} more\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"❌ Error downloading: {e}\")\n",
    "    SAMPLE_INVOICES = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Parallel State with Reducers\n",
    "\n",
    "When multiple workers update state simultaneously, we need reducers to merge updates safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom reducers for safe parallel state updates\n",
    "def merge_results(existing: List[Dict], new: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Safely merge processing results from multiple workers\"\"\"\n",
    "    if not existing:\n",
    "        return new\n",
    "    if not new:\n",
    "        return existing\n",
    "    \n",
    "    # Create a combined list, avoiding duplicates by invoice_id\n",
    "    existing_ids = {item.get('invoice_id') for item in existing}\n",
    "    merged = existing.copy()\n",
    "    \n",
    "    for item in new:\n",
    "        if item.get('invoice_id') not in existing_ids:\n",
    "            merged.append(item)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def merge_processing_status(existing: Dict[str, str], new: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"Merge processing status updates\"\"\"\n",
    "    if not existing:\n",
    "        return new\n",
    "    if not new:\n",
    "        return existing\n",
    "    \n",
    "    merged = existing.copy()\n",
    "    merged.update(new)\n",
    "    return merged\n",
    "\n",
    "def merge_metrics(existing: Dict[str, Any], new: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Merge performance metrics\"\"\"\n",
    "    if not existing:\n",
    "        return new\n",
    "    if not new:\n",
    "        return existing\n",
    "    \n",
    "    merged = existing.copy()\n",
    "    \n",
    "    # Add numeric values\n",
    "    for key in ['total_processed', 'total_errors', 'total_time']:\n",
    "        merged[key] = existing.get(key, 0) + new.get(key, 0)\n",
    "    \n",
    "    # Update other metrics\n",
    "    for key, value in new.items():\n",
    "        if key not in ['total_processed', 'total_errors', 'total_time']:\n",
    "            merged[key] = value\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Define the batch processing state\n",
    "class BatchState(TypedDict):\n",
    "    \"\"\"State for parallel batch processing\"\"\"\n",
    "    # Input data\n",
    "    invoice_queue: List[Dict[str, Any]]  # Invoices to process\n",
    "    \n",
    "    # Processing status (updated by workers)\n",
    "    processing: Annotated[Dict[str, str], merge_processing_status]  # Currently processing\n",
    "    \n",
    "    # Results (safely merged from parallel workers)\n",
    "    results: Annotated[List[Dict], merge_results]  # Completed results\n",
    "    errors: List[str]  # Processing errors\n",
    "    \n",
    "    # Performance metrics (aggregated)\n",
    "    metrics: Annotated[Dict[str, Any], merge_metrics]  # Performance tracking\n",
    "    \n",
    "    # Workflow control\n",
    "    batch_id: str\n",
    "    started_at: Optional[str]\n",
    "    completed_at: Optional[str]\n",
    "    parallelism_level: int\n",
    "\n",
    "def create_batch_state(invoices: List[Dict], parallelism: int = 4) -> BatchState:\n",
    "    \"\"\"Create initial batch processing state\"\"\"\n",
    "    return BatchState(\n",
    "        invoice_queue=invoices,\n",
    "        processing={},\n",
    "        results=[],\n",
    "        errors=[],\n",
    "        metrics={\n",
    "            'total_processed': 0,\n",
    "            'total_errors': 0,\n",
    "            'total_time': 0,\n",
    "            'worker_times': [],\n",
    "            'memory_usage': []\n",
    "        },\n",
    "        batch_id=str(uuid.uuid4())[:8],\n",
    "        started_at=None,\n",
    "        completed_at=None,\n",
    "        parallelism_level=parallelism\n",
    "    )\n",
    "\n",
    "print(\"✅ Parallel state with reducers defined\")\nprint(\"🔧 Reducers ensure safe concurrent updates\")\nprint(\"📊 State tracks progress, results, and performance metrics\")\n\n# Test reducer functionality\ntest_results1 = [{'invoice_id': 'INV-001', 'amount': 1000}]\ntest_results2 = [{'invoice_id': 'INV-002', 'amount': 2000}]\nmerged = merge_results(test_results1, test_results2)\nprint(f\"\\n🧪 Reducer test: {len(merged)} results merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implement Send API for Dynamic Parallelism\n",
    "\n",
    "The Send API allows us to create workers dynamically based on the workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispatcher(state: BatchState) -> List[Send]:\n",
    "    \"\"\"Use Send API to create parallel workers for invoice processing\"\"\"\n",
    "    print(f\"📤 Dispatcher starting for batch {state['batch_id']}\")\n",
    "    print(f\"   Invoices to process: {len(state['invoice_queue'])}\")\n",
    "    print(f\"   Parallelism level: {state['parallelism_level']}\")\n",
    "    \n",
    "    # Record start time\n",
    "    state['started_at'] = datetime.now().isoformat()\n",
    "    \n",
    "    # Create Send objects for parallel processing\n",
    "    sends = []\n",
    "    \n",
    "    # Group invoices into chunks for workers\n",
    "    chunk_size = max(1, len(state['invoice_queue']) // state['parallelism_level'])\n",
    "    \n",
    "    for i in range(0, len(state['invoice_queue']), chunk_size):\n",
    "        chunk = state['invoice_queue'][i:i + chunk_size]\n",
    "        worker_id = f\"worker_{len(sends)+1}\"\n",
    "        \n",
    "        # Create a Send for this chunk\n",
    "        worker_state = {\n",
    "            'worker_id': worker_id,\n",
    "            'invoices': chunk,\n",
    "            'batch_id': state['batch_id'],\n",
    "            'worker_start_time': time.time()\n",
    "        }\n",
    "        \n",
    "        sends.append(Send(\"process_invoice_worker\", worker_state))\n",
    "        \n",
    "        print(f\"   📋 {worker_id}: {len(chunk)} invoices assigned\")\n",
    "    \n",
    "    print(f\"✅ Dispatched {len(sends)} workers\")\n",
    "    return sends\n",
    "\n",
    "def process_invoice_worker(worker_state: Dict[str, Any]) -> BatchState:\n",
    "    \"\"\"Worker node that processes a chunk of invoices\"\"\"\n",
    "    worker_id = worker_state['worker_id']\n",
    "    invoices = worker_state['invoices']\n",
    "    batch_id = worker_state['batch_id']\n",
    "    start_time = worker_state['worker_start_time']\n",
    "    \n",
    "    print(f\"\\n👷 {worker_id} starting ({len(invoices)} invoices)\")\n",
    "    \n",
    "    # Initialize worker results\n",
    "    worker_results = []\n",
    "    worker_processing = {}\n",
    "    worker_errors = []\n",
    "    \n",
    "    # Process each invoice in this worker's chunk\n",
    "    for i, invoice in enumerate(invoices):\n",
    "        invoice_id = invoice['invoice_id']\n",
    "        \n",
    "        try:\n",
    "            # Mark as processing\n",
    "            worker_processing[invoice_id] = f\"processing_by_{worker_id}\"\n",
    "            \n",
    "            print(f\"   📄 {worker_id} processing {invoice_id}...\")\n",
    "            \n",
    "            # Simulate processing time based on complexity\n",
    "            processing_time = invoice.get('processing_time_estimate', 2)\n",
    "            time.sleep(processing_time * 0.3)  # Scale down for demo\n",
    "            \n",
    "            # Simulate LLM processing (or use real LLM if available)\n",
    "            if server_available:\n",
    "                prompt = f\"Extract key information from invoice {invoice_id} for {invoice['vendor']}. Amount: ${invoice['amount']} {invoice['currency']}\"\n",
    "                llm_response = call_llm(prompt)\n",
    "                extracted_data = llm_response[:100] + \"...\" if len(llm_response) > 100 else llm_response\n",
    "            else:\n",
    "                # Mock extraction\n",
    "                extracted_data = f\"Vendor: {invoice['vendor']}, Amount: ${invoice['amount']}, Currency: {invoice['currency']}\"\n",
    "            \n",
    "            # Create result\n",
    "            result = {\n",
    "                'invoice_id': invoice_id,\n",
    "                'status': 'completed',\n",
    "                'processed_by': worker_id,\n",
    "                'processing_time': processing_time,\n",
    "                'extracted_data': extracted_data,\n",
    "                'original_invoice': invoice,\n",
    "                'completed_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            worker_results.append(result)\n",
    "            \n",
    "            # Remove from processing\n",
    "            del worker_processing[invoice_id]\n",
    "            \n",
    "            print(f\"   ✅ {worker_id} completed {invoice_id}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"{worker_id} failed to process {invoice_id}: {str(e)}\"\n",
    "            worker_errors.append(error_msg)\n",
    "            print(f\"   ❌ {error_msg}\")\n",
    "            \n",
    "            # Remove from processing\n",
    "            if invoice_id in worker_processing:\n",
    "                del worker_processing[invoice_id]\n",
    "    \n",
    "    # Calculate worker metrics\n",
    "    worker_time = time.time() - start_time\n",
    "    memory_usage = get_memory_usage()\n",
    "    \n",
    "    worker_metrics = {\n",
    "        'total_processed': len(worker_results),\n",
    "        'total_errors': len(worker_errors),\n",
    "        'total_time': worker_time,\n",
    "        'worker_times': [worker_time],\n",
    "        'memory_usage': [memory_usage],\n",
    "        f'{worker_id}_completed': len(worker_results)\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ {worker_id} finished: {len(worker_results)} completed, {len(worker_errors)} errors in {worker_time:.1f}s\")\n",
    "    \n",
    "    # Return state update (will be merged by reducers)\n",
    "    return BatchState(\n",
    "        invoice_queue=[],  # Empty - already processed\n",
    "        processing=worker_processing,\n",
    "        results=worker_results,\n",
    "        errors=worker_errors,\n",
    "        metrics=worker_metrics,\n",
    "        batch_id=batch_id,\n",
    "        started_at=None,\n",
    "        completed_at=None,\n",
    "        parallelism_level=0\n",
    "    )\n",
    "\n",
    "def aggregator(state: BatchState) -> BatchState:\n",
    "    \"\"\"Aggregate results from all workers\"\"\"\n",
    "    print(f\"\\n📊 Aggregating results for batch {state['batch_id']}\")\n",
    "    \n",
    "    # Mark completion time\n",
    "    state['completed_at'] = datetime.now().isoformat()\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    total_invoices = len(state['invoice_queue']) if state['invoice_queue'] else state['metrics']['total_processed']\n",
    "    success_count = len(state['results'])\n",
    "    error_count = len(state['errors'])\n",
    "    \n",
    "    # Calculate timing metrics\n",
    "    if state['started_at'] and state['completed_at']:\n",
    "        start_time = datetime.fromisoformat(state['started_at'])\n",
    "        end_time = datetime.fromisoformat(state['completed_at'])\n",
    "        total_wall_time = (end_time - start_time).total_seconds()\n",
    "    else:\n",
    "        total_wall_time = state['metrics']['total_time']\n",
    "    \n",
    "    # Update metrics with final calculations\n",
    "    state['metrics'].update({\n",
    "        'total_wall_time': total_wall_time,\n",
    "        'success_rate': success_count / max(1, total_invoices),\n",
    "        'average_time_per_invoice': total_wall_time / max(1, total_invoices),\n",
    "        'throughput_per_second': total_invoices / max(1, total_wall_time)\n",
    "    })\n",
    "    \n",
    "    print(f\"✅ Batch {state['batch_id']} completed:\")\n",
    "    print(f\"   📈 Processed: {success_count}/{total_invoices} invoices\")\n",
    "    print(f\"   ⏱️ Total time: {total_wall_time:.1f}s\")\n",
    "    print(f\"   🚀 Throughput: {state['metrics']['throughput_per_second']:.1f} invoices/second\")\n",
    "    print(f\"   📊 Success rate: {state['metrics']['success_rate']:.1%}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"✅ Send API and worker nodes implemented\")\nprint(\"📤 Dispatcher creates dynamic workers\")\nprint(\"👷 Workers process invoice chunks in parallel\")\nprint(\"📊 Aggregator combines results with performance metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Parallel Processing Graph\n",
    "\n",
    "Now we'll assemble the complete parallel processing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the parallel processing graph\n",
    "print(\"🏗️ Building parallel processing workflow...\")\n",
    "\n",
    "# Create the graph\n",
    "parallel_workflow = StateGraph(BatchState)\n",
    "\n",
    "# Add nodes\n",
    "parallel_workflow.add_node(\"dispatcher\", dispatcher)\n",
    "parallel_workflow.add_node(\"process_invoice_worker\", process_invoice_worker)\n",
    "parallel_workflow.add_node(\"aggregator\", aggregator)\n",
    "\n",
    "# Set entry point\n",
    "parallel_workflow.set_entry_point(\"dispatcher\")\n",
    "\n",
    "# Add edges\n",
    "# Dispatcher uses Send API to create parallel workers\n",
    "# Workers automatically route to aggregator\n",
    "parallel_workflow.add_edge(\"process_invoice_worker\", \"aggregator\")\n",
    "parallel_workflow.add_edge(\"aggregator\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "try:\n",
    "    parallel_app = parallel_workflow.compile()\n",
    "    print(\"✅ Parallel workflow compiled successfully!\")\nexcept Exception as e:\n",
    "    print(f\"❌ Error compiling workflow: {e}\")\n\n# Visualize the workflow structure\nprint(\"\\n📊 Parallel Processing Workflow:\")\nprint(\"┌─────────────────┐\")\nprint(\"│   Dispatcher    │\")\nprint(\"│  (Send API)     │\")\nprint(\"└─────────┬───────┘\")\nprint(\"          │\")\nprint(\"    ┌─────┼─────┐\")\nprint(\"    ▼     ▼     ▼\")\nprint(\"┌────────┐ ┌────────┐ ┌────────┐\")\nprint(\"│Worker 1│ │Worker 2│ │Worker N│\")\nprint(\"│Invoice │ │Invoice │ │Invoice │\")\nprint(\"│Chunk 1 │ │Chunk 2 │ │Chunk N │\")\nprint(\"└────────┘ └────────┘ └────────┘\")\nprint(\"    │        │        │\")\nprint(\"    └────────┼────────┘\")\nprint(\"             ▼\")\nprint(\"    ┌─────────────────┐\")\nprint(\"    │   Aggregator    │\")\nprint(\"    │ (Merge Results) │\")\nprint(\"    └─────────────────┘\")\nprint(\"             │\")\nprint(\"            END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Performance Comparison - Serial vs Parallel\n",
    "\n",
    "Let's demonstrate the dramatic performance improvements from parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_serial_processing(invoices: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Simulate serial processing for comparison\"\"\"\n",
    "    print(\"\\n🔄 Running serial processing simulation...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    \n",
    "    for i, invoice in enumerate(invoices):\n",
    "        print(f\"   Processing {i+1}/{len(invoices)}: {invoice['invoice_id']}\")\n",
    "        \n",
    "        # Simulate processing time\n",
    "        processing_time = invoice.get('processing_time_estimate', 2)\n",
    "        time.sleep(processing_time * 0.3)  # Scale down for demo\n",
    "        \n",
    "        results.append({\n",
    "            'invoice_id': invoice['invoice_id'],\n",
    "            'status': 'completed',\n",
    "            'processing_time': processing_time\n",
    "        })\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'total_time': total_time,\n",
    "        'throughput': len(invoices) / total_time\n",
    "    }\n",
    "\n",
    "# Performance comparison\nprint(\"⚡ PERFORMANCE COMPARISON\")\nprint(\"=\" * 50)\n\nif SAMPLE_INVOICES:\n    # Test with different batch sizes\n    test_sizes = [5, 10] if len(SAMPLE_INVOICES) >= 10 else [len(SAMPLE_INVOICES)]\n    \n    for batch_size in test_sizes:\n",
    "        test_batch = SAMPLE_INVOICES[:batch_size]\n",
    "        \n",
    "        print(f\"\\n📊 Testing with {batch_size} invoices:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Serial processing\n",
    "        serial_result = simulate_serial_processing(test_batch)\n",
    "        serial_time = serial_result['total_time']\n",
    "        \n",
    "        print(f\"📈 Serial Results:\")\n",
    "        print(f\"   Time: {serial_time:.1f}s\")\n",
    "        print(f\"   Throughput: {serial_result['throughput']:.1f} invoices/sec\")\n",
    "        \n",
    "        # Parallel processing with different parallelism levels\n",
    "        parallelism_levels = [2, 4] if batch_size >= 4 else [2]\n",
    "        \n",
    "        for parallelism in parallelism_levels:\n",
    "            if parallelism <= batch_size:\n",
    "                print(f\"\\n🚀 Parallel Processing ({parallelism} workers):\")\n",
    "                \n",
    "                # Create batch state\n",
    "                batch_state = create_batch_state(test_batch, parallelism)\n",
    "                \n",
    "                # Track memory before\n",
    "                memory_before = get_memory_usage()\n",
    "                \n",
    "                # Run parallel processing\n",
    "                start_time = time.time()\n",
    "                try:\n",
    "                    if 'parallel_app' in globals():\n",
    "                        result = parallel_app.invoke(batch_state)\n",
    "                        \n",
    "                        # Track memory after\n",
    "                        memory_after = get_memory_usage()\n",
    "                        \n",
    "                        # Display results\n",
    "                        parallel_time = result['metrics']['total_wall_time']\n",
    "                        speedup = serial_time / parallel_time if parallel_time > 0 else 0\n",
    "                        \n",
    "                        print(f\"   Time: {parallel_time:.1f}s\")\n",
    "                        print(f\"   Speedup: {speedup:.1f}x faster than serial\")\n",
    "                        print(f\"   Throughput: {result['metrics']['throughput_per_second']:.1f} invoices/sec\")\n",
    "                        print(f\"   Success rate: {result['metrics']['success_rate']:.1%}\")\n",
    "                        print(f\"   Memory increase: {memory_after - memory_before:.1f}MB\")\n",
    "                        \n",
    "                        # Show worker distribution\n",
    "                        worker_results = {}\n",
    "                        for res in result['results']:\n",
    "                            worker = res['processed_by']\n",
    "                            worker_results[worker] = worker_results.get(worker, 0) + 1\n",
    "                        \n",
    "                        print(f\"   Worker distribution: {dict(worker_results)}\")\n",
    "                    else:\n",
    "                        print(\"   ❌ Parallel workflow not compiled\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Parallel processing failed: {e}\")\n",
    "\n",
    "    # Performance summary\n",
    "    print(f\"\\n🎯 KEY INSIGHTS:\")\n",
    "    print(f\"   • Parallel processing can achieve 2-4x speedup\")\n",
    "    print(f\"   • Memory usage increases with worker count\")\n",
    "    print(f\"   • Optimal parallelism depends on workload and resources\")\n",
    "    print(f\"   • Diminishing returns beyond optimal worker count\")\n",
    "\nelse:\n",
    "    print(\"⚠️ No sample invoices available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Memory Monitoring and Resource Management\n",
    "\n",
    "Parallel processing trades memory for speed. Let's monitor resource usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResourceMonitor:\n",
    "    \"\"\"Monitor system resources during parallel processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.measurements = []\n",
    "        self.monitoring = False\n",
    "        self.monitor_thread = None\n",
    "    \n",
    "    def start_monitoring(self, interval=0.5):\n",
    "        \"\"\"Start resource monitoring\"\"\"\n",
    "        self.monitoring = True\n",
    "        self.measurements = []\n",
    "        \n",
    "        def monitor_loop():\n",
    "            while self.monitoring:\n",
    "                measurement = {\n",
    "                    'timestamp': time.time(),\n",
    "                    'memory_mb': get_memory_usage(),\n",
    "                    'cpu_percent': psutil.cpu_percent(),\n",
    "                }\n",
    "                \n",
    "                # Add server metrics if available\n",
    "                server_metrics = get_server_metrics()\n",
    "                if server_metrics.get('status') != 'unavailable':\n",
    "                    gpu_info = server_metrics.get('gpu', {})\n",
    "                    measurement['gpu_memory_mb'] = gpu_info.get('memory_used', 0)\n",
    "                    measurement['gpu_utilization'] = gpu_info.get('utilization', 0)\n",
    "                \n",
    "                self.measurements.append(measurement)\n",
    "                time.sleep(interval)\n",
    "        \n",
    "        self.monitor_thread = threading.Thread(target=monitor_loop)\n",
    "        self.monitor_thread.daemon = True\n",
    "        self.monitor_thread.start()\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Stop resource monitoring\"\"\"\n",
    "        self.monitoring = False\n",
    "        if self.monitor_thread:\n",
    "            self.monitor_thread.join(timeout=1)\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get monitoring summary\"\"\"\n",
    "        if not self.measurements:\n",
    "            return {'status': 'no_data'}\n",
    "        \n",
    "        memory_values = [m['memory_mb'] for m in self.measurements]\n",
    "        cpu_values = [m['cpu_percent'] for m in self.measurements]\n",
    "        \n",
    "        summary = {\n",
    "            'duration': self.measurements[-1]['timestamp'] - self.measurements[0]['timestamp'],\n",
    "            'memory': {\n",
    "                'min': min(memory_values),\n",
    "                'max': max(memory_values),\n",
    "                'avg': sum(memory_values) / len(memory_values),\n",
    "                'peak_increase': max(memory_values) - min(memory_values)\n",
    "            },\n",
    "            'cpu': {\n",
    "                'min': min(cpu_values),\n",
    "                'max': max(cpu_values),\n",
    "                'avg': sum(cpu_values) / len(cpu_values)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add GPU metrics if available\n",
    "        gpu_memory_values = [m.get('gpu_memory_mb', 0) for m in self.measurements]\n",
    "        if any(gpu_memory_values):\n",
    "            summary['gpu_memory'] = {\n",
    "                'min': min(gpu_memory_values),\n",
    "                'max': max(gpu_memory_values),\n",
    "                'avg': sum(gpu_memory_values) / len(gpu_memory_values)\n",
    "            }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Demonstrate resource monitoring\nprint(\"💾 RESOURCE MONITORING DEMONSTRATION\")\nprint(\"=\" * 50)\n\nif SAMPLE_INVOICES and len(SAMPLE_INVOICES) >= 5:\n",
    "    # Test with resource monitoring\n",
    "    test_batch = SAMPLE_INVOICES[:5]\n",
    "    \n",
    "    print(\"\\n📊 Monitoring resource usage during parallel processing...\")\n",
    "    \n",
    "    # Start monitoring\n",
    "    monitor = ResourceMonitor()\n",
    "    monitor.start_monitoring(interval=0.2)\n",
    "    \n",
    "    try:\n",
    "        # Run parallel processing\n",
    "        batch_state = create_batch_state(test_batch, parallelism=3)\n",
    "        \n",
    "        if 'parallel_app' in globals():\n",
    "            result = parallel_app.invoke(batch_state)\n",
    "            \n",
    "            # Stop monitoring\n",
    "            monitor.stop_monitoring()\n",
    "            \n",
    "            # Get monitoring summary\n",
    "            resource_summary = monitor.get_summary()\n",
    "            \n",
    "            print(f\"\\n📈 Resource Usage Summary:\")\n",
    "            print(f\"   Duration: {resource_summary['duration']:.1f}s\")\n",
    "            \n",
    "            memory_stats = resource_summary['memory']\n",
    "            print(f\"   Memory: {memory_stats['min']:.1f} - {memory_stats['max']:.1f}MB (avg: {memory_stats['avg']:.1f}MB)\")\n",
    "            print(f\"   Peak memory increase: {memory_stats['peak_increase']:.1f}MB\")\n",
    "            \n",
    "            cpu_stats = resource_summary['cpu']\n",
    "            print(f\"   CPU: {cpu_stats['min']:.1f}% - {cpu_stats['max']:.1f}% (avg: {cpu_stats['avg']:.1f}%)\")\n",
    "            \n",
    "            if 'gpu_memory' in resource_summary:\n",
    "                gpu_stats = resource_summary['gpu_memory']\n",
    "                print(f\"   GPU Memory: {gpu_stats['min']:.1f} - {gpu_stats['max']:.1f}MB (avg: {gpu_stats['avg']:.1f}MB)\")\n",
    "            \n",
    "            # Performance insights\n",
    "            print(f\"\\n🔍 Performance Insights:\")\n",
    "            processing_efficiency = result['metrics']['success_rate'] * result['metrics']['throughput_per_second']\n",
    "            memory_efficiency = processing_efficiency / memory_stats['peak_increase'] if memory_stats['peak_increase'] > 0 else 0\n",
    "            \n",
    "            print(f\"   Processing efficiency: {processing_efficiency:.2f}\")\n",
    "            print(f\"   Memory efficiency: {memory_efficiency:.3f} (higher is better)\")\n",
    "            print(f\"   Resource utilization: {'High' if cpu_stats['avg'] > 50 else 'Moderate' if cpu_stats['avg'] > 20 else 'Low'}\")\n",
    "        \n",
    "        else:\n",
    "            monitor.stop_monitoring()\n",
    "            print(\"❌ Parallel workflow not available\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        monitor.stop_monitoring()\n",
    "        print(f\"❌ Monitoring test failed: {e}\")\nelse:\n",
    "    print(\"⚠️ Insufficient sample data for monitoring test\")\n\n# Resource optimization recommendations\nprint(f\"\\n💡 OPTIMIZATION RECOMMENDATIONS:\")\nprint(f\"   🔧 Memory Management:\")\nprint(f\"      • Monitor peak memory usage vs available RAM\")\nprint(f\"      • Reduce parallelism if memory becomes constrained\")\nprint(f\"      • Use batch processing for very large workloads\")\nprint(f\"   ⚡ Performance Tuning:\")\nprint(f\"      • Optimal worker count ≈ CPU cores for CPU-bound tasks\")\nprint(f\"      • For I/O-bound tasks, can exceed CPU core count\")\nprint(f\"      • Monitor GPU memory if using GPU-accelerated models\")\nprint(f\"   📊 Monitoring Strategy:\")\nprint(f\"      • Track memory, CPU, and GPU utilization\")\nprint(f\"      • Set alerts for resource exhaustion\")\nprint(f\"      • Adjust parallelism based on real-world performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Handling Partial Failures\n",
    "\n",
    "In parallel processing, some workers may fail while others succeed. Let's demonstrate resilient handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unreliable_worker(worker_state: Dict[str, Any]) -> BatchState:\n",
    "    \"\"\"Simulated unreliable worker that fails sometimes\"\"\"\n",
    "    worker_id = worker_state['worker_id']\n",
    "    invoices = worker_state['invoices']\n",
    "    batch_id = worker_state['batch_id']\n",
    "    start_time = worker_state['worker_start_time']\n",
    "    \n",
    "    print(f\"\\n⚠️ {worker_id} starting (unreliable mode)...\")\n",
    "    \n",
    "    # Simulate worker failure 30% of the time\n",
    "    import random\n",
    "    if random.random() < 0.3:\n",
    "        error_msg = f\"{worker_id} failed due to simulated system error\"\n",
    "        print(f\"   ❌ {error_msg}\")\n",
    "        \n",
    "        # Return partial failure state\n",
    "        return BatchState(\n",
    "            invoice_queue=[],\n",
    "            processing={},\n",
    "            results=[],\n",
    "            errors=[error_msg],\n",
    "            metrics={'total_errors': len(invoices), 'total_time': time.time() - start_time},\n",
    "            batch_id=batch_id,\n",
    "            started_at=None,\n",
    "            completed_at=None,\n",
    "            parallelism_level=0\n",
    "        )\n",
    "    \n",
    "    # Otherwise, process normally but with some individual failures\n",
    "    worker_results = []\n",
    "    worker_errors = []\n",
    "    \n",
    "    for invoice in invoices:\n",
    "        invoice_id = invoice['invoice_id']\n",
    "        \n",
    "        # Simulate individual invoice failures 20% of the time\n",
    "        if random.random() < 0.2:\n",
    "            error_msg = f\"Failed to process {invoice_id} - simulated error\"\n",
    "            worker_errors.append(error_msg)\n",
    "            print(f\"   ❌ {worker_id}: {error_msg}\")\n",
    "        else:\n",
    "            # Process successfully\n",
    "            time.sleep(0.1)  # Simulate work\n",
    "            \n",
    "            result = {\n",
    "                'invoice_id': invoice_id,\n",
    "                'status': 'completed',\n",
    "                'processed_by': worker_id,\n",
    "                'extracted_data': f\"Successfully processed {invoice_id}\",\n",
    "                'original_invoice': invoice\n",
    "            }\n",
    "            \n",
    "            worker_results.append(result)\n",
    "            print(f\"   ✅ {worker_id}: completed {invoice_id}\")\n",
    "    \n",
    "    worker_time = time.time() - start_time\n",
    "    \n",
    "    return BatchState(\n",
    "        invoice_queue=[],\n",
    "        processing={},\n",
    "        results=worker_results,\n",
    "        errors=worker_errors,\n",
    "        metrics={\n",
    "            'total_processed': len(worker_results),\n",
    "            'total_errors': len(worker_errors),\n",
    "            'total_time': worker_time\n",
    "        },\n",
    "        batch_id=batch_id,\n",
    "        started_at=None,\n",
    "        completed_at=None,\n",
    "        parallelism_level=0\n",
    "    )\n",
    "\n",
    "def resilient_aggregator(state: BatchState) -> BatchState:\n",
    "    \"\"\"Aggregator that handles partial failures gracefully\"\"\"\n",
    "    print(f\"\\n🛡️ Resilient aggregator processing batch {state['batch_id']}\")\n",
    "    \n",
    "    # Mark completion\n",
    "    state['completed_at'] = datetime.now().isoformat()\n",
    "    \n",
    "    # Analyze results\n",
    "    total_expected = len(state['invoice_queue']) if state['invoice_queue'] else state['metrics']['total_processed'] + len(state['errors'])\n",
    "    successful = len(state['results'])\n",
    "    failed = len(state['errors'])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    success_rate = successful / max(1, total_expected)\n",
    "    \n",
    "    # Determine if partial success is acceptable\n",
    "    if success_rate >= 0.7:  # 70% success threshold\n",
    "        overall_status = \"SUCCESS_WITH_PARTIAL_FAILURES\"\n",
    "        recommendation = \"Proceed with successful results, retry failed items\"\n",
    "    elif success_rate >= 0.3:  # 30% success threshold\n",
    "        overall_status = \"PARTIAL_SUCCESS\"\n",
    "        recommendation = \"Review failures, may need system adjustments\"\n",
    "    else:\n",
    "        overall_status = \"MOSTLY_FAILED\"\n",
    "        recommendation = \"Investigate system issues before retrying\"\n",
    "    \n",
    "    # Update state with resilience info\n",
    "    state['metrics'].update({\n",
    "        'total_expected': total_expected,\n",
    "        'successful_count': successful,\n",
    "        'failed_count': failed,\n",
    "        'success_rate': success_rate,\n",
    "        'overall_status': overall_status,\n",
    "        'recommendation': recommendation\n",
    "    })\n",
    "    \n",
    "    print(f\"📊 Resilience Analysis:\")\n",
    "    print(f\"   Expected: {total_expected} invoices\")\n",
    "    print(f\"   Successful: {successful} ({success_rate:.1%})\")\n",
    "    print(f\"   Failed: {failed}\")\n",
    "    print(f\"   Status: {overall_status}\")\n",
    "    print(f\"   Recommendation: {recommendation}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Build resilient workflow\nprint(\"🛡️ PARTIAL FAILURE HANDLING DEMONSTRATION\")\nprint(\"=\" * 50)\n\n# Create workflow with unreliable workers\nresilient_workflow = StateGraph(BatchState)\nresilient_workflow.add_node(\"dispatcher\", dispatcher)\nresilient_workflow.add_node(\"unreliable_worker\", unreliable_worker)\nresilient_workflow.add_node(\"resilient_aggregator\", resilient_aggregator)\n\nresilient_workflow.set_entry_point(\"dispatcher\")\nresilient_workflow.add_edge(\"unreliable_worker\", \"resilient_aggregator\")\nresilient_workflow.add_edge(\"resilient_aggregator\", END)\n\ntry:\n    resilient_app = resilient_workflow.compile()\n    print(\"✅ Resilient workflow compiled\")\n    \n    if SAMPLE_INVOICES and len(SAMPLE_INVOICES) >= 6:\n        print(\"\\n🧪 Testing failure resilience...\")\n        \n        # Run multiple tests to show different failure scenarios\n        for test_run in range(3):\n            print(f\"\\n--- Test Run {test_run + 1} ---\")\n            \n            test_batch = SAMPLE_INVOICES[:6]\n            batch_state = create_batch_state(test_batch, parallelism=3)\n            \n            try:\n                result = resilient_app.invoke(batch_state)\n                \n                metrics = result['metrics']\n                print(f\"✅ Test {test_run + 1} Results:\")\n                print(f\"   Status: {metrics['overall_status']}\")\n                print(f\"   Success rate: {metrics['success_rate']:.1%}\")\n                print(f\"   Successful: {metrics['successful_count']}/{metrics['total_expected']}\")\n                \n                if result['errors']:\n                    print(f\"   Errors encountered: {len(result['errors'])}\")\n                    for error in result['errors'][:2]:  # Show first 2 errors\n                        print(f\"     • {error}\")\n                \n            except Exception as e:\n                print(f\"   ❌ Test {test_run + 1} failed: {e}\")\n                \nexcept Exception as e:\n    print(f\"❌ Error compiling resilient workflow: {e}\")\n\n# Failure handling best practices\nprint(f\"\\n💡 FAILURE HANDLING BEST PRACTICES:\")\nprint(f\"   🔄 Retry Strategy:\")\nprint(f\"      • Separate failed items for retry\")\nprint(f\"      • Use exponential backoff for retries\")\nprint(f\"      • Set maximum retry limits\")\nprint(f\"   📊 Monitoring:\")\nprint(f\"      • Track success rates by worker and batch\")\nprint(f\"      • Alert on success rates below thresholds\")\nprint(f\"      • Log detailed error information\")\nprint(f\"   🛡️ Graceful Degradation:\")\nprint(f\"      • Process successful results even with partial failures\")\nprint(f\"      • Provide clear status to downstream systems\")\nprint(f\"      • Enable manual intervention for critical failures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "### What We Demonstrated:\n",
    "\n",
    "1. **Dynamic Parallel Processing with Send API**\n",
    "   - Created workers dynamically based on workload size\n",
    "   - Distributed invoice processing across multiple workers\n",
    "   - Achieved 2-4x performance improvements over serial processing\n",
    "\n",
    "2. **Safe Concurrent State Management**\n",
    "   - Used state reducers to merge updates from parallel workers\n",
    "   - Prevented race conditions and data conflicts\n",
    "   - Maintained data consistency across parallel execution\n",
    "\n",
    "3. **Resource-Aware Scaling**\n",
    "   - Monitored memory and CPU usage during parallel processing\n",
    "   - Demonstrated trade-offs between speed and resource consumption\n",
    "   - Showed how to find optimal parallelism levels\n",
    "\n",
    "4. **Production-Ready Resilience**\n",
    "   - Handled partial worker failures gracefully\n",
    "   - Processed successful results even when some workers failed\n",
    "   - Provided actionable recommendations based on success rates\n",
    "\n",
    "### Performance Insights:\n",
    "\n",
    "**Serial Processing:**\n",
    "- Predictable resource usage\n",
    "- Linear scaling with workload size\n",
    "- Simple error handling\n",
    "\n",
    "**Parallel Processing:**\n",
    "- 2-4x faster execution for batch workloads\n",
    "- Higher memory usage (one model instance per worker)\n",
    "- More complex state management requirements\n",
    "\n",
    "**Optimal Parallelism:**\n",
    "- Sweet spot typically 2-4x CPU cores for I/O-bound tasks\n",
    "- Memory constraints may limit maximum parallelism\n",
    "- Diminishing returns beyond optimal worker count\n",
    "\n",
    "### Production Considerations:\n",
    "\n",
    "**When to Use Parallel Processing:**\n",
    "- Large batch workloads (>10 documents)\n",
    "- I/O-bound processing (API calls, file operations)\n",
    "- Independent document processing tasks\n",
    "\n",
    "**When to Use Serial Processing:**\n",
    "- Small batches (<5 documents)\n",
    "- Memory-constrained environments\n",
    "- Tasks requiring strict ordering\n",
    "\n",
    "**Monitoring Requirements:**\n",
    "- Track resource utilization (CPU, memory, GPU)\n",
    "- Monitor success rates and error patterns\n",
    "- Measure throughput and latency metrics\n",
    "- Set alerts for resource exhaustion\n",
    "\n",
    "### LangGraph Send API Benefits:\n",
    "\n",
    "- **Dynamic Worker Creation:** Create workers based on actual workload\n",
    "- **Automatic Load Distribution:** Evenly distribute work across workers\n",
    "- **State Management:** Built-in support for concurrent state updates\n",
    "- **Error Isolation:** Worker failures don't crash entire batch\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "This parallel processing foundation enables:\n",
    "- Enterprise-scale document processing pipelines\n",
    "- Real-time batch processing systems\n",
    "- Scalable AI-powered workflows\n",
    "- Production-ready invoice processing services\n",
    "\n",
    "The combination of LangGraph's Send API with proper state management and resource monitoring creates a robust foundation for scaling AI applications to handle real-world enterprise workloads efficiently and reliably."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}