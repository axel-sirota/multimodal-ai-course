{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Day 2, Session 4 - Demo: Adaptive Prompt System\n",
    "\n",
    "## The Problem with Static Prompts\n",
    "\n",
    "- Same prompt fails on different invoice formats\n",
    "- No learning from failures\n",
    "- Token waste on verbose prompts\n",
    "- No performance tracking\n",
    "\n",
    "**Solution: Adaptive prompt system that evolves!**\n",
    "\n",
    "This demo shows how to build prompts that:\n",
    "- Adapt based on extraction errors\n",
    "- Progressively enhance for difficult documents\n",
    "- A/B test different versions\n",
    "- Optimize token usage\n",
    "- Track performance in real-time\n",
    "\n",
    "**Duration: 15 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration - Instructor will fill these\n",
    "OLLAMA_URL = \"http://XX.XX.XX.XX\"  # Course server IP (port 80)\n",
    "API_TOKEN = \"YOUR_TOKEN_HERE\"      # Instructor provides token\n",
    "MODEL = \"qwen3:8b\"                  # Default model on server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain instructor pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from instructor import patch\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Optional, Dict, Any\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import threading\n",
    "from functools import lru_cache\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Invoice Dataset\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "dropbox_url = \"https://www.dropbox.com/scl/fo/m9hyfmvi78snwv0nh34mo/AMEXxwXMLAOeve-_yj12ck8?rlkey=urinkikgiuven0fro7r4x5rcu&st=hv3of7g7&dl=1\"\n",
    "\n",
    "print(f\"Downloading data from: {dropbox_url}\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(dropbox_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "        z.extractall(\"downloaded_images\")\n",
    "\n",
    "    print(\"✅ Downloaded and extracted images to 'downloaded_images' folder.\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"❌ Error downloading the file: {e}\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(\"❌ Error: The downloaded file is not a valid zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic_models",
   "metadata": {},
   "source": [
    "## 1. Define Structured Output with Pydantic\n",
    "\n",
    "First, we define exactly what we want to extract using Pydantic models for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "models",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineItem(BaseModel):\n",
    "    \"\"\"Individual line item from invoice\"\"\"\n",
    "    description: str = Field(description=\"Product or service description\")\n",
    "    quantity: int = Field(description=\"Number of items\", ge=0)\n",
    "    unit_price: float = Field(description=\"Price per unit\", ge=0)\n",
    "    total: float = Field(description=\"Line total amount\", ge=0)\n",
    "\n",
    "class InvoiceData(BaseModel):\n",
    "    \"\"\"Complete invoice extraction model\"\"\"\n",
    "    invoice_number: str = Field(description=\"Invoice identifier\")\n",
    "    vendor_name: str = Field(description=\"Company issuing invoice\")\n",
    "    invoice_date: str = Field(description=\"Invoice date in YYYY-MM-DD format\")\n",
    "    line_items: List[LineItem] = Field(description=\"List of invoice line items\")\n",
    "    subtotal: float = Field(description=\"Subtotal before tax\", ge=0)\n",
    "    tax_amount: float = Field(description=\"Tax amount\", ge=0)\n",
    "    total_amount: float = Field(description=\"Total amount due\", ge=0)\n",
    "    currency: str = Field(description=\"Currency code (USD, EUR, etc)\")\n",
    "    confidence: float = Field(description=\"Extraction confidence 0-1\", ge=0, le=1)\n",
    "\n",
    "class ExtractionResult(BaseModel):\n",
    "    \"\"\"Wrapper for extraction with metadata\"\"\"\n",
    "    success: bool\n",
    "    data: Optional[InvoiceData] = None\n",
    "    error: Optional[str] = None\n",
    "    tokens_used: int = 0\n",
    "    processing_time: float = 0\n",
    "    prompt_version: str = \"unknown\"\n",
    "\n",
    "print(\"✅ Pydantic models defined for structured extraction\")\n",
    "print(f\"   InvoiceData has {len(InvoiceData.__fields__)} fields\")\n",
    "print(f\"   LineItem has {len(LineItem.__fields__)} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt_templates",
   "metadata": {},
   "source": [
    "## 2. Create Adaptive Prompt Templates\n",
    "\n",
    "We'll create a hierarchy of prompts that get progressively more sophisticated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 1: Basic prompt\n",
    "basic_prompt = PromptTemplate(\n",
    "    template=\"\"\"Extract invoice data from the following text:\n",
    "\n",
    "{text}\n",
    "\n",
    "Return valid JSON matching the InvoiceData schema.\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Level 2: Enhanced with examples\n",
    "enhanced_prompt = PromptTemplate(\n",
    "    template=\"\"\"Extract invoice data from text. Use these examples as guidance:\n",
    "\n",
    "EXAMPLES:\n",
    "{examples}\n",
    "\n",
    "Now extract from this text:\n",
    "{text}\n",
    "\n",
    "Return valid JSON matching the InvoiceData schema with high confidence scores.\"\"\",\n",
    "    input_variables=[\"text\", \"examples\"]\n",
    ")\n",
    "\n",
    "# Level 3: Adaptive with error feedback\n",
    "adaptive_prompt = PromptTemplate(\n",
    "    template=\"\"\"PREVIOUS EXTRACTION FAILED with error: {error}\n",
    "\n",
    "The following fields had problems: {problem_fields}\n",
    "\n",
    "Try again with better parsing. Pay special attention to:\n",
    "- Number formatting (remove commas, handle decimals)\n",
    "- Date formats (convert to YYYY-MM-DD)\n",
    "- Currency symbols (extract amounts only)\n",
    "- Line item structure (ensure quantity × unit_price = total)\n",
    "\n",
    "Text to extract from:\n",
    "{text}\n",
    "\n",
    "Return valid JSON matching the InvoiceData schema.\"\"\",\n",
    "    input_variables=[\"text\", \"error\", \"problem_fields\"]\n",
    ")\n",
    "\n",
    "# Level 4: Chain-of-thought reasoning\n",
    "cot_prompt = PromptTemplate(\n",
    "    template=\"\"\"Extract invoice data step by step:\n",
    "\n",
    "1. First, identify the vendor name and invoice number\n",
    "2. Find the invoice date and convert to YYYY-MM-DD format\n",
    "3. Locate the line items table and extract each row\n",
    "4. For each line item, verify: quantity × unit_price = total\n",
    "5. Find subtotal, tax, and total amounts\n",
    "6. Verify: subtotal + tax = total_amount\n",
    "7. Assign confidence based on data quality\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Think through each step, then return valid JSON.\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Store prompt levels\n",
    "PROMPT_LEVELS = {\n",
    "    'basic': basic_prompt,\n",
    "    'enhanced': enhanced_prompt,\n",
    "    'adaptive': adaptive_prompt,\n",
    "    'cot': cot_prompt\n",
    "}\n",
    "\n",
    "print(\"🎯 Created 4 levels of prompt sophistication:\")\n",
    "for level, prompt in PROMPT_LEVELS.items():\n",
    "    token_estimate = len(prompt.template.split())\n",
    "    print(f\"   {level}: ~{token_estimate} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm_interface",
   "metadata": {},
   "source": [
    "## 3. LLM Interface with Metrics Tracking\n",
    "\n",
    "Create a wrapper for our LLM that tracks performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm_wrapper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveLLM:\n",
    "    \"\"\"LLM wrapper with adaptive prompting and metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, url, token, model):\n",
    "        self.url = url\n",
    "        self.token = token\n",
    "        self.model = model\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.examples_cache = []\n",
    "        \n",
    "    def call_llm(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Make API call to LLM\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": prompt\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.url}/think\",\n",
    "                headers=headers,\n",
    "                json=data\n",
    "            )\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return {\n",
    "                    'response': result.get('response', ''),\n",
    "                    'tokens_used': len(prompt.split()) + len(result.get('response', '').split()),\n",
    "                    'processing_time': processing_time,\n",
    "                    'success': True\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'error': f\"HTTP {response.status_code}\",\n",
    "                    'tokens_used': len(prompt.split()),\n",
    "                    'processing_time': processing_time,\n",
    "                    'success': False\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'tokens_used': len(prompt.split()),\n",
    "                'processing_time': time.time() - start_time,\n",
    "                'success': False\n",
    "            }\n",
    "    \n",
    "    def extract_with_validation(self, text: str, prompt_level: str = 'basic') -> ExtractionResult:\n",
    "        \"\"\"Extract with Pydantic validation\"\"\"\n",
    "        \n",
    "        # Get the appropriate prompt\n",
    "        prompt_template = PROMPT_LEVELS[prompt_level]\n",
    "        \n",
    "        # Handle different prompt types\n",
    "        if prompt_level == 'enhanced':\n",
    "            examples = self._get_examples()\n",
    "            prompt_text = prompt_template.format(text=text, examples=examples)\n",
    "        elif prompt_level == 'adaptive':\n",
    "            # This would be called with error context\n",
    "            prompt_text = prompt_template.format(\n",
    "                text=text, \n",
    "                error=\"Previous validation failed\", \n",
    "                problem_fields=\"amount formatting\"\n",
    "            )\n",
    "        else:\n",
    "            prompt_text = prompt_template.format(text=text)\n",
    "        \n",
    "        # Call LLM\n",
    "        llm_result = self.call_llm(prompt_text)\n",
    "        \n",
    "        if not llm_result['success']:\n",
    "            return ExtractionResult(\n",
    "                success=False,\n",
    "                error=llm_result['error'],\n",
    "                tokens_used=llm_result['tokens_used'],\n",
    "                processing_time=llm_result['processing_time'],\n",
    "                prompt_version=prompt_level\n",
    "            )\n",
    "        \n",
    "        # Try to parse and validate JSON\n",
    "        try:\n",
    "            # Extract JSON from response\n",
    "            response_text = llm_result['response']\n",
    "            json_start = response_text.find('{')\n",
    "            json_end = response_text.rfind('}') + 1\n",
    "            \n",
    "            if json_start == -1 or json_end == 0:\n",
    "                raise ValueError(\"No JSON found in response\")\n",
    "            \n",
    "            json_text = response_text[json_start:json_end]\n",
    "            raw_data = json.loads(json_text)\n",
    "            \n",
    "            # Validate with Pydantic\n",
    "            invoice_data = InvoiceData(**raw_data)\n",
    "            \n",
    "            # Store successful example\n",
    "            self._cache_successful_example(text, invoice_data)\n",
    "            \n",
    "            # Track metrics\n",
    "            self._record_success(prompt_level, llm_result)\n",
    "            \n",
    "            return ExtractionResult(\n",
    "                success=True,\n",
    "                data=invoice_data,\n",
    "                tokens_used=llm_result['tokens_used'],\n",
    "                processing_time=llm_result['processing_time'],\n",
    "                prompt_version=prompt_level\n",
    "            )\n",
    "            \n",
    "        except (json.JSONDecodeError, ValidationError, ValueError) as e:\n",
    "            # Track failure\n",
    "            self._record_failure(prompt_level, str(e), llm_result)\n",
    "            \n",
    "            return ExtractionResult(\n",
    "                success=False,\n",
    "                error=f\"Validation failed: {str(e)}\",\n",
    "                tokens_used=llm_result['tokens_used'],\n",
    "                processing_time=llm_result['processing_time'],\n",
    "                prompt_version=prompt_level\n",
    "            )\n",
    "    \n",
    "    def _get_examples(self) -> str:\n",
    "        \"\"\"Get cached successful examples\"\"\"\n",
    "        if not self.examples_cache:\n",
    "            return \"No examples available yet.\"\n",
    "        \n",
    "        # Return the most recent successful example\n",
    "        example = self.examples_cache[-1]\n",
    "        return f\"Input: {example['text'][:100]}...\\nOutput: {example['output']}\"\n",
    "    \n",
    "    def _cache_successful_example(self, text: str, result: InvoiceData):\n",
    "        \"\"\"Cache successful extractions as examples\"\"\"\n",
    "        self.examples_cache.append({\n",
    "            'text': text,\n",
    "            'output': result.json()\n",
    "        })\n",
    "        \n",
    "        # Keep only recent examples\n",
    "        if len(self.examples_cache) > 5:\n",
    "            self.examples_cache.pop(0)\n",
    "    \n",
    "    def _record_success(self, prompt_level: str, llm_result: Dict):\n",
    "        \"\"\"Record successful extraction metrics\"\"\"\n",
    "        self.metrics[f'{prompt_level}_success'].append(1)\n",
    "        self.metrics[f'{prompt_level}_tokens'].append(llm_result['tokens_used'])\n",
    "        self.metrics[f'{prompt_level}_time'].append(llm_result['processing_time'])\n",
    "    \n",
    "    def _record_failure(self, prompt_level: str, error: str, llm_result: Dict):\n",
    "        \"\"\"Record failed extraction metrics\"\"\"\n",
    "        self.metrics[f'{prompt_level}_success'].append(0)\n",
    "        self.metrics[f'{prompt_level}_tokens'].append(llm_result['tokens_used'])\n",
    "        self.metrics[f'{prompt_level}_time'].append(llm_result['processing_time'])\n",
    "        self.metrics[f'{prompt_level}_errors'].append(error)\n",
    "\n",
    "# Initialize adaptive LLM\n",
    "adaptive_llm = AdaptiveLLM(OLLAMA_URL, API_TOKEN, MODEL)\n",
    "print(\"🤖 Adaptive LLM initialized with metrics tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive_enhancement",
   "metadata": {},
   "source": [
    "## 4. Progressive Enhancement Engine\n",
    "\n",
    "This system tries different prompt levels until extraction succeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive_enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_with_enhancement(text: str, max_attempts: int = 3) -> ExtractionResult:\n",
    "    \"\"\"\n",
    "    Progressively enhance prompt on failure\n",
    "    Start basic → enhanced → adaptive → chain-of-thought\n",
    "    \"\"\"\n",
    "    \n",
    "    enhancement_levels = ['basic', 'enhanced', 'cot', 'adaptive']\n",
    "    \n",
    "    print(f\"🎯 Starting progressive enhancement for text excerpt: '{text[:50]}...'\")\n",
    "    \n",
    "    for attempt, level in enumerate(enhancement_levels[:max_attempts]):\n",
    "        print(f\"\\n   Attempt {attempt + 1}: Using '{level}' prompt\")\n",
    "        \n",
    "        result = adaptive_llm.extract_with_validation(text, level)\n",
    "        \n",
    "        print(f\"   ⏱️  Time: {result.processing_time:.2f}s, Tokens: {result.tokens_used}\")\n",
    "        \n",
    "        if result.success:\n",
    "            print(f\"   ✅ Success with '{level}' prompt!\")\n",
    "            print(f\"   📊 Confidence: {result.data.confidence:.2f}\")\n",
    "            print(f\"   💰 Total amount: {result.data.total_amount} {result.data.currency}\")\n",
    "            return result\n",
    "        else:\n",
    "            print(f\"   ❌ Failed: {result.error[:100]}...\")\n",
    "    \n",
    "    print(f\"\\n💥 All enhancement levels failed after {max_attempts} attempts\")\n",
    "    return result  # Return last attempt\n",
    "\n",
    "# Test with sample invoice text\n",
    "sample_invoice = \"\"\"\n",
    "INVOICE #INV-2024-001\n",
    "TechSupplies Inc.\n",
    "Date: January 15, 2024\n",
    "\n",
    "Line Items:\n",
    "1. Laptop Computer - Qty: 2 - Unit Price: $1,200.00 - Total: $2,400.00\n",
    "2. Software License - Qty: 1 - Unit Price: $500.00 - Total: $500.00\n",
    "\n",
    "Subtotal: $2,900.00\n",
    "Tax (8.5%): $246.50\n",
    "TOTAL: $3,146.50\n",
    "\"\"\"\n",
    "\n",
    "print(\"🧪 Testing Progressive Enhancement:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test the enhancement system\n",
    "enhancement_result = extract_with_enhancement(sample_invoice)\n",
    "\n",
    "if enhancement_result.success:\n",
    "    print(f\"\\n🎉 Final Result:\")\n",
    "    print(f\"   Vendor: {enhancement_result.data.vendor_name}\")\n",
    "    print(f\"   Invoice #: {enhancement_result.data.invoice_number}\")\n",
    "    print(f\"   Items: {len(enhancement_result.data.line_items)}\")\n",
    "    print(f\"   Total: {enhancement_result.data.total_amount} {enhancement_result.data.currency}\")\n",
    "else:\n",
    "    print(f\"\\n💥 Final failure: {enhancement_result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab_testing",
   "metadata": {},
   "source": [
    "## 5. A/B Testing Framework\n",
    "\n",
    "Test different prompt variants to find the best performing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab_testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptVariant:\n",
    "    \"\"\"A/B testing variant for prompts\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, template: PromptTemplate):\n",
    "        self.name = name\n",
    "        self.template = template\n",
    "        self.attempts = 0\n",
    "        self.successes = 0\n",
    "        self.total_tokens = 0\n",
    "        self.total_time = 0\n",
    "        self.errors = []\n",
    "    \n",
    "    @property\n",
    "    def success_rate(self) -> float:\n",
    "        return self.successes / self.attempts if self.attempts > 0 else 0\n",
    "    \n",
    "    @property\n",
    "    def avg_tokens(self) -> float:\n",
    "        return self.total_tokens / self.attempts if self.attempts > 0 else 0\n",
    "    \n",
    "    @property\n",
    "    def avg_time(self) -> float:\n",
    "        return self.total_time / self.attempts if self.attempts > 0 else 0\n",
    "    \n",
    "    def record_result(self, result: ExtractionResult):\n",
    "        \"\"\"Record test result\"\"\"\n",
    "        self.attempts += 1\n",
    "        self.total_tokens += result.tokens_used\n",
    "        self.total_time += result.processing_time\n",
    "        \n",
    "        if result.success:\n",
    "            self.successes += 1\n",
    "        else:\n",
    "            self.errors.append(result.error)\n",
    "\n",
    "# Create test variants\n",
    "concise_template = PromptTemplate(\n",
    "    template=\"Extract invoice data concisely: {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "detailed_template = PromptTemplate(\n",
    "    template=\"\"\"Extract complete invoice information from the following document.\n",
    "    \n",
    "    Please provide:\n",
    "    - Invoice number and vendor name\n",
    "    - Complete line item details with quantities and prices\n",
    "    - All financial totals including tax calculations\n",
    "    - Currency information\n",
    "    - Your confidence in the extraction accuracy\n",
    "    \n",
    "    Document text:\n",
    "    {text}\n",
    "    \n",
    "    Return as valid JSON matching InvoiceData schema.\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Initialize variants\n",
    "variants = {\n",
    "    'concise': PromptVariant('concise', concise_template),\n",
    "    'detailed': PromptVariant('detailed', detailed_template),\n",
    "    'basic': PromptVariant('basic', basic_prompt),\n",
    "    'cot': PromptVariant('cot', cot_prompt)\n",
    "}\n",
    "\n",
    "def run_ab_test(test_texts: List[str], variants: Dict[str, PromptVariant]) -> Dict[str, Dict]:\n",
    "    \"\"\"Run A/B test across multiple prompt variants\"\"\"\n",
    "    \n",
    "    print(f\"🧪 Running A/B test with {len(variants)} variants on {len(test_texts)} samples\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for variant_name, variant in variants.items():\n",
    "        print(f\"\\n📊 Testing variant: {variant_name}\")\n",
    "        \n",
    "        for i, text in enumerate(test_texts):\n",
    "            print(f\"   Sample {i+1}/{len(test_texts)}: \", end=\"\")\n",
    "            \n",
    "            # Simulate extraction with this variant\n",
    "            if variant_name in ['basic', 'cot']:  # Use existing system\n",
    "                result = adaptive_llm.extract_with_validation(text, variant_name)\n",
    "            else:  # Simulate for custom variants\n",
    "                result = ExtractionResult(\n",
    "                    success=random.random() > 0.3,  # 70% success rate simulation\n",
    "                    tokens_used=random.randint(100, 500),\n",
    "                    processing_time=random.uniform(0.5, 3.0),\n",
    "                    prompt_version=variant_name,\n",
    "                    error=\"Simulated parsing error\" if random.random() < 0.3 else None\n",
    "                )\n",
    "            \n",
    "            variant.record_result(result)\n",
    "            print(\"✅\" if result.success else \"❌\")\n",
    "        \n",
    "        # Store results\n",
    "        results[variant_name] = {\n",
    "            'success_rate': variant.success_rate,\n",
    "            'avg_tokens': variant.avg_tokens,\n",
    "            'avg_time': variant.avg_time,\n",
    "            'attempts': variant.attempts\n",
    "        }\n",
    "        \n",
    "        print(f\"   Results: {variant.success_rate:.1%} success, {variant.avg_tokens:.0f} tokens avg\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create test samples\n",
    "test_samples = [\n",
    "    sample_invoice,\n",
    "    \"Invoice 123, ABC Corp, $500 total\",  # Minimal\n",
    "    \"Complex multi-page invoice with 20 line items...\",  # Complex\n",
    "]\n",
    "\n",
    "# Run A/B test\n",
    "ab_results = run_ab_test(test_samples, variants)\n",
    "\n",
    "print(f\"\\n📈 A/B Test Summary:\")\n",
    "print(\"=\" * 40)\n",
    "for variant_name, metrics in ab_results.items():\n",
    "    print(f\"{variant_name:10} | {metrics['success_rate']:.1%} success | {metrics['avg_tokens']:.0f} tokens | {metrics['avg_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance_dashboard",
   "metadata": {},
   "source": [
    "## 6. Real-time Performance Dashboard\n",
    "\n",
    "Visualize prompt performance metrics in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dashboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_performance_dashboard(variants: Dict[str, PromptVariant]):\n",
    "    \"\"\"Display comprehensive performance dashboard\"\"\"\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    variant_names = list(variants.keys())\n",
    "    success_rates = [v.success_rate for v in variants.values()]\n",
    "    avg_tokens = [v.avg_tokens for v in variants.values()]\n",
    "    avg_times = [v.avg_time for v in variants.values()]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Adaptive Prompt Performance Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Success Rate Comparison\n",
    "    bars1 = ax1.bar(variant_names, success_rates, color=['#2ecc71', '#3498db', '#e74c3c', '#f39c12'])\n",
    "    ax1.set_title('Success Rate by Prompt Variant')\n",
    "    ax1.set_ylabel('Success Rate (%)')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, rate in zip(bars1, success_rates):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{rate:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Token Usage Efficiency\n",
    "    bars2 = ax2.bar(variant_names, avg_tokens, color=['#9b59b6', '#34495e', '#e67e22', '#1abc9c'])\n",
    "    ax2.set_title('Average Tokens Used')\n",
    "    ax2.set_ylabel('Tokens')\n",
    "    \n",
    "    for bar, tokens in zip(bars2, avg_tokens):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                f'{tokens:.0f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Processing Time\n",
    "    bars3 = ax3.bar(variant_names, avg_times, color=['#e74c3c', '#f39c12', '#2ecc71', '#3498db'])\n",
    "    ax3.set_title('Average Processing Time')\n",
    "    ax3.set_ylabel('Seconds')\n",
    "    \n",
    "    for bar, time_val in zip(bars3, avg_times):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{time_val:.2f}s', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Efficiency Score (Success Rate / Tokens)\n",
    "    efficiency_scores = [sr / (at/100) if at > 0 else 0 for sr, at in zip(success_rates, avg_tokens)]\n",
    "    bars4 = ax4.bar(variant_names, efficiency_scores, color=['#95a5a6', '#2c3e50', '#8e44ad', '#27ae60'])\n",
    "    ax4.set_title('Efficiency Score (Success Rate / Token Cost)')\n",
    "    ax4.set_ylabel('Efficiency')\n",
    "    \n",
    "    for bar, eff in zip(bars4, efficiency_scores):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{eff:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\n📊 Performance Summary Table:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Variant':<12} {'Success Rate':<12} {'Avg Tokens':<12} {'Avg Time':<12} {'Efficiency':<12}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, name in enumerate(variant_names):\n",
    "        print(f\"{name:<12} {success_rates[i]:<12.1%} {avg_tokens[i]:<12.0f} {avg_times[i]:<12.2f} {efficiency_scores[i]:<12.2f}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    best_success = variant_names[success_rates.index(max(success_rates))]\n",
    "    most_efficient = variant_names[efficiency_scores.index(max(efficiency_scores))]\n",
    "    fastest = variant_names[avg_times.index(min(avg_times))]\n",
    "    \n",
    "    print(f\"\\n🎯 Recommendations:\")\n",
    "    print(f\"   Best Success Rate: {best_success} ({max(success_rates):.1%})\")\n",
    "    print(f\"   Most Efficient: {most_efficient} ({max(efficiency_scores):.2f})\")\n",
    "    print(f\"   Fastest: {fastest} ({min(avg_times):.2f}s)\")\n",
    "\n",
    "# Display the dashboard\n",
    "display_performance_dashboard(variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "token_optimization",
   "metadata": {},
   "source": [
    "## 7. Token Usage Optimization\n",
    "\n",
    "Analyze and optimize prompts for token efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_prompt_tokens(template: str, results: List[ExtractionResult]) -> str:\n",
    "    \"\"\"Optimize prompt for token usage while maintaining performance\"\"\"\n",
    "    \n",
    "    print(f\"🔧 Optimizing prompt template...\")\n",
    "    print(f\"   Original length: {len(template)} characters\")\n",
    "    print(f\"   Original tokens: ~{len(template.split())} words\")\n",
    "    \n",
    "    # Calculate current performance\n",
    "    successful_results = [r for r in results if r.success]\n",
    "    success_rate = len(successful_results) / len(results) if results else 0\n",
    "    avg_tokens = sum(r.tokens_used for r in results) / len(results) if results else 0\n",
    "    \n",
    "    print(f\"   Current success rate: {success_rate:.1%}\")\n",
    "    print(f\"   Current avg tokens: {avg_tokens:.0f}\")\n",
    "    \n",
    "    # Optimization strategies\n",
    "    optimizations = {\n",
    "        'remove_redundancy': template.replace('Return valid JSON matching the InvoiceData schema.', 'Return JSON.'),\n",
    "        'compress_examples': template.replace('Use these examples as guidance:', 'Examples:'),\n",
    "        'shorten_instructions': template.replace(\n",
    "            'Pay special attention to:', 'Focus on:'\n",
    "        ),\n",
    "        'abbreviate': template.replace('invoice', 'inv').replace('extraction', 'extract')\n",
    "    }\n",
    "    \n",
    "    # Show optimization results\n",
    "    print(f\"\\n📈 Optimization Strategies:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    best_optimization = None\n",
    "    best_savings = 0\n",
    "    \n",
    "    for strategy, optimized_template in optimizations.items():\n",
    "        original_tokens = len(template.split())\n",
    "        optimized_tokens = len(optimized_template.split())\n",
    "        savings = original_tokens - optimized_tokens\n",
    "        savings_pct = (savings / original_tokens) * 100\n",
    "        \n",
    "        print(f\"{strategy:<20} | {optimized_tokens:>3} tokens | {savings:>3} saved ({savings_pct:>5.1f}%)\")\n",
    "        \n",
    "        if savings > best_savings:\n",
    "            best_savings = savings\n",
    "            best_optimization = optimized_template\n",
    "    \n",
    "    # Cost analysis\n",
    "    token_cost_per_1k = 0.002  # Example cost\n",
    "    daily_requests = 1000  # Example volume\n",
    "    \n",
    "    current_daily_cost = (avg_tokens * daily_requests * token_cost_per_1k) / 1000\n",
    "    optimized_daily_cost = ((avg_tokens - best_savings) * daily_requests * token_cost_per_1k) / 1000\n",
    "    daily_savings = current_daily_cost - optimized_daily_cost\n",
    "    \n",
    "    print(f\"\\n💰 Cost Impact Analysis:\")\n",
    "    print(f\"   Current daily cost: ${current_daily_cost:.2f}\")\n",
    "    print(f\"   Optimized daily cost: ${optimized_daily_cost:.2f}\")\n",
    "    print(f\"   Daily savings: ${daily_savings:.2f} ({(daily_savings/current_daily_cost)*100:.1f}%)\")\n",
    "    print(f\"   Annual savings: ${daily_savings * 365:.2f}\")\n",
    "    \n",
    "    return best_optimization\n",
    "\n",
    "# Simulate some results for optimization\n",
    "mock_results = [\n",
    "    ExtractionResult(success=True, tokens_used=250, processing_time=1.2, prompt_version='detailed'),\n",
    "    ExtractionResult(success=True, tokens_used=180, processing_time=0.8, prompt_version='basic'),\n",
    "    ExtractionResult(success=False, tokens_used=300, processing_time=1.5, prompt_version='cot', error=\"Parse error\"),\n",
    "    ExtractionResult(success=True, tokens_used=220, processing_time=1.0, prompt_version='enhanced'),\n",
    "]\n",
    "\n",
    "# Optimize the detailed template\n",
    "optimized_template = optimize_prompt_tokens(detailed_template.template, mock_results)\n",
    "\n",
    "print(f\"\\n✅ Optimized Template:\")\n",
    "print(\"=\" * 40)\n",
    "print(optimized_template[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caching",
   "metadata": {},
   "source": [
    "## 8. Prompt Caching and Thread Safety\n",
    "\n",
    "Implement caching for frequently used prompts and ensure thread safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caching",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from functools import lru_cache\n",
    "import hashlib\n",
    "from typing import Tuple\n",
    "\n",
    "class ThreadSafePromptCache:\n",
    "    \"\"\"Thread-safe prompt cache with performance metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.lock = threading.RLock()\n",
    "    \n",
    "    def _generate_key(self, template_key: str, **kwargs) -> str:\n",
    "        \"\"\"Generate cache key from template and parameters\"\"\"\n",
    "        # Create a hash of the template and parameters\n",
    "        content = f\"{template_key}:{sorted(kwargs.items())}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def get_prompt(self, template_key: str, **kwargs) -> Tuple[str, bool]:\n",
    "        \"\"\"Get cached prompt or render new one\"\"\"\n",
    "        cache_key = self._generate_key(template_key, **kwargs)\n",
    "        \n",
    "        with self.lock:\n",
    "            if cache_key in self.cache:\n",
    "                self.hits += 1\n",
    "                return self.cache[cache_key], True  # (prompt, was_cached)\n",
    "            \n",
    "            self.misses += 1\n",
    "            \n",
    "            # Render new prompt\n",
    "            if template_key in PROMPT_LEVELS:\n",
    "                template = PROMPT_LEVELS[template_key]\n",
    "                rendered_prompt = template.format(**kwargs)\n",
    "            else:\n",
    "                rendered_prompt = f\"Unknown template: {template_key}\"\n",
    "            \n",
    "            # Cache the result\n",
    "            if len(self.cache) >= self.max_size:\n",
    "                # Remove oldest entry (simple FIFO)\n",
    "                oldest_key = next(iter(self.cache))\n",
    "                del self.cache[oldest_key]\n",
    "            \n",
    "            self.cache[cache_key] = rendered_prompt\n",
    "            return rendered_prompt, False\n",
    "    \n",
    "    @property\n",
    "    def hit_rate(self) -> float:\n",
    "        \"\"\"Calculate cache hit rate\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        return self.hits / total if total > 0 else 0\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        with self.lock:\n",
    "            return {\n",
    "                'hits': self.hits,\n",
    "                'misses': self.misses,\n",
    "                'hit_rate': self.hit_rate,\n",
    "                'cache_size': len(self.cache),\n",
    "                'max_size': self.max_size\n",
    "            }\n",
    "\n",
    "# Initialize prompt cache\n",
    "prompt_cache = ThreadSafePromptCache(max_size=50)\n",
    "\n",
    "def simulate_concurrent_usage():\n",
    "    \"\"\"Simulate multiple threads using the prompt cache\"\"\"\n",
    "    \n",
    "    def worker_thread(thread_id: int, iterations: int):\n",
    "        \"\"\"Simulate a worker thread making prompt requests\"\"\"\n",
    "        for i in range(iterations):\n",
    "            # Simulate different prompt requests\n",
    "            templates = ['basic', 'enhanced', 'cot']\n",
    "            template = random.choice(templates)\n",
    "            \n",
    "            if template == 'enhanced':\n",
    "                prompt, cached = prompt_cache.get_prompt(\n",
    "                    template, \n",
    "                    text=f\"invoice text {i}\", \n",
    "                    examples=\"example data\"\n",
    "                )\n",
    "            else:\n",
    "                prompt, cached = prompt_cache.get_prompt(\n",
    "                    template, \n",
    "                    text=f\"invoice text {i}\"\n",
    "                )\n",
    "            \n",
    "            # Simulate some processing time\n",
    "            time.sleep(0.01)\n",
    "    \n",
    "    print(\"🧵 Testing concurrent prompt cache usage...\")\n",
    "    \n",
    "    # Create and start multiple threads\n",
    "    threads = []\n",
    "    for i in range(5):  # 5 concurrent threads\n",
    "        thread = threading.Thread(target=worker_thread, args=(i, 20))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    \n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    return prompt_cache.get_stats()\n",
    "\n",
    "# Test concurrent cache usage\n",
    "cache_stats = simulate_concurrent_usage()\n",
    "\n",
    "print(f\"\\n📊 Cache Performance Results:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total requests: {cache_stats['hits'] + cache_stats['misses']}\")\n",
    "print(f\"Cache hits: {cache_stats['hits']}\")\n",
    "print(f\"Cache misses: {cache_stats['misses']}\")\n",
    "print(f\"Hit rate: {cache_stats['hit_rate']:.1%}\")\n",
    "print(f\"Cache utilization: {cache_stats['cache_size']}/{cache_stats['max_size']} slots\")\n",
    "\n",
    "# Performance impact calculation\n",
    "rendering_time_saved = cache_stats['hits'] * 0.002  # Assume 2ms saved per cache hit\n",
    "print(f\"\\n⚡ Performance Impact:\")\n",
    "print(f\"Rendering time saved: {rendering_time_saved*1000:.1f}ms\")\n",
    "print(f\"Memory usage: ~{cache_stats['cache_size'] * 0.5:.1f}KB\")\n",
    "\n",
    "if cache_stats['hit_rate'] > 0.5:\n",
    "    print(\"✅ Cache is effective - high hit rate achieved\")\n",
    "else:\n",
    "    print(\"⚠️ Cache hit rate could be improved - consider larger cache size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "live_demo",
   "metadata": {},
   "source": [
    "## 9. Live Demo: Complete Adaptive System\n",
    "\n",
    "Demonstrate the full adaptive prompt system working together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "live_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_adaptive_demo():\n",
    "    \"\"\"Run complete demonstration of adaptive prompt system\"\"\"\n",
    "    \n",
    "    print(\"🎬 LIVE DEMO: Complete Adaptive Prompt System\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Difficult invoice that will challenge the system\n",
    "    difficult_invoice = \"\"\"\n",
    "    FACTURA #F-2024-0156\n",
    "    Empresa: Suministros Técnicos S.A.\n",
    "    Fecha: 15/01/2024\n",
    "    \n",
    "    Artículos:\n",
    "    1) Ordenador portátil - Cant: 2 - Precio unit: 1.200,00€ - Total: 2.400,00€\n",
    "    2) Licencia software - Cant: 1 - Precio unit: 500,00€ - Total: 500,00€\n",
    "    \n",
    "    Subtotal: 2.900,00€\n",
    "    IVA (21%): 609,00€\n",
    "    TOTAL: 3.509,00€\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"📄 Processing challenging invoice (Spanish, different format):\")\n",
    "    print(difficult_invoice[:100] + \"...\")\n",
    "    \n",
    "    # Track system adaptation\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\\n🎯 Adaptive System in Action:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Step 1: Try basic extraction\n",
    "    print(\"\\n1️⃣ Attempting basic extraction...\")\n",
    "    basic_result = adaptive_llm.extract_with_validation(difficult_invoice, 'basic')\n",
    "    \n",
    "    if basic_result.success:\n",
    "        print(\"   ✅ Basic prompt succeeded!\")\n",
    "        final_result = basic_result\n",
    "    else:\n",
    "        print(f\"   ❌ Basic failed: {basic_result.error[:50]}...\")\n",
    "        \n",
    "        # Step 2: Progressive enhancement\n",
    "        print(\"\\n2️⃣ Applying progressive enhancement...\")\n",
    "        final_result = extract_with_enhancement(difficult_invoice, max_attempts=2)\n",
    "    \n",
    "    # Step 3: Cache the successful pattern\n",
    "    if final_result.success:\n",
    "        print(\"\\n3️⃣ Caching successful prompt pattern...\")\n",
    "        successful_prompt, was_cached = prompt_cache.get_prompt(\n",
    "            final_result.prompt_version, \n",
    "            text=difficult_invoice\n",
    "        )\n",
    "        print(f\"   📦 Prompt cached for future similar invoices\")\n",
    "    \n",
    "    # Step 4: Display final metrics\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n📊 Final Results:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    if final_result.success:\n",
    "        print(f\"✅ Extraction successful with '{final_result.prompt_version}' prompt\")\n",
    "        print(f\"   Vendor: {final_result.data.vendor_name}\")\n",
    "        print(f\"   Invoice #: {final_result.data.invoice_number}\")\n",
    "        print(f\"   Total: {final_result.data.total_amount} {final_result.data.currency}\")\n",
    "        print(f\"   Confidence: {final_result.data.confidence:.2f}\")\n",
    "        print(f\"   Items extracted: {len(final_result.data.line_items)}\")\n",
    "    else:\n",
    "        print(f\"❌ All attempts failed: {final_result.error}\")\n",
    "    \n",
    "    print(f\"\\n⚡ Performance Metrics:\")\n",
    "    print(f\"   Total processing time: {total_time:.2f}s\")\n",
    "    print(f\"   Tokens used: {final_result.tokens_used}\")\n",
    "    print(f\"   Cache hit rate: {prompt_cache.hit_rate:.1%}\")\n",
    "    \n",
    "    # Step 5: Show learning impact\n",
    "    print(f\"\\n🧠 System Learning:\")\n",
    "    print(f\"   Examples cached: {len(adaptive_llm.examples_cache)}\")\n",
    "    print(f\"   Successful patterns identified: {final_result.prompt_version}\")\n",
    "    print(f\"   Future similar invoices will use optimized approach\")\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "# Run the complete demo\n",
    "demo_result = run_complete_adaptive_demo()\n",
    "\n",
    "print(f\"\\n🎉 Demo complete! Adaptive system {'succeeded' if demo_result.success else 'needs tuning'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key_learnings",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "### Adaptive Prompting Strategies\n",
    "\n",
    "1. **Progressive Enhancement**\n",
    "   - Start with simple prompts for cost efficiency\n",
    "   - Escalate complexity only when needed\n",
    "   - Each level adds specific capabilities\n",
    "\n",
    "2. **Performance-Driven Optimization**\n",
    "   - A/B test different prompt variants\n",
    "   - Track success rates, token usage, and processing time\n",
    "   - Choose prompts based on efficiency metrics\n",
    "\n",
    "3. **Learning from Failures**\n",
    "   - Capture error patterns to improve prompts\n",
    "   - Use successful examples as few-shot learning\n",
    "   - Adapt prompts based on document characteristics\n",
    "\n",
    "4. **Token Economics**\n",
    "   - Optimize prompts for token efficiency\n",
    "   - Cache frequently used prompt patterns\n",
    "   - Balance verbosity with accuracy needs\n",
    "\n",
    "5. **Structured Validation**\n",
    "   - Use Pydantic models for guaranteed output structure\n",
    "   - Implement retry logic for validation failures\n",
    "   - Track confidence scores for quality assessment\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "- **Thread Safety**: Ensure prompt caching works in concurrent environments\n",
    "- **Memory Management**: Monitor cache size and prompt complexity\n",
    "- **Cost Control**: Track token usage and optimize for efficiency\n",
    "- **Quality Assurance**: Validate outputs and maintain accuracy metrics\n",
    "- **Continuous Learning**: Update prompts based on real-world performance\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Implement prompt versioning for A/B testing in production\n",
    "- Add automatic prompt optimization based on success metrics\n",
    "- Integrate with document classification for prompt selection\n",
    "- Build feedback loops for continuous prompt improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}