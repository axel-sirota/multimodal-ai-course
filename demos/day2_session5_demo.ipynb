{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Day 2, Session 5 - Demo: Voice-Enabled Invoice Assistant\n",
    "\n",
    "## Voice Completes the Multimodal Experience\n",
    "\n",
    "- Users can describe complex requirements verbally\n",
    "- Hands-free operation for warehouse/field workers\n",
    "- Accessibility for visually impaired users\n",
    "- Natural conversation flow with invoices\n",
    "\n",
    "**Today: Architecture overview, not full implementation!**\n",
    "\n",
    "This demo shows the complete architecture for integrating voice capabilities with our invoice processing agent. We'll explore how to combine speech-to-text, our LangGraph agent, and text-to-speech into a seamless conversational experience.\n",
    "\n",
    "**Duration: 15 minutes**\n",
    "\n",
    "**Note**: Due to time constraints, we focus on architecture and integration patterns rather than full implementation. The concepts shown here can be extended for production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration - Instructor will fill these\n",
    "OLLAMA_URL = \"http://XX.XX.XX.XX\"  # Course server IP (port 80)\n",
    "API_TOKEN = \"YOUR_TOKEN_HERE\"      # Instructor provides token\n",
    "MODEL = \"qwen3:8b\"                  # Default model on server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show required libraries (not all will be installed)\n",
    "\"\"\"\n",
    "Required packages for production voice system:\n",
    "- livekit-agents: WebRTC signaling and rooms\n",
    "- openai-whisper: Speech-to-text\n",
    "- elevenlabs: Text-to-speech\n",
    "- pyaudio: Audio streaming\n",
    "- webrtcvad: Voice activity detection\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, List\n",
    "import json\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture",
   "metadata": {},
   "source": [
    "## 1. Voice-Enabled Invoice Assistant Architecture\n",
    "\n",
    "Let's start by understanding the complete system architecture. This diagram shows how all components work together to create a seamless voice experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architecture_diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = \"\"\"\n",
    "VOICE-ENABLED INVOICE ASSISTANT ARCHITECTURE\n",
    "=============================================\n",
    "\n",
    "[User Microphone] \n",
    "        â†“ (WebRTC)\n",
    "[LiveKit Server] â† Signaling â†’ [Browser/App]\n",
    "        â†“\n",
    "[Audio Stream Buffer]\n",
    "        â†“\n",
    "[Voice Activity Detection]\n",
    "        â†“ (when speech detected)\n",
    "[Whisper STT] â†’ \"Show me invoice INV-001\"\n",
    "        â†“\n",
    "[LangGraph Agent]\n",
    "    â”œâ†’ [Vision Node] â†’ Process invoice image\n",
    "    â”œâ†’ [LLM Node] â†’ Reasoning\n",
    "    â””â†’ [Tool Nodes] â†’ API calls\n",
    "        â†“\n",
    "[Response Generator] â†’ \"The total is $1,250\"\n",
    "        â†“\n",
    "[ElevenLabs TTS]\n",
    "        â†“\n",
    "[Audio Stream]\n",
    "        â†“ (WebRTC)\n",
    "[User Speaker]\n",
    "\n",
    "KEY COMPONENTS:\n",
    "- WebRTC: Real-time audio streaming\n",
    "- VAD: Detects when user is speaking\n",
    "- STT: Converts speech to text\n",
    "- LangGraph: Our existing invoice agent\n",
    "- TTS: Converts response back to speech\n",
    "- Streaming: Low-latency audio delivery\n",
    "\"\"\"\n",
    "\n",
    "print(architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voice_pipeline",
   "metadata": {},
   "source": [
    "## 2. Voice Pipeline Components\n",
    "\n",
    "Now let's examine each component in detail. Understanding these building blocks is crucial for implementing a production voice system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voice_components",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VoiceConfig:\n",
    "    \"\"\"Configuration for voice processing pipeline\"\"\"\n",
    "    sample_rate: int = 16000  # Standard rate for speech processing\n",
    "    chunk_duration_ms: int = 30  # For VAD processing\n",
    "    whisper_model: str = \"base.en\"  # Balance of speed vs accuracy\n",
    "    elevenlabs_voice: str = \"Rachel\"  # Natural sounding voice\n",
    "    silence_threshold_ms: int = 1000  # When to stop listening\n",
    "    max_audio_length_s: int = 30  # Prevent infinite recording\n",
    "\n",
    "class AudioBuffer:\n",
    "    \"\"\"Ring buffer for audio chunks with overflow protection\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        self.total_duration = 0\n",
    "    \n",
    "    def add(self, chunk: bytes, duration_ms: int):\n",
    "        \"\"\"Add audio chunk to buffer with sliding window\"\"\"\n",
    "        if len(self.buffer) >= self.max_size:\n",
    "            # Remove oldest chunk\n",
    "            old_chunk, old_duration = self.buffer.pop(0)\n",
    "            self.total_duration -= old_duration\n",
    "        \n",
    "        self.buffer.append((chunk, duration_ms))\n",
    "        self.total_duration += duration_ms\n",
    "    \n",
    "    def get_audio_data(self) -> bytes:\n",
    "        \"\"\"Concatenate all chunks for processing\"\"\"\n",
    "        return b''.join(chunk for chunk, _ in self.buffer)\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear buffer after processing\"\"\"\n",
    "        self.buffer.clear()\n",
    "        self.total_duration = 0\n",
    "\n",
    "class VoiceActivityDetector:\n",
    "    \"\"\"Detect when user is speaking vs silence\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VoiceConfig):\n",
    "        self.config = config\n",
    "        self.is_speaking = False\n",
    "        self.silence_start = None\n",
    "        # In production: Initialize WebRTC VAD\n",
    "        # import webrtcvad\n",
    "        # self.vad = webrtcvad.Vad(2)  # Aggressiveness level 0-3\n",
    "    \n",
    "    def process_chunk(self, audio_chunk: bytes) -> Dict[str, bool]:\n",
    "        \"\"\"Process audio chunk and detect speech state changes\"\"\"\n",
    "        # In production, use actual VAD:\n",
    "        # is_speech = self.vad.is_speech(audio_chunk, self.config.sample_rate)\n",
    "        \n",
    "        # Mock implementation for demo\n",
    "        import random\n",
    "        is_speech = random.random() > 0.7  # Simulate 30% speech detection\n",
    "        \n",
    "        current_time = time.time()\n",
    "        \n",
    "        # State machine for speech detection\n",
    "        if is_speech:\n",
    "            if not self.is_speaking:\n",
    "                # Speech started\n",
    "                self.is_speaking = True\n",
    "                self.silence_start = None\n",
    "                return {'speech_started': True, 'speech_ended': False}\n",
    "        else:\n",
    "            if self.is_speaking:\n",
    "                # Potential speech end - start silence timer\n",
    "                if self.silence_start is None:\n",
    "                    self.silence_start = current_time\n",
    "                elif current_time - self.silence_start > (self.config.silence_threshold_ms / 1000):\n",
    "                    # Speech ended after sufficient silence\n",
    "                    self.is_speaking = False\n",
    "                    self.silence_start = None\n",
    "                    return {'speech_started': False, 'speech_ended': True}\n",
    "        \n",
    "        return {'speech_started': False, 'speech_ended': False}\n",
    "\n",
    "# Test the components\n",
    "print(\"ðŸ”§ Voice pipeline components initialized\")\n",
    "print(f\"   Config: {VoiceConfig().sample_rate}Hz, {VoiceConfig().whisper_model} model\")\n",
    "print(f\"   Buffer: Max {AudioBuffer().max_size} chunks\")\n",
    "print(f\"   VAD: {VoiceConfig().silence_threshold_ms}ms silence threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whisper_integration",
   "metadata": {},
   "source": [
    "## 3. Whisper Speech-to-Text Integration\n",
    "\n",
    "Whisper is OpenAI's speech recognition model. Understanding how to integrate it effectively is key to good voice experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whisper_stt",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperSTT:\n",
    "    \"\"\"Speech-to-text integration with OpenAI Whisper\"\"\"\n",
    "    \n",
    "    def __init__(self, model_size: str = \"base\"):\n",
    "        \"\"\"\n",
    "        Whisper model sizes and their characteristics:\n",
    "        \n",
    "        Model    | Parameters | Speed  | Accuracy | Use Case\n",
    "        ---------|------------|--------|----------|----------\n",
    "        tiny     | 39M        | ~32x   | Good     | Real-time, mobile\n",
    "        base     | 74M        | ~16x   | Better   | Balanced performance\n",
    "        small    | 244M       | ~6x    | Good     | Server deployment\n",
    "        medium   | 769M       | ~2x    | Very good| High accuracy needs\n",
    "        large    | 1550M      | ~1x    | Excellent| Best possible accuracy\n",
    "        \n",
    "        For production: Consider faster-whisper or WhisperX for better performance\n",
    "        \"\"\"\n",
    "        self.model_size = model_size\n",
    "        self.model = None  # Would load actual model in production\n",
    "        print(f\"ðŸŽ¤ Initialized Whisper STT with '{model_size}' model\")\n",
    "    \n",
    "    async def transcribe_audio_buffer(self, audio_buffer: AudioBuffer) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process complete audio buffer when user stops speaking\n",
    "        \n",
    "        Key challenge: Whisper works best with complete utterances\n",
    "        Streaming approaches exist but add complexity\n",
    "        \"\"\"\n",
    "        if audio_buffer.total_duration < 500:  # Too short\n",
    "            return {'text': '', 'confidence': 0.0, 'language': 'en'}\n",
    "        \n",
    "        # Get concatenated audio data\n",
    "        audio_data = audio_buffer.get_audio_data()\n",
    "        \n",
    "        print(f\"   ðŸ”„ Transcribing {audio_buffer.total_duration}ms of audio...\")\n",
    "        \n",
    "        # In production:\n",
    "        # import whisper\n",
    "        # result = self.model.transcribe(\n",
    "        #     audio_data,\n",
    "        #     language='en',  # Can auto-detect\n",
    "        #     task='transcribe',  # vs 'translate'\n",
    "        #     temperature=0.0  # Deterministic output\n",
    "        # )\n",
    "        # return {\n",
    "        #     'text': result['text'].strip(),\n",
    "        #     'confidence': result.get('avg_logprob', 0.0),\n",
    "        #     'language': result.get('language', 'en'),\n",
    "        #     'segments': result.get('segments', [])\n",
    "        # }\n",
    "        \n",
    "        # Mock transcription for demo\n",
    "        mock_transcriptions = [\n",
    "            \"Show me invoice INV-001\",\n",
    "            \"What's the total amount for the recent TechCorp invoice?\",\n",
    "            \"Extract all line items from this invoice\",\n",
    "            \"Who is the vendor on invoice number ABC-123?\",\n",
    "            \"Calculate the tax amount on this document\"\n",
    "        ]\n",
    "        \n",
    "        import random\n",
    "        transcription = random.choice(mock_transcriptions)\n",
    "        \n",
    "        # Simulate processing time\n",
    "        await asyncio.sleep(0.5)  # Typical Whisper latency\n",
    "        \n",
    "        return {\n",
    "            'text': transcription,\n",
    "            'confidence': 0.95,\n",
    "            'language': 'en',\n",
    "            'duration_ms': audio_buffer.total_duration\n",
    "        }\n",
    "    \n",
    "    def optimize_for_domain(self, invoice_vocabulary: List[str]):\n",
    "        \"\"\"\n",
    "        Optimization strategies for invoice-specific speech:\n",
    "        \n",
    "        1. Custom vocabulary: Common invoice terms\n",
    "        2. Prompt engineering: Guide model context\n",
    "        3. Post-processing: Fix common OCR/STT errors\n",
    "        \"\"\"\n",
    "        self.invoice_terms = invoice_vocabulary\n",
    "        print(f\"   ðŸ“ Loaded {len(invoice_vocabulary)} domain-specific terms\")\n",
    "\n",
    "# Test Whisper integration\n",
    "print(\"ðŸ§ª Testing Whisper STT integration...\")\n",
    "\n",
    "whisper_stt = WhisperSTT(\"base\")\n",
    "\n",
    "# Add domain optimization\n",
    "invoice_vocabulary = [\n",
    "    \"invoice\", \"vendor\", \"total\", \"subtotal\", \"tax\", \"line item\",\n",
    "    \"quantity\", \"unit price\", \"amount\", \"due date\", \"invoice number\"\n",
    "]\n",
    "whisper_stt.optimize_for_domain(invoice_vocabulary)\n",
    "\n",
    "# Simulate audio processing\n",
    "test_buffer = AudioBuffer()\n",
    "test_buffer.add(b\"mock_audio_data\", 2000)  # 2 second recording\n",
    "\n",
    "print(f\"\\n   ðŸ“Š Simulated audio buffer: {test_buffer.total_duration}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "langgraph_bridge",
   "metadata": {},
   "source": [
    "## 4. Bridging Voice to LangGraph Agent\n",
    "\n",
    "This is where the magic happens - connecting voice input to our existing invoice processing agent. The key is maintaining context and handling multimodal inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voice_graph_bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceToGraphBridge:\n",
    "    \"\"\"Bridge between voice pipeline and LangGraph invoice agent\"\"\"\n",
    "    \n",
    "    def __init__(self, langgraph_app, llm_config: Dict[str, str]):\n",
    "        self.graph = langgraph_app\n",
    "        self.llm_config = llm_config\n",
    "        self.conversation_history = []\n",
    "        self.current_invoice_context = None\n",
    "        \n",
    "        print(f\"ðŸ”— Voice-to-Graph bridge initialized\")\n",
    "        print(f\"   LLM: {llm_config.get('url', 'Not configured')}/{llm_config.get('model', 'unknown')}\")\n",
    "    \n",
    "    async def process_voice_command(self, \n",
    "                                   transcription: str, \n",
    "                                   invoice_image: Optional[bytes] = None,\n",
    "                                   user_context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process voice command through LangGraph agent\n",
    "        \n",
    "        This is where we adapt voice input for our existing agent:\n",
    "        1. Clean and enhance transcription\n",
    "        2. Maintain conversation context\n",
    "        3. Route to appropriate graph nodes\n",
    "        4. Format response for voice output\n",
    "        \"\"\"\n",
    "        print(f\"   ðŸŽ¯ Processing voice command: '{transcription}'\")\n",
    "        \n",
    "        # Step 1: Enhance transcription with context\n",
    "        enhanced_query = self._enhance_voice_query(transcription)\n",
    "        \n",
    "        # Step 2: Prepare state for LangGraph\n",
    "        graph_state = {\n",
    "            \"user_query\": enhanced_query,\n",
    "            \"original_transcription\": transcription,\n",
    "            \"modality\": \"voice\",\n",
    "            \"conversation_history\": self.conversation_history[-5:],  # Last 5 exchanges\n",
    "            \"require_voice_response\": True,\n",
    "            \"response_style\": \"conversational\",  # vs \"formal\"\n",
    "            \"max_response_length\": 150  # Keep voice responses concise\n",
    "        }\n",
    "        \n",
    "        # Step 3: Add invoice image if provided\n",
    "        if invoice_image:\n",
    "            graph_state[\"invoice_image\"] = invoice_image\n",
    "            graph_state[\"has_visual_input\"] = True\n",
    "        elif self.current_invoice_context:\n",
    "            # Use previously loaded invoice\n",
    "            graph_state[\"invoice_data\"] = self.current_invoice_context\n",
    "            graph_state[\"has_context\"] = True\n",
    "        \n",
    "        # Step 4: Execute graph (simulated)\n",
    "        print(f\"   âš™ï¸ Executing LangGraph with voice-optimized state...\")\n",
    "        \n",
    "        # In production:\n",
    "        # result = await self.graph.ainvoke(graph_state)\n",
    "        \n",
    "        # Mock graph execution\n",
    "        result = await self._simulate_graph_execution(graph_state)\n",
    "        \n",
    "        # Step 5: Store conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"user_input\": transcription,\n",
    "            \"agent_response\": result[\"final_answer\"],\n",
    "            \"timestamp\": time.time(),\n",
    "            \"invoice_referenced\": result.get(\"invoice_id\")\n",
    "        })\n",
    "        \n",
    "        # Step 6: Optimize response for voice\n",
    "        voice_response = self._optimize_for_voice_output(result[\"final_answer\"])\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"voice_response\": voice_response,\n",
    "            \"detailed_data\": result.get(\"structured_data\"),\n",
    "            \"confidence\": result.get(\"confidence\", 0.9),\n",
    "            \"processing_time_ms\": result.get(\"processing_time\", 800)\n",
    "        }\n",
    "    \n",
    "    def _enhance_voice_query(self, transcription: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and enhance voice transcription for better LLM processing\n",
    "        \n",
    "        Common voice query issues:\n",
    "        - Informal language: \"what's\" â†’ \"what is\"\n",
    "        - Missing context: \"show me the total\" â†’ \"show me the total amount for the current invoice\"\n",
    "        - Ambiguous references: \"that invoice\" â†’ \"invoice INV-001\"\n",
    "        \"\"\"\n",
    "        enhanced = transcription.lower().strip()\n",
    "        \n",
    "        # Basic cleanup\n",
    "        replacements = {\n",
    "            \"what's\": \"what is\",\n",
    "            \"show me\": \"please extract\",\n",
    "            \"tell me\": \"provide information about\",\n",
    "            \"that invoice\": f\"the current invoice {self.current_invoice_context.get('id', '') if self.current_invoice_context else ''}\"\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            enhanced = enhanced.replace(old, new)\n",
    "        \n",
    "        # Add context if missing\n",
    "        if self.current_invoice_context and \"invoice\" not in enhanced:\n",
    "            enhanced += f\" for invoice {self.current_invoice_context['id']}\"\n",
    "        \n",
    "        return enhanced\n",
    "    \n",
    "    async def _simulate_graph_execution(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate LangGraph execution for demo purposes\"\"\"\n",
    "        # Simulate processing time\n",
    "        await asyncio.sleep(0.8)\n",
    "        \n",
    "        query = state[\"user_query\"]\n",
    "        \n",
    "        # Generate appropriate response based on query type\n",
    "        if \"total\" in query or \"amount\" in query:\n",
    "            response = \"The total amount for invoice INV-001 is $1,250.00 including tax.\"\n",
    "            structured_data = {\"total_amount\": 1250.00, \"currency\": \"USD\", \"includes_tax\": True}\n",
    "        elif \"vendor\" in query or \"company\" in query:\n",
    "            response = \"The vendor for this invoice is TechSupplies Corporation.\"\n",
    "            structured_data = {\"vendor_name\": \"TechSupplies Corporation\"}\n",
    "        elif \"line item\" in query or \"items\" in query:\n",
    "            response = \"This invoice contains 3 line items: Laptop computers, software licenses, and consulting services.\"\n",
    "            structured_data = {\"line_items_count\": 3, \"categories\": [\"hardware\", \"software\", \"services\"]}\n",
    "        else:\n",
    "            response = \"I can help you analyze invoice data. Try asking about totals, vendors, or line items.\"\n",
    "            structured_data = {\"suggestion\": \"ask_specific_question\"}\n",
    "        \n",
    "        return {\n",
    "            \"final_answer\": response,\n",
    "            \"structured_data\": structured_data,\n",
    "            \"confidence\": 0.92,\n",
    "            \"processing_time\": 800,\n",
    "            \"invoice_id\": \"INV-001\"\n",
    "        }\n",
    "    \n",
    "    def _optimize_for_voice_output(self, text_response: str) -> str:\n",
    "        \"\"\"\n",
    "        Optimize text response for voice output\n",
    "        \n",
    "        Voice-specific considerations:\n",
    "        - Break up long numbers: \"1250.00\" â†’ \"one thousand two hundred fifty dollars\"\n",
    "        - Add pauses: \"The total is... one thousand dollars\"\n",
    "        - Remove visual formatting: No bullet points, tables\n",
    "        - Simplify complex sentences\n",
    "        \"\"\"\n",
    "        # Remove common text artifacts\n",
    "        cleaned = text_response.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "        \n",
    "        # Add natural pauses\n",
    "        if \"$\" in cleaned:\n",
    "            # Add pause before dollar amounts\n",
    "            cleaned = cleaned.replace(\"is $\", \"is... \")\n",
    "        \n",
    "        # Limit length for voice\n",
    "        if len(cleaned) > 150:\n",
    "            sentences = cleaned.split(\". \")\n",
    "            cleaned = sentences[0] + \".\"\n",
    "        \n",
    "        return cleaned\n",
    "\n",
    "# Test the bridge\n",
    "print(\"ðŸ§ª Testing Voice-to-Graph bridge...\")\n",
    "\n",
    "# Mock LangGraph app\n",
    "mock_graph = None  # Would be actual LangGraph application\n",
    "\n",
    "bridge = VoiceToGraphBridge(\n",
    "    langgraph_app=mock_graph,\n",
    "    llm_config={\"url\": OLLAMA_URL, \"model\": MODEL, \"token\": API_TOKEN}\n",
    ")\n",
    "\n",
    "# Set current invoice context\n",
    "bridge.current_invoice_context = {\"id\": \"INV-001\", \"vendor\": \"TechSupplies Corp\"}\n",
    "\n",
    "print(f\"   ðŸ“‹ Invoice context set: {bridge.current_invoice_context['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tts_integration",
   "metadata": {},
   "source": [
    "## 5. Text-to-Speech with ElevenLabs\n",
    "\n",
    "Converting our agent's responses back to natural-sounding speech is the final piece of the voice experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elevenlabs_tts",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElevenLabsTTS:\n",
    "    \"\"\"Text-to-speech integration with ElevenLabs API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, voice_id: str = \"Rachel\"):\n",
    "        \"\"\"\n",
    "        ElevenLabs voice model options:\n",
    "        \n",
    "        Model         | Latency | Quality | Use Case\n",
    "        --------------|---------|---------|----------\n",
    "        Turbo v2.5    | 32ms    | Good    | Real-time conversation\n",
    "        Multilingual  | 200ms   | Excellent| Multi-language support\n",
    "        Voice Cloning | 500ms   | Custom  | Branded voice experience\n",
    "        \n",
    "        Voice characteristics:\n",
    "        - Rachel: Professional, clear\n",
    "        - Adam: Friendly, conversational\n",
    "        - Bella: Warm, approachable\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.voice_id = voice_id\n",
    "        self.model = \"eleven_turbo_v2_5\"  # Optimized for low latency\n",
    "        print(f\"ðŸ”Š Initialized ElevenLabs TTS with '{voice_id}' voice\")\n",
    "    \n",
    "    async def synthesize_streaming(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate speech with streaming for low latency\n",
    "        \n",
    "        Streaming approach:\n",
    "        1. Send text to ElevenLabs API\n",
    "        2. Receive audio chunks as they're generated\n",
    "        3. Start playback immediately (don't wait for completion)\n",
    "        4. Handle user interruptions gracefully\n",
    "        \"\"\"\n",
    "        print(f\"   ðŸŽµ Synthesizing: '{text[:50]}{'...' if len(text) > 50 else ''}'\")\n",
    "        \n",
    "        # In production:\n",
    "        # import httpx\n",
    "        # \n",
    "        # url = f\"https://api.elevenlabs.io/v1/text-to-speech/{self.voice_id}/stream\"\n",
    "        # headers = {\n",
    "        #     \"Accept\": \"audio/mpeg\",\n",
    "        #     \"Content-Type\": \"application/json\",\n",
    "        #     \"xi-api-key\": self.api_key\n",
    "        # }\n",
    "        # \n",
    "        # data = {\n",
    "        #     \"text\": text,\n",
    "        #     \"model_id\": self.model,\n",
    "        #     \"voice_settings\": {\n",
    "        #         \"stability\": 0.5,      # Voice consistency\n",
    "        #         \"similarity_boost\": 0.75,  # Voice similarity\n",
    "        #         \"style\": 0.5,         # Expressiveness\n",
    "        #         \"use_speaker_boost\": True\n",
    "        #     }\n",
    "        # }\n",
    "        # \n",
    "        # async with httpx.AsyncClient() as client:\n",
    "        #     async with client.stream('POST', url, headers=headers, json=data) as response:\n",
    "        #         audio_chunks = []\n",
    "        #         async for chunk in response.aiter_bytes():\n",
    "        #             audio_chunks.append(chunk)\n",
    "        #             yield chunk  # Stream immediately\n",
    "        # \n",
    "        # return b''.join(audio_chunks)\n",
    "        \n",
    "        # Mock implementation for demo\n",
    "        estimated_duration = len(text) * 0.05  # ~50ms per character\n",
    "        await asyncio.sleep(0.1)  # Simulate API latency\n",
    "        \n",
    "        return {\n",
    "            \"audio_data\": b\"mock_audio_mp3_data\",\n",
    "            \"duration_seconds\": estimated_duration,\n",
    "            \"sample_rate\": 22050,\n",
    "            \"format\": \"mp3\",\n",
    "            \"voice_used\": self.voice_id,\n",
    "            \"model_used\": self.model\n",
    "        }\n",
    "    \n",
    "    def optimize_for_conversation(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Optimize text for conversational speech synthesis\n",
    "        \n",
    "        Techniques:\n",
    "        1. Add SSML for natural pauses\n",
    "        2. Phonetic spelling for technical terms\n",
    "        3. Emphasize important information\n",
    "        \"\"\"\n",
    "        # Add natural pauses\n",
    "        optimized = text.replace(\", \", \", <break time='0.3s'/> \")\n",
    "        optimized = optimized.replace(\". \", \". <break time='0.5s'/> \")\n",
    "        \n",
    "        # Emphasize monetary amounts\n",
    "        import re\n",
    "        optimized = re.sub(r'\\$([0-9,]+\\.?[0-9]*)', r'<emphasis level=\"strong\">$\\1</emphasis>', optimized)\n",
    "        \n",
    "        # Slow down invoice numbers for clarity\n",
    "        optimized = re.sub(r'(INV-[A-Z0-9]+)', r'<prosody rate=\"slow\">\\1</prosody>', optimized)\n",
    "        \n",
    "        return optimized\n",
    "    \n",
    "    async def handle_interruption(self):\n",
    "        \"\"\"\n",
    "        Handle user interruptions (barge-in)\n",
    "        \n",
    "        When user starts speaking while agent is talking:\n",
    "        1. Stop current TTS immediately\n",
    "        2. Clear audio buffer\n",
    "        3. Switch to listening mode\n",
    "        \"\"\"\n",
    "        print(\"   â¹ï¸ User interruption detected - stopping TTS\")\n",
    "        # In production: Stop streaming audio\n",
    "        # await self.stop_current_synthesis()\n",
    "\n",
    "# Test TTS integration\n",
    "print(\"ðŸ§ª Testing ElevenLabs TTS integration...\")\n",
    "\n",
    "# Mock API key for demo\n",
    "tts = ElevenLabsTTS(api_key=\"demo_key\", voice_id=\"Rachel\")\n",
    "\n",
    "test_response = \"The total amount for invoice INV-001 is $1,250.00 including tax.\"\n",
    "optimized_text = tts.optimize_for_conversation(test_response)\n",
    "\n",
    "print(f\"\\n   ðŸ“ Original: {test_response}\")\n",
    "print(f\"   ðŸŽ­ Optimized: {optimized_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conversation_management",
   "metadata": {},
   "source": [
    "## 6. Conversation Flow and Turn-Taking\n",
    "\n",
    "Managing natural conversation flow is one of the most challenging aspects of voice interfaces. This includes handling interruptions, managing silence, and ensuring smooth turn-taking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conversation_manager",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationManager:\n",
    "    \"\"\"Manage conversation state and turn-taking in voice interactions\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VoiceConfig):\n",
    "        self.config = config\n",
    "        self.user_speaking = False\n",
    "        self.agent_speaking = False\n",
    "        self.last_speech_time = None\n",
    "        self.conversation_state = \"listening\"  # listening, processing, speaking, waiting\n",
    "        self.interruption_count = 0\n",
    "        \n",
    "        print(f\"ðŸ’¬ Conversation manager initialized\")\n",
    "        print(f\"   Silence threshold: {config.silence_threshold_ms}ms\")\n",
    "    \n",
    "    def handle_user_speech_start(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        User starts talking - manage interruptions and state transitions\n",
    "        \n",
    "        Scenarios:\n",
    "        1. User speaks while agent is quiet â†’ Normal turn\n",
    "        2. User interrupts agent speaking â†’ Barge-in behavior\n",
    "        3. User continues after pause â†’ Extended input\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if self.agent_speaking:\n",
    "            # User is interrupting - handle barge-in\n",
    "            print(\"   ðŸ”„ User interruption detected (barge-in)\")\n",
    "            self.interruption_count += 1\n",
    "            \n",
    "            # Stop agent speech immediately\n",
    "            action = self._stop_agent_speech()\n",
    "            \n",
    "            # Switch to listening mode\n",
    "            self.conversation_state = \"listening\"\n",
    "            self.agent_speaking = False\n",
    "            \n",
    "            return {\n",
    "                \"action\": \"handle_interruption\",\n",
    "                \"previous_state\": \"agent_speaking\",\n",
    "                \"stop_tts\": True,\n",
    "                \"interruption_count\": self.interruption_count\n",
    "            }\n",
    "        \n",
    "        elif self.conversation_state == \"waiting\":\n",
    "            # Normal user turn\n",
    "            print(\"   ðŸŽ¤ User started speaking\")\n",
    "            self.conversation_state = \"listening\"\n",
    "            \n",
    "            return {\n",
    "                \"action\": \"start_listening\",\n",
    "                \"previous_state\": \"waiting\",\n",
    "                \"start_recording\": True\n",
    "            }\n",
    "        \n",
    "        # Update state\n",
    "        self.user_speaking = True\n",
    "        self.last_speech_time = current_time\n",
    "        \n",
    "        return {\"action\": \"continue_listening\"}\n",
    "    \n",
    "    def handle_silence(self, duration_ms: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Handle silence periods - decide when to process speech\n",
    "        \n",
    "        Silence handling strategy:\n",
    "        - Short pause: Continue waiting\n",
    "        - Medium pause: End of utterance, process speech\n",
    "        - Long pause: Timeout, prompt user\n",
    "        \"\"\"\n",
    "        if not self.user_speaking:\n",
    "            return {\"action\": \"no_change\"}\n",
    "        \n",
    "        if duration_ms >= self.config.silence_threshold_ms:\n",
    "            # End of user utterance\n",
    "            print(f\"   â¸ï¸ Silence detected ({duration_ms}ms) - processing speech\")\n",
    "            \n",
    "            self.user_speaking = False\n",
    "            self.conversation_state = \"processing\"\n",
    "            \n",
    "            return {\n",
    "                \"action\": \"process_speech\",\n",
    "                \"silence_duration\": duration_ms,\n",
    "                \"should_transcribe\": True\n",
    "            }\n",
    "        \n",
    "        return {\"action\": \"continue_listening\"}\n",
    "    \n",
    "    def handle_agent_response_start(self, estimated_duration_s: float) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Agent starts speaking - update state and prepare for potential interruptions\n",
    "        \"\"\"\n",
    "        print(f\"   ðŸ¤– Agent starting to speak ({estimated_duration_s:.1f}s)\")\n",
    "        \n",
    "        self.agent_speaking = True\n",
    "        self.conversation_state = \"speaking\"\n",
    "        \n",
    "        return {\n",
    "            \"action\": \"start_speaking\",\n",
    "            \"estimated_duration\": estimated_duration_s,\n",
    "            \"monitor_for_interruption\": True\n",
    "        }\n",
    "    \n",
    "    def handle_agent_response_complete(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Agent finished speaking - return to listening state\n",
    "        \"\"\"\n",
    "        print(\"   âœ… Agent finished speaking - waiting for user\")\n",
    "        \n",
    "        self.agent_speaking = False\n",
    "        self.conversation_state = \"waiting\"\n",
    "        \n",
    "        return {\n",
    "            \"action\": \"wait_for_user\",\n",
    "            \"ready_for_input\": True\n",
    "        }\n",
    "    \n",
    "    def _stop_agent_speech(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Immediately stop agent speech due to interruption\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"stop_tts_immediately\": True,\n",
    "            \"clear_audio_buffer\": True,\n",
    "            \"interruption_handled\": True\n",
    "        }\n",
    "    \n",
    "    def get_conversation_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get conversation flow statistics for optimization\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"current_state\": self.conversation_state,\n",
    "            \"user_speaking\": self.user_speaking,\n",
    "            \"agent_speaking\": self.agent_speaking,\n",
    "            \"interruption_count\": self.interruption_count,\n",
    "            \"last_speech_time\": self.last_speech_time\n",
    "        }\n",
    "\n",
    "# Test conversation management\n",
    "print(\"ðŸ§ª Testing conversation flow management...\")\n",
    "\n",
    "config = VoiceConfig()\n",
    "conversation = ConversationManager(config)\n",
    "\n",
    "# Simulate conversation flow\n",
    "print(\"\\n   ðŸ“Š Simulating conversation scenarios:\")\n",
    "\n",
    "# Scenario 1: Normal user turn\n",
    "result1 = conversation.handle_user_speech_start()\n",
    "print(f\"   1. User starts speaking: {result1['action']}\")\n",
    "\n",
    "# Scenario 2: User finishes speaking\n",
    "result2 = conversation.handle_silence(1200)  # 1.2 seconds\n",
    "print(f\"   2. Silence detected: {result2['action']}\")\n",
    "\n",
    "# Scenario 3: Agent responds\n",
    "result3 = conversation.handle_agent_response_start(3.5)\n",
    "print(f\"   3. Agent speaking: {result3['action']}\")\n",
    "\n",
    "# Scenario 4: User interrupts\n",
    "result4 = conversation.handle_user_speech_start()\n",
    "print(f\"   4. User interrupts: {result4['action']}\")\n",
    "\n",
    "# Show stats\n",
    "stats = conversation.get_conversation_stats()\n",
    "print(f\"\\n   ðŸ“ˆ Conversation stats: State={stats['current_state']}, Interruptions={stats['interruption_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latency_optimization",
   "metadata": {},
   "source": [
    "## 7. Latency Optimization Strategies\n",
    "\n",
    "Voice interfaces are extremely sensitive to latency. Let's examine where delays occur and how to minimize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latency_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatencyOptimizer:\n",
    "    \"\"\"Analyze and optimize voice pipeline latency\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.latency_breakdown = {\n",
    "            \"vad_processing\": 30,      # Voice Activity Detection\n",
    "            \"stt_whisper_base\": 2000,  # Whisper speech-to-text\n",
    "            \"langgraph_processing\": 1500,  # Invoice agent\n",
    "            \"tts_elevenlabs\": 500,     # Text-to-speech\n",
    "            \"network_roundtrip\": 100,  # Network delays\n",
    "            \"audio_buffering\": 50      # Audio pipeline delays\n",
    "        }\n",
    "        \n",
    "        self.optimizations = {\n",
    "            \"vad_processing\": 30,      # Already optimized\n",
    "            \"stt_whisper_base\": 500,   # faster-whisper or WhisperX\n",
    "            \"langgraph_processing\": 800,   # Caching + prompt optimization\n",
    "            \"tts_elevenlabs\": 75,      # Streaming TTS\n",
    "            \"network_roundtrip\": 50,   # Edge deployment\n",
    "            \"audio_buffering\": 30      # Optimized audio pipeline\n",
    "        }\n",
    "    \n",
    "    def analyze_current_latency(self) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate current end-to-end latency\"\"\"\n",
    "        total_baseline = sum(self.latency_breakdown.values())\n",
    "        total_optimized = sum(self.optimizations.values())\n",
    "        improvement = total_baseline - total_optimized\n",
    "        \n",
    "        return {\n",
    "            \"baseline_ms\": total_baseline,\n",
    "            \"optimized_ms\": total_optimized,\n",
    "            \"improvement_ms\": improvement,\n",
    "            \"improvement_percent\": (improvement / total_baseline) * 100,\n",
    "            \"breakdown\": self.latency_breakdown,\n",
    "            \"optimized_breakdown\": self.optimizations\n",
    "        }\n",
    "    \n",
    "    def get_optimization_recommendations(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get specific recommendations for latency reduction\"\"\"\n",
    "        recommendations = [\n",
    "            {\n",
    "                \"component\": \"Speech-to-Text\",\n",
    "                \"current_ms\": 2000,\n",
    "                \"optimized_ms\": 500,\n",
    "                \"improvement\": \"75%\",\n",
    "                \"method\": \"faster-whisper or WhisperX\",\n",
    "                \"description\": \"Replace standard Whisper with optimized implementations\",\n",
    "                \"difficulty\": \"Medium\",\n",
    "                \"impact\": \"High\"\n",
    "            },\n",
    "            {\n",
    "                \"component\": \"LangGraph Processing\",\n",
    "                \"current_ms\": 1500,\n",
    "                \"optimized_ms\": 800,\n",
    "                \"improvement\": \"47%\",\n",
    "                \"method\": \"Prompt caching + model optimization\",\n",
    "                \"description\": \"Cache frequent queries and optimize prompt templates\",\n",
    "                \"difficulty\": \"Low\",\n",
    "                \"impact\": \"High\"\n",
    "            },\n",
    "            {\n",
    "                \"component\": \"Text-to-Speech\",\n",
    "                \"current_ms\": 500,\n",
    "                \"optimized_ms\": 75,\n",
    "                \"improvement\": \"85%\",\n",
    "                \"method\": \"Streaming TTS with ElevenLabs Turbo\",\n",
    "                \"description\": \"Start audio playback while generating rest of speech\",\n",
    "                \"difficulty\": \"Medium\",\n",
    "                \"impact\": \"High\"\n",
    "            },\n",
    "            {\n",
    "                \"component\": \"Network\",\n",
    "                \"current_ms\": 100,\n",
    "                \"optimized_ms\": 50,\n",
    "                \"improvement\": \"50%\",\n",
    "                \"method\": \"Edge deployment\",\n",
    "                \"description\": \"Deploy closer to users with CDN or edge compute\",\n",
    "                \"difficulty\": \"High\",\n",
    "                \"impact\": \"Medium\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return sorted(recommendations, key=lambda x: x[\"current_ms\"], reverse=True)\n",
    "    \n",
    "    def simulate_optimization_impact(self, optimizations_applied: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate the impact of applying specific optimizations\"\"\"\n",
    "        current_total = sum(self.latency_breakdown.values())\n",
    "        optimized_total = current_total\n",
    "        \n",
    "        # Apply selected optimizations\n",
    "        for opt in optimizations_applied:\n",
    "            if opt in self.latency_breakdown:\n",
    "                current_latency = self.latency_breakdown[opt]\n",
    "                optimized_latency = self.optimizations[opt]\n",
    "                reduction = current_latency - optimized_latency\n",
    "                optimized_total -= reduction\n",
    "        \n",
    "        improvement = current_total - optimized_total\n",
    "        \n",
    "        # Classify result\n",
    "        if optimized_total < 1000:\n",
    "            quality = \"Excellent (<1s)\"\n",
    "        elif optimized_total < 2000:\n",
    "            quality = \"Good (<2s)\"\n",
    "        elif optimized_total < 3000:\n",
    "            quality = \"Acceptable (<3s)\"\n",
    "        else:\n",
    "            quality = \"Poor (>3s)\"\n",
    "        \n",
    "        return {\n",
    "            \"current_ms\": current_total,\n",
    "            \"optimized_ms\": optimized_total,\n",
    "            \"improvement_ms\": improvement,\n",
    "            \"improvement_percent\": (improvement / current_total) * 100,\n",
    "            \"user_experience\": quality,\n",
    "            \"optimizations_applied\": optimizations_applied\n",
    "        }\n",
    "\n",
    "# Analyze current latency\n",
    "print(\"âš¡ Voice Pipeline Latency Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "optimizer = LatencyOptimizer()\n",
    "analysis = optimizer.analyze_current_latency()\n",
    "\n",
    "print(f\"\\nðŸ“Š LATENCY BREAKDOWN:\")\n",
    "print(f\"{'Component':<25} | {'Current':<8} | {'Optimized':<10} | {'Improvement':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for component, current_ms in analysis['breakdown'].items():\n",
    "    optimized_ms = analysis['optimized_breakdown'][component]\n",
    "    improvement = current_ms - optimized_ms\n",
    "    improvement_pct = (improvement / current_ms) * 100 if current_ms > 0 else 0\n",
    "    \n",
    "    print(f\"{component.replace('_', ' ').title():<25} | {current_ms:<8}ms | {optimized_ms:<10}ms | {improvement_pct:<12.1f}%\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'TOTAL':<25} | {analysis['baseline_ms']:<8}ms | {analysis['optimized_ms']:<10}ms | {analysis['improvement_percent']:<12.1f}%\")\n",
    "\n",
    "# Show target performance\n",
    "print(f\"\\nðŸŽ¯ PERFORMANCE TARGETS:\")\n",
    "print(f\"   Current: {analysis['baseline_ms']}ms ({analysis['baseline_ms']/1000:.1f}s)\")\n",
    "print(f\"   Optimized: {analysis['optimized_ms']}ms ({analysis['optimized_ms']/1000:.1f}s)\")\n",
    "print(f\"   Target: <2000ms (<2s) for good user experience\")\n",
    "\n",
    "if analysis['optimized_ms'] < 2000:\n",
    "    print(f\"   âœ… Target achieved with optimizations!\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ Additional optimizations needed\")\n",
    "\n",
    "# Show top recommendations\n",
    "recommendations = optimizer.get_optimization_recommendations()\n",
    "print(f\"\\nðŸš€ TOP OPTIMIZATION OPPORTUNITIES:\")\n",
    "\n",
    "for i, rec in enumerate(recommendations[:3], 1):\n",
    "    print(f\"\\n   {i}. {rec['component']} ({rec['improvement']} improvement)\")\n",
    "    print(f\"      Method: {rec['method']}\")\n",
    "    print(f\"      Impact: {rec['impact']}, Difficulty: {rec['difficulty']}\")\n",
    "\n",
    "# Simulate applying all optimizations\n",
    "all_components = list(optimizer.latency_breakdown.keys())\n",
    "simulation = optimizer.simulate_optimization_impact(all_components)\n",
    "\n",
    "print(f\"\\nðŸŽ‰ WITH ALL OPTIMIZATIONS:\")\n",
    "print(f\"   End-to-end latency: {simulation['optimized_ms']}ms ({simulation['optimized_ms']/1000:.1f}s)\")\n",
    "print(f\"   Improvement: {simulation['improvement_percent']:.1f}%\")\n",
    "print(f\"   User experience: {simulation['user_experience']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete_pipeline",
   "metadata": {},
   "source": [
    "## 8. Complete Voice Pipeline Demonstration\n",
    "\n",
    "Let's put it all together and simulate the complete voice-enabled invoice processing flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def voice_invoice_pipeline_demo():\n",
    "    \"\"\"\n",
    "    Complete demonstration of voice-enabled invoice processing\n",
    "    \n",
    "    This simulates the full pipeline from user speech to agent response\n",
    "    showing all the integration points and timing.\n",
    "    \"\"\"\n",
    "    print(\"ðŸŽ¬ COMPLETE VOICE PIPELINE DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize all components\n",
    "    config = VoiceConfig()\n",
    "    audio_buffer = AudioBuffer()\n",
    "    vad = VoiceActivityDetector(config)\n",
    "    whisper = WhisperSTT(\"base\")\n",
    "    bridge = VoiceToGraphBridge(None, {\"url\": OLLAMA_URL, \"model\": MODEL})\n",
    "    tts = ElevenLabsTTS(\"demo_key\", \"Rachel\")\n",
    "    conversation = ConversationManager(config)\n",
    "    \n",
    "    print(f\"\\nðŸ”§ All components initialized\")\n",
    "    \n",
    "    # Set invoice context\n",
    "    bridge.current_invoice_context = {\n",
    "        \"id\": \"INV-001\",\n",
    "        \"vendor\": \"TechSupplies Corp\",\n",
    "        \"total\": 1250.00\n",
    "    }\n",
    "    \n",
    "    print(f\"ðŸ“‹ Invoice context: {bridge.current_invoice_context['id']}\")\n",
    "    \n",
    "    # Simulate complete interaction\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ðŸŽ­ SIMULATING VOICE INTERACTION\")\n",
    "    print(f\"=\" * 60)\n",
    "    \n",
    "    # Step 1: User starts speaking\n",
    "    print(f\"\\n1ï¸âƒ£ USER STARTS SPEAKING\")\n",
    "    conversation_action = conversation.handle_user_speech_start()\n",
    "    print(f\"   Action: {conversation_action['action']}\")\n",
    "    \n",
    "    # Step 2: Simulate audio capture\n",
    "    print(f\"\\n2ï¸âƒ£ AUDIO CAPTURE & VAD\")\n",
    "    for i in range(5):  # Simulate 5 audio chunks\n",
    "        mock_audio_chunk = f\"audio_chunk_{i}\".encode()\n",
    "        vad_result = vad.process_chunk(mock_audio_chunk)\n",
    "        \n",
    "        if vad_result['speech_started']:\n",
    "            print(f\"   ðŸŽ¤ Speech detected in chunk {i}\")\n",
    "        \n",
    "        audio_buffer.add(mock_audio_chunk, 100)  # 100ms chunks\n",
    "        await asyncio.sleep(0.05)  # Real-time simulation\n",
    "    \n",
    "    print(f\"   ðŸ“Š Captured {audio_buffer.total_duration}ms of audio\")\n",
    "    \n",
    "    # Step 3: Detect end of speech\n",
    "    print(f\"\\n3ï¸âƒ£ SILENCE DETECTION\")\n",
    "    silence_action = conversation.handle_silence(1200)\n",
    "    print(f\"   Action: {silence_action['action']}\")\n",
    "    \n",
    "    if silence_action['action'] == 'process_speech':\n",
    "        # Step 4: Speech-to-text\n",
    "        print(f\"\\n4ï¸âƒ£ SPEECH-TO-TEXT (WHISPER)\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        transcription_result = await whisper.transcribe_audio_buffer(audio_buffer)\n",
    "        stt_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"   ðŸ“ Transcription: '{transcription_result['text']}'\")\n",
    "        print(f\"   ðŸŽ¯ Confidence: {transcription_result['confidence']:.2f}\")\n",
    "        print(f\"   â±ï¸ Processing time: {stt_time:.2f}s\")\n",
    "        \n",
    "        # Step 5: LangGraph processing\n",
    "        print(f\"\\n5ï¸âƒ£ LANGGRAPH AGENT PROCESSING\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        agent_result = await bridge.process_voice_command(\n",
    "            transcription_result['text']\n",
    "        )\n",
    "        \n",
    "        agent_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"   ðŸ¤– Agent response: '{agent_result['voice_response']}'\")\n",
    "        print(f\"   ðŸŽ¯ Confidence: {agent_result['confidence']:.2f}\")\n",
    "        print(f\"   â±ï¸ Processing time: {agent_time:.2f}s\")\n",
    "        \n",
    "        # Step 6: Text-to-speech\n",
    "        print(f\"\\n6ï¸âƒ£ TEXT-TO-SPEECH (ELEVENLABS)\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Optimize text for speech\n",
    "        optimized_text = tts.optimize_for_conversation(agent_result['voice_response'])\n",
    "        \n",
    "        tts_result = await tts.synthesize_streaming(optimized_text)\n",
    "        tts_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"   ðŸ”Š Audio generated: {tts_result['duration_seconds']:.1f}s duration\")\n",
    "        print(f\"   ðŸŽµ Voice: {tts_result['voice_used']}\")\n",
    "        print(f\"   â±ï¸ Generation time: {tts_time:.2f}s\")\n",
    "        \n",
    "        # Step 7: Play response\n",
    "        print(f\"\\n7ï¸âƒ£ AUDIO PLAYBACK\")\n",
    "        conversation.handle_agent_response_start(tts_result['duration_seconds'])\n",
    "        \n",
    "        # Simulate playback\n",
    "        print(f\"   ðŸ”Š Playing audio response...\")\n",
    "        await asyncio.sleep(tts_result['duration_seconds'])  # Simulate playback time\n",
    "        \n",
    "        conversation.handle_agent_response_complete()\n",
    "        print(f\"   âœ… Playback complete - ready for next user input\")\n",
    "        \n",
    "        # Step 8: Calculate total latency\n",
    "        print(f\"\\n8ï¸âƒ£ PERFORMANCE SUMMARY\")\n",
    "        total_processing_time = stt_time + agent_time + tts_time\n",
    "        \n",
    "        print(f\"   ðŸ“Š Latency breakdown:\")\n",
    "        print(f\"      STT (Whisper): {stt_time:.2f}s\")\n",
    "        print(f\"      Agent (LangGraph): {agent_time:.2f}s\")\n",
    "        print(f\"      TTS (ElevenLabs): {tts_time:.2f}s\")\n",
    "        print(f\"      Total processing: {total_processing_time:.2f}s\")\n",
    "        \n",
    "        # User experience assessment\n",
    "        if total_processing_time < 2.0:\n",
    "            experience = \"Excellent - feels conversational\"\n",
    "        elif total_processing_time < 3.0:\n",
    "            experience = \"Good - acceptable for most users\"\n",
    "        elif total_processing_time < 5.0:\n",
    "            experience = \"Fair - users may notice delay\"\n",
    "        else:\n",
    "            experience = \"Poor - feels unnatural\"\n",
    "        \n",
    "        print(f\"      User experience: {experience}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ðŸŽ‰ VOICE PIPELINE DEMONSTRATION COMPLETE\")\n",
    "    print(f\"=\" * 60)\n",
    "\n",
    "# Run the complete demonstration\n",
    "await voice_invoice_pipeline_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "production_considerations",
   "metadata": {},
   "source": [
    "## 9. Production Deployment Considerations\n",
    "\n",
    "Building a demo is one thing - deploying a production voice system requires additional considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "production_checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "production_considerations = \"\"\"\n",
    "ðŸ­ PRODUCTION DEPLOYMENT CHECKLIST\n",
    "=====================================\n",
    "\n",
    "ðŸ“¡ INFRASTRUCTURE:\n",
    "   âœ“ WebRTC media server (LiveKit/Agora/Twilio)\n",
    "   âœ“ Load balancers for voice processing servers\n",
    "   âœ“ CDN for audio content delivery\n",
    "   âœ“ Auto-scaling based on concurrent sessions\n",
    "   âœ“ Geographic distribution for latency\n",
    "\n",
    "ðŸ”’ SECURITY & PRIVACY:\n",
    "   âœ“ End-to-end encryption for audio streams\n",
    "   âœ“ Audio data retention policies\n",
    "   âœ“ GDPR/CCPA compliance for voice data\n",
    "   âœ“ User consent for voice recording\n",
    "   âœ“ Secure API authentication\n",
    "\n",
    "âš¡ PERFORMANCE OPTIMIZATION:\n",
    "   âœ“ GPU acceleration for Whisper\n",
    "   âœ“ Model quantization for faster inference\n",
    "   âœ“ Response caching for common queries\n",
    "   âœ“ Streaming optimizations\n",
    "   âœ“ Circuit breakers for service failures\n",
    "\n",
    "ðŸ“Š MONITORING & ANALYTICS:\n",
    "   âœ“ Real-time latency monitoring\n",
    "   âœ“ Speech recognition accuracy tracking\n",
    "   âœ“ User satisfaction metrics\n",
    "   âœ“ Error rate dashboards\n",
    "   âœ“ Resource utilization alerts\n",
    "\n",
    "ðŸ”§ RELIABILITY:\n",
    "   âœ“ Graceful degradation (fallback to text)\n",
    "   âœ“ Service health checks\n",
    "   âœ“ Automated failover\n",
    "   âœ“ Data backup and recovery\n",
    "   âœ“ Disaster recovery procedures\n",
    "\n",
    "ðŸ‘¥ USER EXPERIENCE:\n",
    "   âœ“ Onboarding and voice training\n",
    "   âœ“ Accessibility features\n",
    "   âœ“ Multi-language support\n",
    "   âœ“ Background noise handling\n",
    "   âœ“ User feedback collection\n",
    "\n",
    "ðŸ’° COST OPTIMIZATION:\n",
    "   âœ“ Usage-based pricing models\n",
    "   âœ“ Resource scheduling (scale down off-hours)\n",
    "   âœ“ Model efficiency optimizations\n",
    "   âœ“ Bandwidth optimization\n",
    "   âœ“ Cost monitoring and alerts\n",
    "\n",
    "ðŸ§ª TESTING STRATEGY:\n",
    "   âœ“ Automated voice quality testing\n",
    "   âœ“ Load testing with realistic audio\n",
    "   âœ“ Accent and language variety testing\n",
    "   âœ“ Noise robustness testing\n",
    "   âœ“ Integration testing across components\n",
    "\"\"\"\n",
    "\n",
    "print(production_considerations)\n",
    "\n",
    "# Architecture alternatives\n",
    "architecture_options = {\n",
    "    \"STT Options\": {\n",
    "        \"OpenAI Whisper\": {\"pros\": \"Open source, high accuracy\", \"cons\": \"Higher latency\"},\n",
    "        \"Deepgram API\": {\"pros\": \"Low latency, streaming\", \"cons\": \"Cost per minute\"},\n",
    "        \"Google Speech\": {\"pros\": \"Robust, multi-language\", \"cons\": \"Vendor lock-in\"},\n",
    "        \"Azure Speech\": {\"pros\": \"Enterprise features\", \"cons\": \"Complex pricing\"}\n",
    "    },\n",
    "    \"TTS Options\": {\n",
    "        \"ElevenLabs\": {\"pros\": \"Natural voices, streaming\", \"cons\": \"Cost per character\"},\n",
    "        \"Cartesia\": {\"pros\": \"Ultra-low latency\", \"cons\": \"Limited voices\"},\n",
    "        \"Azure Neural\": {\"pros\": \"Enterprise grade\", \"cons\": \"Setup complexity\"},\n",
    "        \"AWS Polly\": {\"pros\": \"AWS integration\", \"cons\": \"Voice quality\"}\n",
    "    },\n",
    "    \"WebRTC Platforms\": {\n",
    "        \"LiveKit\": {\"pros\": \"Open source, feature-rich\", \"cons\": \"Self-hosting required\"},\n",
    "        \"Agora\": {\"pros\": \"Global infrastructure\", \"cons\": \"Higher costs\"},\n",
    "        \"Twilio\": {\"pros\": \"Simple integration\", \"cons\": \"Limited customization\"},\n",
    "        \"Daily.co\": {\"pros\": \"Developer-friendly\", \"cons\": \"Smaller ecosystem\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸ”§ ARCHITECTURE COMPONENT OPTIONS:\")\n",
    "print(f\"=\" * 50)\n",
    "\n",
    "for category, options in architecture_options.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for option, details in options.items():\n",
    "        print(f\"   â€¢ {option}\")\n",
    "        print(f\"     Pros: {details['pros']}\")\n",
    "        print(f\"     Cons: {details['cons']}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ RECOMMENDATION FOR PRODUCTION:\")\n",
    "print(f\"   STT: Deepgram API (for low latency) + Whisper (for high accuracy)\")\n",
    "print(f\"   TTS: ElevenLabs Turbo (for quality) + fallback to faster options\")\n",
    "print(f\"   WebRTC: LiveKit (for control) or Daily.co (for simplicity)\")\n",
    "print(f\"   LLM: Keep existing Ollama setup with caching optimizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key_learnings",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "### Voice Interface Complexity\n",
    "\n",
    "1. **Multi-Component Integration**\n",
    "   - Voice systems require coordination of 6+ components\n",
    "   - Each component adds latency and potential failure points\n",
    "   - Integration complexity grows exponentially with features\n",
    "\n",
    "2. **Latency is Critical**\n",
    "   - Users expect <2 second response times for natural conversation\n",
    "   - Each 500ms of delay significantly degrades user experience\n",
    "   - Optimization must happen at every layer of the stack\n",
    "\n",
    "3. **State Management Challenges**\n",
    "   - Turn-taking requires sophisticated state machines\n",
    "   - Interruption handling is complex but essential\n",
    "   - Conversation context must be maintained across turns\n",
    "\n",
    "### Architecture Patterns\n",
    "\n",
    "1. **Modular Design**\n",
    "   - Each component should be swappable\n",
    "   - Abstract interfaces allow technology upgrades\n",
    "   - Microservices approach enables scaling individual components\n",
    "\n",
    "2. **Streaming Everything**\n",
    "   - Stream audio capture for real-time VAD\n",
    "   - Stream TTS generation for perceived performance\n",
    "   - Stream LLM responses when possible\n",
    "\n",
    "3. **Graceful Degradation**\n",
    "   - Fall back to text interface if voice fails\n",
    "   - Continue with partial results if some components fail\n",
    "   - Provide user feedback about system state\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "1. **Privacy First**\n",
    "   - Voice data is highly sensitive\n",
    "   - Implement data minimization principles\n",
    "   - Clear user consent and data retention policies\n",
    "\n",
    "2. **Monitor Everything**\n",
    "   - Track latency at each component\n",
    "   - Monitor speech recognition accuracy\n",
    "   - Measure user satisfaction and completion rates\n",
    "\n",
    "3. **Plan for Scale**\n",
    "   - Voice processing is compute-intensive\n",
    "   - WebRTC requires specialized infrastructure\n",
    "   - Geographic distribution essential for global deployment\n",
    "\n",
    "### Next Steps for Implementation\n",
    "\n",
    "1. **Start Simple**: Begin with push-to-talk rather than continuous listening\n",
    "2. **Optimize Incrementally**: Focus on one latency bottleneck at a time\n",
    "3. **Test Extensively**: Voice interfaces have many edge cases\n",
    "4. **Gather Feedback**: User testing reveals real-world usage patterns\n",
    "5. **Plan for Costs**: Voice APIs can be expensive at scale\n",
    "\n",
    "### Integration with Existing Agent\n",
    "\n",
    "The beauty of this architecture is that your existing LangGraph invoice agent requires minimal changes:\n",
    "- Add voice-optimized response formatting\n",
    "- Handle conversation context in state\n",
    "- Optimize prompts for voice queries\n",
    "- The core reasoning and tool calling remains unchanged\n",
    "\n",
    "Voice adds a powerful interface layer while preserving all your existing agent capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}