{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2, Session 1 Lab: Add Vision to Text Agent\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "**Estimated Time:** 40 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Prerequisites:** Completion of Day 1 labs\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Extend Text Agents with Vision**\n",
    "   - Modify yesterday's support agent to handle images\n",
    "   - Implement proper state management for multimodal data\n",
    "   - Build conditional routing based on input modality\n",
    "\n",
    "2. **Implement Real Vision Models**\n",
    "   - Load and use CLIP for image classification\n",
    "   - Apply BLIP-2 for image captioning\n",
    "   - Integrate EasyOCR for text extraction\n",
    "   - Manage GPU memory effectively\n",
    "\n",
    "3. **Build Production-Ready Multimodal Systems**\n",
    "   - Handle different input combinations gracefully\n",
    "   - Implement error handling for GPU limitations\n",
    "   - Create robust vision processing pipelines\n",
    "\n",
    "### Real-World Application\n",
    "\n",
    "This lab simulates building a customer support system that can:\n",
    "- Answer text questions about invoices\n",
    "- Process uploaded invoice images\n",
    "- Handle combined text+image queries\n",
    "- Extract structured data from documents\n",
    "\n",
    "### Lab Structure\n",
    "\n",
    "1. **Extend State Management** (5 minutes)\n",
    "2. **Implement Vision Models** (15 minutes)  \n",
    "3. **Build Vision Processing Nodes** (10 minutes)\n",
    "4. **Update Graph Routing** (5 minutes)\n",
    "5. **Test with Real Invoices** (5 minutes)\n",
    "\n",
    "Let's transform your text agent into a multimodal powerhouse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server configuration - instructor provides actual values\n",
    "OLLAMA_URL = \"http://XX.XX.XX.XX\"  # Course server IP\n",
    "API_TOKEN = \"YOUR_TOKEN_HERE\"      # Instructor provides token\n",
    "MODEL = \"qwen3:8b\"                  # Default model on server\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import torch\n",
    "from typing import Dict, List, Optional, Any, TypedDict\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU memory monitoring\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Get current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3    # GB\n",
    "        return {\"allocated\": allocated, \"reserved\": reserved}\n",
    "    return {\"allocated\": 0, \"reserved\": 0}\n",
    "\n",
    "# Health check\n",
    "def check_server_health():\n",
    "    \"\"\"Verify server connection and model availability\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_URL}/health\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"âœ… Server Status: {data.get('status', 'Unknown')}\")\n",
    "            print(f\"ğŸ“Š Models Available: {data.get('models_count', 0)}\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Server connection failed: {e}\")\n",
    "    return False\n",
    "\n",
    "# LLM calling function\n",
    "def call_llm(prompt, model=MODEL):\n",
    "    \"\"\"Call the LLM with a prompt\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/think\",\n",
    "            headers=headers,\n",
    "            json=data\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get('response', '')\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "print(\"ğŸ”Œ Connecting to course server...\")\n",
    "server_available = check_server_health()\n",
    "\n",
    "print(f\"\\nğŸ–¥ï¸ GPU Status: {'Available' if torch.cuda.is_available() else 'Not Available'}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_mem = get_gpu_memory()\n",
    "    print(f\"ğŸ’¾ GPU Memory: {gpu_mem['allocated']:.1f}GB allocated, {gpu_mem['reserved']:.1f}GB reserved\")\n",
    "\n",
    "# Install required packages for vision processing\n",
    "print(\"\\nğŸ“¦ Installing vision processing packages...\")\n",
    "!pip install -q transformers torch torchvision easyocr pillow\n",
    "print(\"âœ… Packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download real invoice dataset\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "dropbox_url = \"https://www.dropbox.com/scl/fo/m9hyfmvi78snwv0nh34mo/AMEXxwXMLAOeve-_yj12ck8?rlkey=urinkikgiuven0fro7r4x5rcu&st=hv3of7g7&dl=1\"\n",
    "\n",
    "print(\"ğŸ“¦ Downloading invoice dataset...\")\n",
    "try:\n",
    "    response = requests.get(dropbox_url)\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "        z.extractall(\"invoice_images\")\n",
    "    print(\"âœ… Downloaded invoice dataset\")\n",
    "    \n",
    "    # List available images for testing\n",
    "    INVOICE_FILES = []\n",
    "    for root, dirs, files in os.walk(\"invoice_images\"):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                full_path = os.path.join(root, file)\n",
    "                INVOICE_FILES.append(full_path)\n",
    "                print(f\"  ğŸ“„ {full_path}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Total images available: {len(INVOICE_FILES)}\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"âŒ Error downloading: {e}\")\n",
    "    INVOICE_FILES = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Extend State Management (5 minutes)\n",
    "\n",
    "First, extend yesterday's simple support state to handle multimodal data.\n",
    "\n",
    "**Your Task:** Complete the `MultimodalSupportState` class by adding the missing fields for image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Optional, Dict, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Yesterday's simple state for reference\n",
    "class SimpleSupportState(TypedDict):\n",
    "    \"\"\"Yesterday's basic support state\"\"\"\n",
    "    question: str\n",
    "    answer: str\n",
    "    context: str\n",
    "\n",
    "# TODO: Complete the multimodal state\n",
    "class MultimodalSupportState(TypedDict):\n",
    "    \"\"\"Enhanced state for multimodal support agent\"\"\"\n",
    "    # Basic text fields (from yesterday)\n",
    "    question: Optional[str]\n",
    "    answer: Optional[str]\n",
    "    context: str\n",
    "    \n",
    "    # TODO: Add image-related fields\n",
    "    # Hint: You need fields for:\n",
    "    # - List of base64 encoded images\n",
    "    # - List of image descriptions\n",
    "    # - List of extracted text from images\n",
    "    # - Current modality being processed\n",
    "    # - Confidence score for vision processing\n",
    "    # - Processing metrics dictionary\n",
    "    \n",
    "    # TODO: Uncomment and complete these fields:\n",
    "    # images: List[str]  # Base64 encoded images\n",
    "    # image_descriptions: List[str]  # Generated descriptions\n",
    "    # extracted_text: List[str]  # OCR results\n",
    "    # modality: str  # 'text', 'image', or 'multimodal'\n",
    "    # vision_confidence: float  # Confidence in vision processing\n",
    "    # processing_metrics: Dict[str, Any]  # Performance tracking\n",
    "\n",
    "def create_initial_state(question=None, images=None):\n",
    "    \"\"\"Create a clean initial state for testing\"\"\"\n",
    "    # TODO: Complete this function to create a proper initial state\n",
    "    # Use the MultimodalSupportState structure\n",
    "    \n",
    "    state = {\n",
    "        \"question\": question,\n",
    "        \"answer\": None,\n",
    "        \"context\": \"\",\n",
    "        # TODO: Add the missing fields with appropriate default values\n",
    "    }\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Test your state creation\n",
    "print(\"ğŸ§ª Testing state creation...\")\n",
    "test_state = create_initial_state(\"What is this document?\", [])\n",
    "print(f\"Created state with {len(test_state)} fields\")\n",
    "\n",
    "# TODO: Verify your state has all required fields\n",
    "required_fields = ['question', 'answer', 'context', 'images', 'image_descriptions', \n",
    "                  'extracted_text', 'modality', 'vision_confidence', 'processing_metrics']\n",
    "\n",
    "missing_fields = [field for field in required_fields if field not in test_state]\n",
    "if missing_fields:\n",
    "    print(f\"âŒ Missing fields: {missing_fields}\")\n",
    "    print(\"ğŸ’¡ Hint: Check your MultimodalSupportState definition\")\nelse:\n",
    "    print(\"âœ… All required fields present!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement Vision Models (15 minutes)\n",
    "\n",
    "Load and configure real vision models optimized for T4 GPU (16GB memory).\n",
    "\n",
    "**Your Task:** Complete the vision model setup and processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n",
    "import easyocr\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Global model storage\n",
    "VISION_MODELS = {}\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear GPU cache to manage memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def setup_vision_models():\n",
    "    \"\"\"Load vision models optimized for T4 GPU\"\"\"\n",
    "    global VISION_MODELS\n",
    "    \n",
    "    print(\"ğŸ”§ Setting up vision models...\")\n",
    "    \n",
    "    # Check initial GPU memory\n",
    "    initial_memory = get_gpu_memory()\n",
    "    print(f\"ğŸ“Š Initial GPU memory: {initial_memory['allocated']:.1f}GB\")\n",
    "    \n",
    "    try:\n",
    "        # TODO: Load CLIP model for image classification\n",
    "        # Hint: Use \"openai/clip-vit-base-patch32\" - it's smaller and faster\n",
    "        # Remember to move to GPU if available\n",
    "        \n",
    "        print(\"ğŸ“¸ Loading CLIP model...\")\n",
    "        # TODO: Uncomment and complete:\n",
    "        # clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        # clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        # \n",
    "        # if torch.cuda.is_available():\n",
    "        #     clip_model = clip_model.to('cuda')\n",
    "        # \n",
    "        # VISION_MODELS['clip_model'] = clip_model\n",
    "        # VISION_MODELS['clip_processor'] = clip_processor\n",
    "        \n",
    "        memory_after_clip = get_gpu_memory()\n",
    "        print(f\"   GPU memory after CLIP: {memory_after_clip['allocated']:.1f}GB\")\n",
    "        \n",
    "        # TODO: Load BLIP-2 model for image captioning\n",
    "        # Hint: Use \"Salesforce/blip-image-captioning-base\" for T4 compatibility\n",
    "        \n",
    "        print(\"ğŸ“ Loading BLIP model...\")\n",
    "        # TODO: Uncomment and complete:\n",
    "        # blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        # blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        # \n",
    "        # if torch.cuda.is_available():\n",
    "        #     blip_model = blip_model.to('cuda')\n",
    "        # \n",
    "        # VISION_MODELS['blip_model'] = blip_model\n",
    "        # VISION_MODELS['blip_processor'] = blip_processor\n",
    "        \n",
    "        memory_after_blip = get_gpu_memory()\n",
    "        print(f\"   GPU memory after BLIP: {memory_after_blip['allocated']:.1f}GB\")\n",
    "        \n",
    "        # TODO: Initialize EasyOCR reader\n",
    "        # Hint: Use English and common European languages\n",
    "        \n",
    "        print(\"ğŸ”¤ Loading EasyOCR...\")\n",
    "        # TODO: Uncomment and complete:\n",
    "        # ocr_reader = easyocr.Reader(['en', 'fr', 'de', 'es'], gpu=torch.cuda.is_available())\n",
    "        # VISION_MODELS['ocr_reader'] = ocr_reader\n",
    "        \n",
    "        final_memory = get_gpu_memory()\n",
    "        print(f\"   Final GPU memory: {final_memory['allocated']:.1f}GB\")\n",
    "        \n",
    "        total_used = final_memory['allocated'] - initial_memory['allocated']\n",
    "        print(f\"âœ… Models loaded successfully! Used {total_used:.1f}GB GPU memory\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading models: {e}\")\n",
    "        print(\"ğŸ’¡ Try clearing GPU cache or using smaller models\")\n",
    "        clear_gpu_cache()\n",
    "        return False\n",
    "\n",
    "def encode_image_to_base64(image_path: str) -> str:\n",
    "    \"\"\"Convert image file to base64 string\"\"\"\n",
    "    try:\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding image: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def decode_base64_to_image(base64_string: str) -> Image.Image:\n",
    "    \"\"\"Convert base64 string to PIL Image\"\"\"\n",
    "    try:\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        return Image.open(io.BytesIO(image_data))\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding image: {e}\")\n",
    "        return None\n",
    "\n",
    "# TODO: Call your setup function\n",
    "print(\"ğŸš€ Initializing vision models...\")\n",
    "models_loaded = setup_vision_models()\n",
    "\nif not models_loaded:\n",
    "    print(\"\\nâš ï¸ Models not loaded. Check GPU memory and try again.\")\n",
    "    print(\"ğŸ’¡ You can continue with mock implementations for now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision processing functions\n",
    "\n",
    "def classify_image_with_clip(image: Image.Image, categories: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Classify image using CLIP model\"\"\"\n",
    "    # TODO: Implement CLIP classification\n",
    "    # Steps:\n",
    "    # 1. Get CLIP model and processor from VISION_MODELS\n",
    "    # 2. Process image and text categories\n",
    "    # 3. Compute similarities\n",
    "    # 4. Return top category with confidence\n",
    "    \n",
    "    if 'clip_model' not in VISION_MODELS:\n",
    "        return {\"category\": \"unknown\", \"confidence\": 0.0, \"error\": \"CLIP model not loaded\"}\n",
    "    \n",
    "    try:\n",
    "        # TODO: Uncomment and complete:\n",
    "        # model = VISION_MODELS['clip_model']\n",
    "        # processor = VISION_MODELS['clip_processor']\n",
    "        # \n",
    "        # # Process inputs\n",
    "        # inputs = processor(text=categories, images=image, return_tensors=\"pt\", padding=True)\n",
    "        # \n",
    "        # if torch.cuda.is_available():\n",
    "        #     inputs = {k: v.to('cuda') if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "        # \n",
    "        # # Get predictions\n",
    "        # with torch.no_grad():\n",
    "        #     outputs = model(**inputs)\n",
    "        #     logits_per_image = outputs.logits_per_image\n",
    "        #     probs = logits_per_image.softmax(dim=-1)\n",
    "        # \n",
    "        # # Get top prediction\n",
    "        # top_prob, top_idx = probs[0].max(dim=0)\n",
    "        # \n",
    "        # return {\n",
    "        #     \"category\": categories[top_idx.item()],\n",
    "        #     \"confidence\": top_prob.item(),\n",
    "        #     \"all_scores\": {cat: prob.item() for cat, prob in zip(categories, probs[0])}\n",
    "        # }\n",
    "        \n",
    "        # Mock implementation for now\n",
    "        return {\"category\": \"invoice\", \"confidence\": 0.85, \"method\": \"mock\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"category\": \"unknown\", \"confidence\": 0.0, \"error\": str(e)}\n",
    "\n",
    "def generate_image_caption(image: Image.Image) -> Dict[str, Any]:\n",
    "    \"\"\"Generate caption for image using BLIP\"\"\"\n",
    "    # TODO: Implement BLIP captioning\n",
    "    # Steps:\n",
    "    # 1. Get BLIP model and processor\n",
    "    # 2. Process image\n",
    "    # 3. Generate caption\n",
    "    # 4. Return caption with confidence\n",
    "    \n",
    "    if 'blip_model' not in VISION_MODELS:\n",
    "        return {\"caption\": \"Unable to generate caption\", \"confidence\": 0.0, \"error\": \"BLIP model not loaded\"}\n",
    "    \n",
    "    try:\n",
    "        # TODO: Uncomment and complete:\n",
    "        # model = VISION_MODELS['blip_model']\n",
    "        # processor = VISION_MODELS['blip_processor']\n",
    "        # \n",
    "        # # Process image\n",
    "        # inputs = processor(image, return_tensors=\"pt\")\n",
    "        # \n",
    "        # if torch.cuda.is_available():\n",
    "        #     inputs = {k: v.to('cuda') if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "        # \n",
    "        # # Generate caption\n",
    "        # with torch.no_grad():\n",
    "        #     out = model.generate(**inputs, max_length=50)\n",
    "        # \n",
    "        # caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "        # \n",
    "        # return {\n",
    "        #     \"caption\": caption,\n",
    "        #     \"confidence\": 0.8,  # BLIP doesn't provide confidence directly\n",
    "        #     \"length\": len(caption.split())\n",
    "        # }\n",
    "        \n",
    "        # Mock implementation for now\n",
    "        return {\"caption\": \"A professional invoice document with company information and itemized costs\", \"confidence\": 0.75, \"method\": \"mock\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"caption\": \"Caption generation failed\", \"confidence\": 0.0, \"error\": str(e)}\n",
    "\n",
    "def extract_text_with_ocr(image: Image.Image) -> Dict[str, Any]:\n",
    "    \"\"\"Extract text from image using EasyOCR\"\"\"\n",
    "    # TODO: Implement OCR text extraction\n",
    "    # Steps:\n",
    "    # 1. Get OCR reader\n",
    "    # 2. Convert PIL image to format EasyOCR expects\n",
    "    # 3. Extract text with bounding boxes\n",
    "    # 4. Return text and confidence scores\n",
    "    \n",
    "    if 'ocr_reader' not in VISION_MODELS:\n",
    "        return {\"text\": \"\", \"confidence\": 0.0, \"error\": \"OCR reader not loaded\"}\n",
    "    \n",
    "    try:\n",
    "        # TODO: Uncomment and complete:\n",
    "        # reader = VISION_MODELS['ocr_reader']\n",
    "        # \n",
    "        # # Convert PIL to numpy array\n",
    "        # image_array = np.array(image)\n",
    "        # \n",
    "        # # Extract text\n",
    "        # results = reader.readtext(image_array)\n",
    "        # \n",
    "        # # Process results\n",
    "        # extracted_text = []\n",
    "        # confidences = []\n",
    "        # \n",
    "        # for (bbox, text, conf) in results:\n",
    "        #     if conf > 0.5:  # Filter low confidence\n",
    "        #         extracted_text.append(text)\n",
    "        #         confidences.append(conf)\n",
    "        # \n",
    "        # full_text = ' '.join(extracted_text)\n",
    "        # avg_confidence = np.mean(confidences) if confidences else 0.0\n",
    "        # \n",
    "        # return {\n",
    "        #     \"text\": full_text,\n",
    "        #     \"confidence\": avg_confidence,\n",
    "        #     \"word_count\": len(extracted_text),\n",
    "        #     \"individual_results\": results\n",
    "        # }\n",
    "        \n",
    "        # Mock implementation for now\n",
    "        mock_text = \"INVOICE\\nCompany: TechSupplies Co.\\nAmount: $15,000.00\\nDate: 2024-01-15\\nVAT: GB123456789\"\n",
    "        return {\"text\": mock_text, \"confidence\": 0.92, \"word_count\": 8, \"method\": \"mock\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"text\": \"\", \"confidence\": 0.0, \"error\": str(e)}\n",
    "\n",
    "print(\"âœ… Vision processing functions defined\")\nprint(\"ğŸ“ TODO: Complete the implementation by uncommenting and filling in the TODOs\")\nprint(\"ğŸ’¡ Start with mock implementations, then replace with real models once loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Build Vision Processing Nodes (10 minutes)\n",
    "\n",
    "Create LangGraph nodes that process different aspects of images.\n",
    "\n",
    "**Your Task:** Complete the vision processing nodes for the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_modality(state: MultimodalSupportState) -> MultimodalSupportState:\n",
    "    \"\"\"Detect whether input is text, image, or multimodal\"\"\"\n",
    "    # TODO: Implement modality detection\n",
    "    # Check what types of input are present and set the modality field\n",
    "    \n",
    "    has_text = state.get('question') and len(state['question'].strip()) > 0\n",
    "    has_images = state.get('images') and len(state['images']) > 0\n",
    "    \n",
    "    # TODO: Set the modality based on inputs\n",
    "    if has_text and has_images:\n",
    "        # TODO: Set state['modality'] = \"multimodal\"\n",
    "        pass\n",
    "    elif has_images:\n",
    "        # TODO: Set state['modality'] = \"image\"\n",
    "        pass\n",
    "    elif has_text:\n",
    "        # TODO: Set state['modality'] = \"text\"\n",
    "        pass\n",
    "    else:\n",
    "        # TODO: Set state['modality'] = \"unknown\"\n",
    "        pass\n",
    "    \n",
    "    print(f\"ğŸ” Detected modality: {state.get('modality', 'unknown')}\")\n",
    "    return state\n",
    "\n",
    "def classify_document(state: MultimodalSupportState) -> MultimodalSupportState:\n",
    "    \"\"\"Classify document type using CLIP\"\"\"\n",
    "    if not state.get('images'):\n",
    "        return state\n",
    "    \n",
    "    print(\"ğŸ“‹ Classifying document type...\")\n",
    "    \n",
    "    # TODO: Process each image with CLIP\n",
    "    categories = [\"invoice\", \"receipt\", \"contract\", \"letter\", \"form\", \"other document\"]\n",
    "    \n",
    "    classifications = []\n",
    "    \n",
    "    for i, image_b64 in enumerate(state['images']):\n",
    "        try:\n",
    "            # TODO: Decode base64 to PIL Image\n",
    "            # image = decode_base64_to_image(image_b64)\n",
    "            # \n",
    "            # if image:\n",
    "            #     result = classify_image_with_clip(image, categories)\n",
    "            #     classifications.append(result)\n",
    "            #     print(f\"   Image {i+1}: {result['category']} (confidence: {result['confidence']:.2f})\")\n",
    "            \n",
    "            # Mock for now\n",
    "            classifications.append({\"category\": \"invoice\", \"confidence\": 0.85})\n",
    "            print(f\"   Image {i+1}: invoice (confidence: 0.85)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Error classifying image {i+1}: {e}\")\n",
    "            classifications.append({\"category\": \"unknown\", \"confidence\": 0.0})\n",
    "    \n",
    "    # TODO: Store classifications in processing_metrics\n",
    "    if 'processing_metrics' not in state:\n",
    "        state['processing_metrics'] = {}\n",
    "    state['processing_metrics']['classifications'] = classifications\n",
    "    \n",
    "    return state\n",
    "\n",
    "def describe_image(state: MultimodalSupportState) -> MultimodalSupportState:\n",
    "    \"\"\"Generate descriptions for images using BLIP\"\"\"\n",
    "    if not state.get('images'):\n",
    "        return state\n",
    "    \n",
    "    print(\"ğŸ“ Generating image descriptions...\")\n",
    "    \n",
    "    descriptions = []\n",
    "    confidences = []\n",
    "    \n",
    "    # TODO: Process each image with BLIP\n",
    "    for i, image_b64 in enumerate(state['images']):\n",
    "        try:\n",
    "            # TODO: Decode and caption the image\n",
    "            # image = decode_base64_to_image(image_b64)\n",
    "            # \n",
    "            # if image:\n",
    "            #     result = generate_image_caption(image)\n",
    "            #     descriptions.append(result['caption'])\n",
    "            #     confidences.append(result['confidence'])\n",
    "            #     print(f\"   Image {i+1}: {result['caption'][:100]}...\")\n",
    "            \n",
    "            # Mock for now\n",
    "            mock_description = \"A professional invoice document with company letterhead, itemized costs, and payment details\"\n",
    "            descriptions.append(mock_description)\n",
    "            confidences.append(0.75)\n",
    "            print(f\"   Image {i+1}: {mock_description[:60]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Error describing image {i+1}: {e}\")\n",
    "            descriptions.append(\"Unable to describe image\")\n",
    "            confidences.append(0.0)\n",
    "    \n",
    "    # TODO: Update state with descriptions\n",
    "    state['image_descriptions'] = descriptions\n",
    "    \n",
    "    # Update confidence score\n",
    "    avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0\n",
    "    state['vision_confidence'] = avg_confidence\n",
    "    \n",
    "    return state\n",
    "\n",
    "def extract_text_ocr(state: MultimodalSupportState) -> MultimodalSupportState:\n",
    "    \"\"\"Extract text from images using OCR\"\"\"\n",
    "    if not state.get('images'):\n",
    "        return state\n",
    "    \n",
    "    print(\"ğŸ”¤ Extracting text with OCR...\")\n",
    "    \n",
    "    extracted_texts = []\n",
    "    ocr_confidences = []\n",
    "    \n",
    "    # TODO: Process each image with OCR\n",
    "    for i, image_b64 in enumerate(state['images']):\n",
    "        try:\n",
    "            # TODO: Extract text from image\n",
    "            # image = decode_base64_to_image(image_b64)\n",
    "            # \n",
    "            # if image:\n",
    "            #     result = extract_text_with_ocr(image)\n",
    "            #     extracted_texts.append(result['text'])\n",
    "            #     ocr_confidences.append(result['confidence'])\n",
    "            #     print(f\"   Image {i+1}: Extracted {len(result['text'])} characters (conf: {result['confidence']:.2f})\")\n",
    "            \n",
    "            # Mock for now\n",
    "            mock_text = \"INVOICE\\nCompany: TechSupplies Co.\\nAmount: $15,000.00\\nDate: 2024-01-15\\nVAT: GB123456789\"\n",
    "            extracted_texts.append(mock_text)\n",
    "            ocr_confidences.append(0.92)\n",
    "            print(f\"   Image {i+1}: Extracted {len(mock_text)} characters (conf: 0.92)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Error extracting text from image {i+1}: {e}\")\n",
    "            extracted_texts.append(\"\")\n",
    "            ocr_confidences.append(0.0)\n",
    "    \n",
    "    # TODO: Update state with extracted text\n",
    "    state['extracted_text'] = extracted_texts\n",
    "    \n",
    "    # Update processing metrics\n",
    "    if 'processing_metrics' not in state:\n",
    "        state['processing_metrics'] = {}\n",
    "    state['processing_metrics']['ocr_confidences'] = ocr_confidences\n",
    "    \n",
    "    return state\n",
    "\n",
    "def validate_extraction(state: MultimodalSupportState) -> MultimodalSupportState:\n",
    "    \"\"\"Validate that vision processing succeeded\"\"\"\n",
    "    print(\"âœ… Validating extraction results...\")\n",
    "    \n",
    "    # TODO: Check if vision processing was successful\n",
    "    has_descriptions = state.get('image_descriptions') and any(desc for desc in state['image_descriptions'])\n",
    "    has_text = state.get('extracted_text') and any(text for text in state['extracted_text'])\n",
    "    confidence_ok = state.get('vision_confidence', 0) > 0.5\n",
    "    \n",
    "    validation_status = {\n",
    "        'has_descriptions': has_descriptions,\n",
    "        'has_extracted_text': has_text,\n",
    "        'confidence_acceptable': confidence_ok,\n",
    "        'overall_success': has_descriptions and has_text and confidence_ok\n",
    "    }\n",
    "    \n",
    "    if 'processing_metrics' not in state:\n",
    "        state['processing_metrics'] = {}\n",
    "    state['processing_metrics']['validation'] = validation_status\n",
    "    \n",
    "    print(f\"   Descriptions: {'âœ…' if has_descriptions else 'âŒ'}\")\n",
    "    print(f\"   Extracted text: {'âœ…' if has_text else 'âŒ'}\")\n",
    "    print(f\"   Confidence: {'âœ…' if confidence_ok else 'âŒ'} ({state.get('vision_confidence', 0):.2f})\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"âœ… Vision processing nodes defined\")\nprint(\"ğŸ“ TODO: Complete the implementations by filling in the TODO sections\")\nprint(\"ğŸ’¡ Test each node individually before building the full graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Update Graph Routing (5 minutes)\n",
    "\n",
    "Build a workflow that routes based on detected modality.\n",
    "\n",
    "**Your Task:** Complete the multimodal workflow graph with proper routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_answer(state: MultimodalSupportState) -> MultimodalSupportState:\n",
    "    \"\"\"Generate final answer using all available information\"\"\"\n",
    "    print(\"ğŸ¤– Generating final answer...\")\n",
    "    \n",
    "    # Collect all available information\n",
    "    context_parts = []\n",
    "    \n",
    "    if state.get('question'):\n",
    "        context_parts.append(f\"Question: {state['question']}\")\n",
    "    \n",
    "    if state.get('image_descriptions'):\n",
    "        context_parts.append(f\"Image descriptions: {' | '.join(state['image_descriptions'])}\")\n",
    "    \n",
    "    if state.get('extracted_text'):\n",
    "        context_parts.append(f\"Extracted text: {' | '.join(state['extracted_text'])}\")\n",
    "    \n",
    "    # TODO: Use LLM to generate comprehensive answer\n",
    "    if server_available and context_parts:\n",
    "        context = \"\\n\".join(context_parts)\n",
    "        prompt = f\"\"\"Based on the following information about a document, provide a helpful response:\n",
    "\n{context}\n",
    "\nProvide a clear, informative answer that addresses the question and incorporates all available information.\"\"\"\n",
    "        \n",
    "        response = call_llm(prompt)\n",
    "        state['answer'] = response\n",
    "    else:\n",
    "        # Fallback answer\n",
    "        state['answer'] = f\"Based on the {state.get('modality', 'unknown')} input, I can provide information about the document.\"\n",
    "    \n",
    "    state['context'] = \"\\n\".join(context_parts)\n",
    "    \n",
    "    return state\n",
    "\n",
    "def route_by_modality(state: MultimodalSupportState) -> str:\n",
    "    \"\"\"Route processing based on detected modality\"\"\"\n",
    "    modality = state.get('modality', 'unknown')\n",
    "    \n",
    "    # TODO: Implement routing logic\n",
    "    if modality == 'text':\n",
    "        return 'generate_answer'  # Skip vision processing\n",
    "    elif modality in ['image', 'multimodal']:\n",
    "        return 'classify_document'  # Start vision processing\n",
    "    else:\n",
    "        return 'generate_answer'  # Default fallback\n",
    "\n",
    "# TODO: Build the multimodal workflow\n",
    "print(\"ğŸ—ï¸ Building multimodal workflow...\")\n",
    "\n",
    "# Create the graph\n",
    "multimodal_workflow = StateGraph(MultimodalSupportState)\n",
    "\n",
    "# TODO: Add all the nodes\n",
    "multimodal_workflow.add_node(\"detect_modality\", detect_modality)\n",
    "multimodal_workflow.add_node(\"classify_document\", classify_document)\n",
    "multimodal_workflow.add_node(\"describe_image\", describe_image)\n",
    "multimodal_workflow.add_node(\"extract_text_ocr\", extract_text_ocr)\n",
    "multimodal_workflow.add_node(\"validate_extraction\", validate_extraction)\n",
    "multimodal_workflow.add_node(\"generate_answer\", generate_final_answer)\n",
    "\n",
    "# TODO: Set entry point\n",
    "multimodal_workflow.set_entry_point(\"detect_modality\")\n",
    "\n",
    "# TODO: Add conditional routing after modality detection\n",
    "multimodal_workflow.add_conditional_edges(\n",
    "    \"detect_modality\",\n",
    "    route_by_modality,\n",
    "    {\n",
    "        \"classify_document\": \"classify_document\",\n",
    "        \"generate_answer\": \"generate_answer\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# TODO: Add sequential edges for vision processing\n",
    "multimodal_workflow.add_edge(\"classify_document\", \"describe_image\")\n",
    "multimodal_workflow.add_edge(\"describe_image\", \"extract_text_ocr\")\n",
    "multimodal_workflow.add_edge(\"extract_text_ocr\", \"validate_extraction\")\n",
    "multimodal_workflow.add_edge(\"validate_extraction\", \"generate_answer\")\n",
    "\n",
    "# TODO: End after generating answer\n",
    "multimodal_workflow.add_edge(\"generate_answer\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "try:\n",
    "    multimodal_app = multimodal_workflow.compile()\n",
    "    print(\"âœ… Multimodal workflow compiled successfully!\")\nexcept Exception as e:\n",
    "    print(f\"âŒ Error compiling workflow: {e}\")\n    print(\"ğŸ’¡ Check your state definition and node implementations\")\n\n# Visualize the workflow structure\nprint(\"\\nğŸ“Š Workflow Structure:\")\nprint(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\nprint(\"â”‚ Detect          â”‚\")\nprint(\"â”‚ Modality        â”‚\")\nprint(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\")\nprint(\"          â”‚\")\nprint(\"     â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\")\nprint(\"     â–¼         â–¼\")\nprint(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”\")\nprint(\"â”‚ Vision  â”‚ â”‚ Directâ”‚\")\nprint(\"â”‚Pipeline â”‚ â”‚Answer â”‚\")\nprint(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜\")\nprint(\"     â”‚         â”‚\")\nprint(\"     â–¼         â”‚\")\nprint(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\")\nprint(\"â”‚   Final     â”‚ â”‚\")\nprint(\"â”‚  Answer     â”‚â—„â”˜\")\nprint(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Test with Real Invoices (5 minutes)\n",
    "\n",
    "Test your multimodal agent with different scenarios.\n",
    "\n",
    "**Your Task:** Run the test scenarios and debug any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multimodal_agent():\n",
    "    \"\"\"Test the multimodal agent with various scenarios\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§ª TESTING MULTIMODAL SUPPORT AGENT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test 1: Text-only query\n",
    "    print(\"\\nğŸ“ Test 1: Text-only Query\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    try:\n",
    "        text_state = create_initial_state(\n",
    "            question=\"What information is typically found on an invoice?\",\n",
    "            images=[]\n",
    "        )\n",
    "        \n",
    "        if 'multimodal_app' in globals():\n",
    "            result = multimodal_app.invoke(text_state)\n",
    "            print(f\"âœ… Success: {result.get('answer', 'No answer')[:150]}...\")\n",
    "            print(f\"   Modality: {result.get('modality')}\")\n",
    "        else:\n",
    "            print(\"âŒ Workflow not compiled\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Text test failed: {e}\")\n",
    "    \n",
    "    # Test 2: Image-only query\n",
    "    print(\"\\nğŸ–¼ï¸ Test 2: Image-only Query\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if INVOICE_FILES:\n",
    "        try:\n",
    "            # TODO: Test with real invoice image\n",
    "            sample_image = INVOICE_FILES[0]\n",
    "            encoded_image = encode_image_to_base64(sample_image)\n",
    "            \n",
    "            if encoded_image:\n",
    "                image_state = create_initial_state(\n",
    "                    question=None,\n",
    "                    images=[encoded_image]\n",
    "                )\n",
    "                \n",
    "                if 'multimodal_app' in globals():\n",
    "                    result = multimodal_app.invoke(image_state)\n",
    "                    print(f\"âœ… Success: Processed image\")\n",
    "                    print(f\"   Modality: {result.get('modality')}\")\n",
    "                    print(f\"   Descriptions: {len(result.get('image_descriptions', []))} generated\")\n",
    "                    print(f\"   Extracted text: {len(result.get('extracted_text', []))} results\")\n",
    "                    print(f\"   Confidence: {result.get('vision_confidence', 0):.2f}\")\n",
    "                else:\n",
    "                    print(\"âŒ Workflow not compiled\")\n",
    "            else:\n",
    "                print(\"âŒ Could not encode image\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Image test failed: {e}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No invoice images available for testing\")\n",
    "    \n",
    "    # Test 3: Multimodal query\n",
    "    print(\"\\nğŸ­ Test 3: Multimodal Query (Text + Image)\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    if INVOICE_FILES:\n",
    "        try:\n",
    "            sample_image = INVOICE_FILES[0]\n",
    "            encoded_image = encode_image_to_base64(sample_image)\n",
    "            \n",
    "            if encoded_image:\n",
    "                multimodal_state = create_initial_state(\n",
    "                    question=\"What is the total amount on this invoice?\",\n",
    "                    images=[encoded_image]\n",
    "                )\n",
    "                \n",
    "                if 'multimodal_app' in globals():\n",
    "                    result = multimodal_app.invoke(multimodal_state)\n",
    "                    print(f\"âœ… Success: {result.get('answer', 'No answer')[:150]}...\")\n",
    "                    print(f\"   Modality: {result.get('modality')}\")\n",
    "                    print(f\"   Processing metrics: {result.get('processing_metrics', {})}\")\n",
    "                else:\n",
    "                    print(\"âŒ Workflow not compiled\")\n",
    "            else:\n",
    "                print(\"âŒ Could not encode image\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Multimodal test failed: {e}\")\n",
    "    \n",
    "    # Test 4: Error handling\n",
    "    print(\"\\nâš ï¸ Test 4: Error Handling\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    try:\n",
    "        # Test with invalid base64\n",
    "        error_state = create_initial_state(\n",
    "            question=\"Analyze this image\",\n",
    "            images=[\"invalid_base64_string\"]\n",
    "        )\n",
    "        \n",
    "        if 'multimodal_app' in globals():\n",
    "            result = multimodal_app.invoke(error_state)\n",
    "            print(f\"âœ… Error handled gracefully\")\n",
    "            print(f\"   Answer generated: {bool(result.get('answer'))}\")\n",
    "        else:\n",
    "            print(\"âŒ Workflow not compiled\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error handling test: {e}\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(\"\\nğŸ“Š Performance Summary\")\n",
    "    print(\"-\" * 25)\n",
    "    final_memory = get_gpu_memory()\n",
    "    print(f\"GPU Memory: {final_memory['allocated']:.1f}GB allocated\")\n",
    "    print(f\"Models loaded: {len(VISION_MODELS)}\")\n",
    "    print(f\"Test images available: {len(INVOICE_FILES)}\")\n",
    "\n",
    "# TODO: Run the tests\n",
    "test_multimodal_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Completion and Self-Assessment\n",
    "\n",
    "### What You've Built\n",
    "\n",
    "Congratulations! You've successfully extended a text-only agent into a full multimodal system with:\n",
    "\n",
    "1. **Enhanced State Management**\n",
    "   - Extended simple text state to handle images and metadata\n",
    "   - Added proper tracking for vision processing results\n",
    "   - Implemented modality detection and routing\n",
    "\n",
    "2. **Real Vision Models**\n",
    "   - Integrated CLIP for image classification\n",
    "   - Used BLIP-2 for automatic image captioning\n",
    "   - Applied EasyOCR for text extraction\n",
    "   - Managed GPU memory effectively\n",
    "\n",
    "3. **Production-Ready Pipeline**\n",
    "   - Built conditional routing based on input type\n",
    "   - Implemented error handling for GPU limitations\n",
    "   - Created comprehensive testing scenarios\n",
    "\n",
    "### Self-Assessment Questions\n",
    "\n",
    "Rate your understanding (1-5 scale) and provide brief explanations:\n",
    "\n",
    "1. **State Management** (1-5): ___\n",
    "   - How does multimodal state differ from text-only state?\n",
    "   - What challenges arise when combining different data types?\n",
    "\n",
    "2. **Vision Model Integration** (1-5): ___\n",
    "   - Why is GPU memory management critical for vision models?\n",
    "   - How do you choose appropriate models for your hardware?\n",
    "\n",
    "3. **Workflow Design** (1-5): ___\n",
    "   - How does conditional routing improve efficiency?\n",
    "   - What are the trade-offs between sequential and parallel processing?\n",
    "\n",
    "4. **Error Handling** (1-5): ___\n",
    "   - What types of errors are common in vision processing?\n",
    "   - How do you ensure graceful degradation?\n",
    "\n",
    "5. **Performance Optimization** (1-5): ___\n",
    "   - What strategies help manage GPU memory usage?\n",
    "   - How do you balance accuracy with processing speed?\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "**GPU Out of Memory (OOM)**\n",
    "- Use smaller model variants (e.g., CLIP base instead of large)\n",
    "- Clear cache between model loads: `torch.cuda.empty_cache()`\n",
    "- Process images in smaller batches\n",
    "- Use fp16 precision for inference\n",
    "\n",
    "**Poor OCR Results**\n",
    "- Preprocess images (resize, enhance contrast)\n",
    "- Use appropriate language settings for EasyOCR\n",
    "- Filter results by confidence threshold\n",
    "- Consider multiple OCR engines for redundancy\n",
    "\n",
    "**Slow Processing**\n",
    "- Cache model outputs when possible\n",
    "- Use GPU acceleration for all models\n",
    "- Implement batch processing for multiple images\n",
    "- Consider model quantization for speed\n",
    "\n",
    "### Extensions for Advanced Students\n",
    "\n",
    "If you completed the lab early, try these enhancements:\n",
    "\n",
    "1. **Confidence Thresholds**\n",
    "   - Implement dynamic confidence thresholds\n",
    "   - Route to human review for low-confidence results\n",
    "\n",
    "2. **Ensemble Approach**\n",
    "   - Combine multiple vision models for better accuracy\n",
    "   - Weight results based on model strengths\n",
    "\n",
    "3. **Image Preprocessing**\n",
    "   - Add image enhancement pipeline\n",
    "   - Implement automatic rotation detection\n",
    "\n",
    "4. **Batch Processing**\n",
    "   - Process multiple invoices simultaneously\n",
    "   - Implement progress tracking for large batches\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further your multimodal AI expertise:\n",
    "\n",
    "1. **Experiment with Different Models**\n",
    "   - Try LayoutLM for document understanding\n",
    "   - Explore newer vision-language models\n",
    "\n",
    "2. **Optimize for Production**\n",
    "   - Implement model quantization\n",
    "   - Add comprehensive monitoring\n",
    "\n",
    "3. **Build Domain-Specific Models**\n",
    "   - Fine-tune models on your specific document types\n",
    "   - Create custom classification categories\n",
    "\n",
    "**Congratulations!** You've successfully built a production-ready multimodal AI system that can process both text and images intelligently!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}