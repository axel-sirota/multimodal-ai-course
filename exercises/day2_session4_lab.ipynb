{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Day 2, Session 4 - Lab: Build a Prompt Management System\n",
    "\n",
    "## Goal: Build Production Prompt Management System\n",
    "\n",
    "You'll create a comprehensive prompt system that:\n",
    "- Adapts to different invoice formats\n",
    "- Guarantees structured outputs\n",
    "- Handles validation failures gracefully\n",
    "- Tracks performance metrics\n",
    "- Enables A/B testing of prompt variants\n",
    "\n",
    "This system will make your invoice agent reliable and maintainable!\n",
    "\n",
    "**Time Allocation: 45 minutes**\n",
    "- Task 1: Define Structured Output Models (8 min)\n",
    "- Task 2: Create Base Extraction Prompts (10 min)\n",
    "- Task 3: Implement Conditional Prompts (8 min)\n",
    "- Task 4: Add Instructor Integration (10 min)\n",
    "- Task 5: Implement Prompt Caching (5 min)\n",
    "- Task 6: Build Testing Framework (5 min)\n",
    "- Task 7: Add Versioning System (4 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration - Instructor will fill these\n",
    "OLLAMA_URL = \"http://XX.XX.XX.XX\"  # Course server IP (port 80)\n",
    "API_TOKEN = \"YOUR_TOKEN_HERE\"      # Instructor provides token\n",
    "MODEL = \"qwen3:8b\"                  # Default model on server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain instructor pydantic jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from instructor import patch\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Optional, Dict, Any\n",
    "import json\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "import hashlib\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Invoice Dataset\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "dropbox_url = \"https://www.dropbox.com/scl/fo/m9hyfmvi78snwv0nh34mo/AMEXxwXMLAOeve-_yj12ck8?rlkey=urinkikgiuven0fro7r4x5rcu&st=hv3of7g7&dl=1\"\n",
    "\n",
    "print(f\"Downloading data from: {dropbox_url}\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(dropbox_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "        z.extractall(\"downloaded_images\")\n",
    "\n",
    "    print(\"‚úÖ Downloaded and extracted images to 'downloaded_images' folder.\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå Error downloading the file: {e}\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(\"‚ùå Error: The downloaded file is not a valid zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1_title",
   "metadata": {},
   "source": [
    "## Task 1: Define Structured Output Models (8 minutes)\n",
    "\n",
    "### Create Pydantic Models for Invoice Data\n",
    "\n",
    "Define strict schemas that will be enforced on LLM outputs.\n",
    "Include validation rules and default values.\n",
    "\n",
    "**TODO Instructions:**\n",
    "1. Complete the LineItem validator to ensure total = quantity √ó unit_price\n",
    "2. Add more validators to InvoiceExtraction for business rules\n",
    "3. Test your models with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineItem(BaseModel):\n",
    "    \"\"\"Individual invoice line item\"\"\"\n",
    "    description: str = Field(description=\"Item description\")\n",
    "    quantity: float = Field(gt=0, description=\"Quantity ordered\")\n",
    "    unit_price: float = Field(ge=0, description=\"Price per unit\")\n",
    "    total: float = Field(ge=0, description=\"Line total\")\n",
    "    \n",
    "    @validator('total')\n",
    "    def validate_total(cls, v, values):\n",
    "        \"\"\"Ensure total = quantity * unit_price\"\"\"\n",
    "        # TODO: Implement validation\n",
    "        # if 'quantity' in values and 'unit_price' in values:\n",
    "        #     expected = values['quantity'] * values['unit_price']\n",
    "        #     if abs(v - expected) > 0.01:  # Allow small rounding differences\n",
    "        #         raise ValueError(f\"Total {v} doesn't match quantity {values['quantity']} √ó unit_price {values['unit_price']} = {expected}\")\n",
    "        # return v\n",
    "        \n",
    "        print(\"TODO: Complete total validation\")\n",
    "        return v\n",
    "\n",
    "class InvoiceExtraction(BaseModel):\n",
    "    \"\"\"Complete invoice data structure\"\"\"\n",
    "    invoice_number: str = Field(regex=r'^[A-Z0-9\\-]+$')\n",
    "    vendor_name: str = Field(min_length=2)\n",
    "    invoice_date: datetime\n",
    "    due_date: Optional[datetime] = None\n",
    "    \n",
    "    line_items: List[LineItem]\n",
    "    \n",
    "    subtotal: float = Field(ge=0)\n",
    "    tax_rate: float = Field(ge=0, le=1)\n",
    "    tax_amount: float = Field(ge=0)\n",
    "    total_amount: float = Field(ge=0)\n",
    "    \n",
    "    currency: str = Field(regex=r'^[A-Z]{3}$', default=\"USD\")\n",
    "    confidence_score: float = Field(ge=0, le=1)\n",
    "    \n",
    "    # TODO: Add more validators\n",
    "    @validator('tax_amount')\n",
    "    def validate_tax_calculation(cls, v, values):\n",
    "        \"\"\"Validate tax calculation\"\"\"\n",
    "        # TODO: Check if tax_amount = subtotal √ó tax_rate\n",
    "        print(\"TODO: Implement tax validation\")\n",
    "        return v\n",
    "    \n",
    "    @validator('total_amount')\n",
    "    def validate_total_amount(cls, v, values):\n",
    "        \"\"\"Validate total = subtotal + tax\"\"\"\n",
    "        # TODO: Check if total_amount = subtotal + tax_amount\n",
    "        print(\"TODO: Implement total amount validation\")\n",
    "        return v\n",
    "    \n",
    "    @validator('subtotal')\n",
    "    def validate_subtotal_matches_line_items(cls, v, values):\n",
    "        \"\"\"Validate subtotal matches sum of line items\"\"\"\n",
    "        # TODO: Check if subtotal = sum of all line item totals\n",
    "        print(\"TODO: Implement subtotal validation\")\n",
    "        return v\n",
    "\n",
    "# Test your models\n",
    "print(\"üß™ Testing Pydantic models...\")\n",
    "\n",
    "# TODO: Create test data and validate\n",
    "test_line_item = {\n",
    "    \"description\": \"Test Product\",\n",
    "    \"quantity\": 2.0,\n",
    "    \"unit_price\": 100.0,\n",
    "    \"total\": 200.0\n",
    "}\n",
    "\n",
    "try:\n",
    "    item = LineItem(**test_line_item)\n",
    "    print(f\"‚úÖ LineItem validation passed: {item.description}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LineItem validation failed: {e}\")\n",
    "\n",
    "print(\"\\nüìù TODO Summary for Task 1:\")\n",
    "print(\"1. Complete validate_total() in LineItem\")\n",
    "print(\"2. Implement tax_amount validation\")\n",
    "print(\"3. Add total_amount validation\")\n",
    "print(\"4. Validate subtotal matches line items sum\")\n",
    "print(\"5. Test with complete invoice data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2_title",
   "metadata": {},
   "source": [
    "## Task 2: Create Base Extraction Prompts (10 minutes)\n",
    "\n",
    "### Build LangChain Prompt Templates\n",
    "\n",
    "Create reusable, parameterized prompts with variables\n",
    "for dynamic content and few-shot examples.\n",
    "\n",
    "**TODO Instructions:**\n",
    "1. Complete the few-shot examples with realistic invoice data\n",
    "2. Create specialized prompts for different document types\n",
    "3. Test prompt rendering with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2_prompts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic extraction prompt\n",
    "basic_extraction_prompt = PromptTemplate(\n",
    "    input_variables=[\"invoice_text\", \"output_format\"],\n",
    "    template=\"\"\"Extract invoice information from the following text:\n",
    "\n",
    "{invoice_text}\n",
    "\n",
    "Return the data in this format:\n",
    "{output_format}\n",
    "\n",
    "Be precise and extract only what you can clearly identify.\"\"\"\n",
    ")\n",
    "\n",
    "# Few-shot prompt with examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Invoice #INV-001 from Acme Corp, Date: 2024-01-15, Item: Laptop $1200, Total: $1200\",\n",
    "        \"output\": '{\"invoice_number\": \"INV-001\", \"vendor_name\": \"Acme Corp\", \"total_amount\": 1200.0}'\n",
    "    },\n",
    "    # TODO: Add 2-3 more examples\n",
    "    # {\n",
    "    #     \"input\": \"Invoice #ABC-123 from TechSupplies Inc...\",\n",
    "    #     \"output\": '{\"invoice_number\": \"ABC-123\", ...}'\n",
    "    # },\n",
    "]\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=PromptTemplate(\n",
    "        input_variables=[\"input\", \"output\"],\n",
    "        template=\"Input: {input}\\nOutput: {output}\"\n",
    "    ),\n",
    "    prefix=\"Extract invoice data following these examples:\",\n",
    "    suffix=\"Now extract from:\\n{invoice_text}\",\n",
    "    input_variables=[\"invoice_text\"]\n",
    ")\n",
    "\n",
    "# Chain-of-thought prompt\n",
    "cot_extraction_prompt = PromptTemplate(\n",
    "    input_variables=[\"invoice_text\"],\n",
    "    template=\"\"\"Let's extract invoice data step by step.\n",
    "\n",
    "Step 1: Identify the vendor\n",
    "Step 2: Find invoice number and date\n",
    "Step 3: Extract line items\n",
    "Step 4: Calculate totals\n",
    "\n",
    "Invoice text:\n",
    "{invoice_text}\n",
    "\n",
    "Now perform each step:\"\"\"\n",
    ")\n",
    "\n",
    "# TODO: Create specialized prompts\n",
    "table_specific_prompt = PromptTemplate(\n",
    "    input_variables=[\"invoice_text\"],\n",
    "    template=\"\"\"This invoice contains complex tables. Extract data carefully:\n",
    "\n",
    "Focus on:\n",
    "- Table headers to identify columns\n",
    "- Row-by-row data extraction\n",
    "- Proper alignment of quantities and prices\n",
    "\n",
    "Invoice:\n",
    "{invoice_text}\n",
    "\n",
    "Extract table data systematically:\"\"\"\n",
    ")\n",
    "\n",
    "handwritten_prompt = PromptTemplate(\n",
    "    input_variables=[\"invoice_text\"],\n",
    "    template=\"\"\"This appears to be handwritten or poor quality text. Extract carefully:\n",
    "\n",
    "- Some characters may be unclear\n",
    "- Use context to infer missing information\n",
    "- Mark uncertain extractions with lower confidence\n",
    "\n",
    "Text:\n",
    "{invoice_text}\n",
    "\n",
    "Extract what you can determine:\"\"\"\n",
    ")\n",
    "\n",
    "# Test prompt rendering\n",
    "print(\"üß™ Testing prompt templates...\")\n",
    "\n",
    "sample_invoice = \"Invoice #TEST-001 from Sample Corp, Date: 2024-01-15, Total: $500.00\"\n",
    "sample_format = \"JSON with invoice_number, vendor_name, total_amount\"\n",
    "\n",
    "# Test basic prompt\n",
    "try:\n",
    "    rendered = basic_extraction_prompt.format(\n",
    "        invoice_text=sample_invoice,\n",
    "        output_format=sample_format\n",
    "    )\n",
    "    print(f\"‚úÖ Basic prompt rendered ({len(rendered)} chars)\")\n",
    "    print(f\"   Preview: {rendered[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Basic prompt failed: {e}\")\n",
    "\n",
    "# Test few-shot prompt\n",
    "try:\n",
    "    rendered = few_shot_prompt.format(invoice_text=sample_invoice)\n",
    "    print(f\"‚úÖ Few-shot prompt rendered ({len(rendered)} chars)\")\n",
    "    print(f\"   Examples included: {len(examples)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Few-shot prompt failed: {e}\")\n",
    "\n",
    "# Test CoT prompt\n",
    "try:\n",
    "    rendered = cot_extraction_prompt.format(invoice_text=sample_invoice)\n",
    "    print(f\"‚úÖ Chain-of-thought prompt rendered ({len(rendered)} chars)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå CoT prompt failed: {e}\")\n",
    "\n",
    "print(\"\\nüìù TODO Summary for Task 2:\")\n",
    "print(\"1. Add 2-3 more realistic examples to few_shot_prompt\")\n",
    "print(\"2. Create error-specific enhancement prompts\")\n",
    "print(\"3. Add prompts for multi-language invoices\")\n",
    "print(\"4. Test all prompts with various invoice formats\")\n",
    "print(\"5. Measure prompt token usage for optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task3_title",
   "metadata": {},
   "source": [
    "## Task 3: Implement Conditional Prompts (8 minutes)\n",
    "\n",
    "### Create Document-Specific Prompts\n",
    "\n",
    "Different document types and qualities need different approaches.\n",
    "Build a routing system to select optimal prompts.\n",
    "\n",
    "**TODO Instructions:**\n",
    "1. Complete the document analysis logic\n",
    "2. Implement prompt selection based on characteristics\n",
    "3. Add error-based prompt enhancement\n",
    "4. Test with different invoice types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3_conditional",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptSelector:\n",
    "    \"\"\"Select optimal prompt based on document characteristics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.prompts = {\n",
    "            'high_quality': basic_extraction_prompt,\n",
    "            'medium_quality': few_shot_prompt,\n",
    "            'poor_quality': cot_extraction_prompt,\n",
    "            'complex_table': table_specific_prompt,\n",
    "            'handwritten': handwritten_prompt\n",
    "        }\n",
    "        self.usage_stats = defaultdict(int)\n",
    "    \n",
    "    def analyze_document(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze document characteristics\n",
    "        TODO: Implement analysis logic\n",
    "        - Check text length\n",
    "        - Detect tables\n",
    "        - Assess quality indicators\n",
    "        \"\"\"\n",
    "        characteristics = {\n",
    "            'length': len(text),\n",
    "            'has_tables': False,\n",
    "            'quality_score': 1.0,\n",
    "            'language': 'en',\n",
    "            'complexity': 'simple'\n",
    "        }\n",
    "        \n",
    "        # TODO: Implement real analysis\n",
    "        # Check for table indicators\n",
    "        # table_indicators = ['|', '\\t', 'qty', 'quantity', 'price', 'total']\n",
    "        # characteristics['has_tables'] = any(indicator in text.lower() for indicator in table_indicators)\n",
    "        \n",
    "        # Assess quality based on text patterns\n",
    "        # quality_indicators = ['invoice', 'date', 'total', 'vendor']\n",
    "        # found_indicators = sum(1 for indicator in quality_indicators if indicator in text.lower())\n",
    "        # characteristics['quality_score'] = found_indicators / len(quality_indicators)\n",
    "        \n",
    "        # Detect complexity\n",
    "        # if len(text) > 1000:\n",
    "        #     characteristics['complexity'] = 'complex'\n",
    "        # elif characteristics['has_tables']:\n",
    "        #     characteristics['complexity'] = 'medium'\n",
    "        \n",
    "        print(f\"TODO: Complete document analysis - currently returning defaults\")\n",
    "        return characteristics\n",
    "    \n",
    "    def select_prompt(self, text: str) -> PromptTemplate:\n",
    "        \"\"\"\n",
    "        Choose best prompt for document\n",
    "        TODO: Implement selection logic\n",
    "        \"\"\"\n",
    "        characteristics = self.analyze_document(text)\n",
    "        \n",
    "        # TODO: Route to appropriate prompt based on characteristics\n",
    "        selected_prompt_key = 'high_quality'  # Default\n",
    "        \n",
    "        # if characteristics['quality_score'] < 0.3:\n",
    "        #     selected_prompt_key = 'poor_quality'\n",
    "        # elif characteristics['has_tables']:\n",
    "        #     selected_prompt_key = 'complex_table'\n",
    "        # elif characteristics['quality_score'] < 0.7:\n",
    "        #     selected_prompt_key = 'medium_quality'\n",
    "        # else:\n",
    "        #     selected_prompt_key = 'high_quality'\n",
    "        \n",
    "        self.usage_stats[selected_prompt_key] += 1\n",
    "        \n",
    "        print(f\"Selected prompt: {selected_prompt_key} (TODO: implement real selection logic)\")\n",
    "        return self.prompts[selected_prompt_key]\n",
    "    \n",
    "    def get_usage_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Get prompt usage statistics\"\"\"\n",
    "        return dict(self.usage_stats)\n",
    "\n",
    "def enhance_prompt_for_errors(base_prompt: PromptTemplate, \n",
    "                              errors: List[str]) -> PromptTemplate:\n",
    "    \"\"\"\n",
    "    Add error-specific instructions to prompt\n",
    "    TODO: Implement enhancement logic\n",
    "    \"\"\"\n",
    "    enhancement_instructions = []\n",
    "    \n",
    "    # TODO: Add error-specific enhancements\n",
    "    # if \"missing_vendor\" in errors:\n",
    "    #     enhancement_instructions.append(\n",
    "    #         \"IMPORTANT: Look carefully for company name, vendor, or seller information.\"\n",
    "    #     )\n",
    "    # \n",
    "    # if \"invalid_totals\" in errors:\n",
    "    #     enhancement_instructions.append(\n",
    "    #         \"IMPORTANT: Verify all calculations. Check that line totals = quantity √ó unit_price.\"\n",
    "    #     )\n",
    "    # \n",
    "    # if \"date_parsing\" in errors:\n",
    "    #     enhancement_instructions.append(\n",
    "    #         \"IMPORTANT: Convert dates to YYYY-MM-DD format. Look for date patterns like MM/DD/YYYY.\"\n",
    "    #     )\n",
    "    \n",
    "    if enhancement_instructions:\n",
    "        enhanced_template = \"\\n\".join(enhancement_instructions) + \"\\n\\n\" + base_prompt.template\n",
    "        return PromptTemplate(\n",
    "            input_variables=base_prompt.input_variables,\n",
    "            template=enhanced_template\n",
    "        )\n",
    "    \n",
    "    print(\"TODO: Implement error-specific prompt enhancements\")\n",
    "    return base_prompt\n",
    "\n",
    "# Test the prompt selector\n",
    "print(\"üß™ Testing prompt selection system...\")\n",
    "\n",
    "prompt_selector = PromptSelector()\n",
    "\n",
    "# Test with different document types\n",
    "test_documents = [\n",
    "    (\"Simple invoice: ABC Corp, $500 total\", \"simple\"),\n",
    "    (\"Complex table with multiple items and calculations...\", \"complex\"),\n",
    "    (\"Poorly scanned text with missing characters...\", \"poor_quality\")\n",
    "]\n",
    "\n",
    "for doc_text, doc_type in test_documents:\n",
    "    print(f\"\\nüìÑ Testing {doc_type} document:\")\n",
    "    characteristics = prompt_selector.analyze_document(doc_text)\n",
    "    selected_prompt = prompt_selector.select_prompt(doc_text)\n",
    "    print(f\"   Characteristics: {characteristics}\")\n",
    "    print(f\"   Selected prompt type: {type(selected_prompt).__name__}\")\n",
    "\n",
    "# Test error enhancement\n",
    "print(f\"\\nüîß Testing error-based enhancement:\")\n",
    "sample_errors = [\"missing_vendor\", \"invalid_totals\"]\n",
    "enhanced_prompt = enhance_prompt_for_errors(basic_extraction_prompt, sample_errors)\n",
    "print(f\"   Enhanced prompt length: {len(enhanced_prompt.template)} chars\")\n",
    "\n",
    "# Show usage statistics\n",
    "print(f\"\\nüìä Prompt usage statistics:\")\n",
    "for prompt_type, count in prompt_selector.get_usage_stats().items():\n",
    "    print(f\"   {prompt_type}: {count} uses\")\n",
    "\n",
    "print(\"\\nüìù TODO Summary for Task 3:\")\n",
    "print(\"1. Complete analyze_document() with real quality assessment\")\n",
    "print(\"2. Implement smart prompt selection in select_prompt()\")\n",
    "print(\"3. Add error-specific enhancements in enhance_prompt_for_errors()\")\n",
    "print(\"4. Add support for multi-language document detection\")\n",
    "print(\"5. Test with real invoice samples of varying quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task4_title",
   "metadata": {},
   "source": [
    "## Task 4: Add Instructor Integration (10 minutes)\n",
    "\n",
    "### Guaranteed Structured Output with Instructor\n",
    "\n",
    "Use Instructor library to ensure LLM outputs match\n",
    "your Pydantic schemas with automatic retry on failure.\n",
    "\n",
    "**TODO Instructions:**\n",
    "1. Implement the LLM calling logic\n",
    "2. Add JSON parsing and Pydantic validation\n",
    "3. Implement retry logic with prompt enhancement\n",
    "4. Test with sample invoice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4_instructor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str) -> str:\n",
    "    \"\"\"Call the course LLM server\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/think\",\n",
    "            headers=headers,\n",
    "            json=data\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json().get('response', '')\n",
    "        else:\n",
    "            raise Exception(f\"HTTP {response.status_code}: {response.text}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"LLM call failed: {str(e)}\")\n",
    "\n",
    "def extract_with_retry(text: str, max_attempts: int = 3) -> InvoiceExtraction:\n",
    "    \"\"\"\n",
    "    Extract invoice data with validation and retry\n",
    "    TODO: Implement extraction with Instructor\n",
    "    \"\"\"\n",
    "    extraction_errors = []\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        print(f\"\\nüîÑ Extraction attempt {attempt + 1}/{max_attempts}\")\n",
    "        \n",
    "        try:\n",
    "            # Select appropriate prompt\n",
    "            prompt_template = prompt_selector.select_prompt(text)\n",
    "            \n",
    "            # Enhance prompt if we have previous errors\n",
    "            if extraction_errors:\n",
    "                prompt_template = enhance_prompt_for_errors(prompt_template, extraction_errors)\n",
    "            \n",
    "            # Format prompt\n",
    "            if 'output_format' in prompt_template.input_variables:\n",
    "                schema_desc = \"JSON matching InvoiceExtraction schema with all required fields\"\n",
    "                prompt = prompt_template.format(invoice_text=text, output_format=schema_desc)\n",
    "            else:\n",
    "                prompt = prompt_template.format(invoice_text=text)\n",
    "            \n",
    "            print(f\"   üìù Using prompt type: {type(prompt_template).__name__}\")\n",
    "            print(f\"   üìè Prompt length: {len(prompt)} chars\")\n",
    "            \n",
    "            # Call LLM\n",
    "            print(f\"   ü§ñ Calling LLM...\")\n",
    "            raw_response = call_llm(prompt)\n",
    "            print(f\"   üìÑ Response length: {len(raw_response)} chars\")\n",
    "            \n",
    "            # TODO: Parse and validate with Pydantic\n",
    "            # Extract JSON from response\n",
    "            # json_start = raw_response.find('{')\n",
    "            # json_end = raw_response.rfind('}') + 1\n",
    "            # \n",
    "            # if json_start == -1 or json_end == 0:\n",
    "            #     raise ValueError(\"No JSON found in response\")\n",
    "            # \n",
    "            # json_text = raw_response[json_start:json_end]\n",
    "            # raw_data = json.loads(json_text)\n",
    "            # \n",
    "            # # Create InvoiceExtraction instance\n",
    "            # extraction = InvoiceExtraction(**raw_data)\n",
    "            # \n",
    "            # print(f\"   ‚úÖ Extraction successful!\")\n",
    "            # print(f\"      Vendor: {extraction.vendor_name}\")\n",
    "            # print(f\"      Invoice #: {extraction.invoice_number}\")\n",
    "            # print(f\"      Total: {extraction.total_amount} {extraction.currency}\")\n",
    "            # \n",
    "            # return extraction\n",
    "            \n",
    "            # TODO: For now, create mock extraction for testing\n",
    "            mock_extraction = InvoiceExtraction(\n",
    "                invoice_number=\"TEST-001\",\n",
    "                vendor_name=\"Mock Vendor\",\n",
    "                invoice_date=datetime.now(),\n",
    "                line_items=[],\n",
    "                subtotal=100.0,\n",
    "                tax_rate=0.08,\n",
    "                tax_amount=8.0,\n",
    "                total_amount=108.0,\n",
    "                confidence_score=0.85\n",
    "            )\n",
    "            \n",
    "            print(f\"   ‚úÖ Mock extraction created (TODO: implement real parsing)\")\n",
    "            return mock_extraction\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            extraction_errors.append(error_msg)\n",
    "            print(f\"   ‚ùå Attempt {attempt + 1} failed: {error_msg[:100]}...\")\n",
    "            \n",
    "            # TODO: Handle validation errors\n",
    "            # if attempt < max_attempts - 1:\n",
    "            #     # Analyze error type for prompt enhancement\n",
    "            #     if \"vendor_name\" in error_msg:\n",
    "            #         extraction_errors.append(\"missing_vendor\")\n",
    "            #     if \"total\" in error_msg or \"calculation\" in error_msg:\n",
    "            #         extraction_errors.append(\"invalid_totals\")\n",
    "            #     if \"date\" in error_msg:\n",
    "            #         extraction_errors.append(\"date_parsing\")\n",
    "            \n",
    "            if attempt == max_attempts - 1:\n",
    "                print(f\"   üí• All attempts failed. Last error: {error_msg}\")\n",
    "                raise Exception(f\"Extraction failed after {max_attempts} attempts: {error_msg}\")\n",
    "\n",
    "def batch_extract(texts: List[str]) -> List[InvoiceExtraction]:\n",
    "    \"\"\"Extract from multiple invoices\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"\\nüìÑ Processing invoice {i+1}/{len(texts)}\")\n",
    "        try:\n",
    "            extraction = extract_with_retry(text, max_attempts=2)\n",
    "            results.append(extraction)\n",
    "            print(f\"   ‚úÖ Success: {extraction.vendor_name} - {extraction.total_amount}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed: {str(e)[:100]}...\")\n",
    "            results.append(None)\n",
    "    \n",
    "    success_rate = sum(1 for r in results if r is not None) / len(results)\n",
    "    print(f\"\\nüìä Batch processing complete: {success_rate:.1%} success rate\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the extraction system\n",
    "print(\"üß™ Testing extraction with retry system...\")\n",
    "\n",
    "sample_invoices = [\n",
    "    \"Invoice #INV-001 from TechCorp, Date: 2024-01-15, Total: $1500.00\",\n",
    "    \"Complex invoice with multiple line items and tax calculations...\",\n",
    "    \"Poorly formatted invoice text that might cause parsing errors...\"\n",
    "]\n",
    "\n",
    "# Test single extraction\n",
    "print(f\"\\nüîç Testing single extraction:\")\n",
    "try:\n",
    "    result = extract_with_retry(sample_invoices[0])\n",
    "    print(f\"‚úÖ Single extraction successful\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Single extraction failed: {e}\")\n",
    "\n",
    "# Test batch extraction\n",
    "print(f\"\\nüì¶ Testing batch extraction:\")\n",
    "batch_results = batch_extract(sample_invoices[:2])  # Test with first 2\n",
    "\n",
    "print(\"\\nüìù TODO Summary for Task 4:\")\n",
    "print(\"1. Implement JSON parsing from LLM response\")\n",
    "print(\"2. Add Pydantic validation with proper error handling\")\n",
    "print(\"3. Enhance error classification for better prompt adaptation\")\n",
    "print(\"4. Add confidence scoring based on validation success\")\n",
    "print(\"5. Implement fallback strategies for persistent failures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task5_title",
   "metadata": {},
   "source": [
    "## Task 5: Implement Prompt Caching (5 minutes)\n",
    "\n",
    "### Cache Rendered Prompts for Performance\n",
    "\n",
    "Avoid re-rendering identical prompts and cache\n",
    "LLM responses for identical inputs.\n",
    "\n",
    "**TODO Instructions:**\n",
    "1. Implement cache key generation\n",
    "2. Add thread-safe prompt rendering\n",
    "3. Implement response caching with TTL\n",
    "4. Test cache performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task5_caching",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from functools import lru_cache\n",
    "import time\n",
    "\n",
    "class PromptCache:\n",
    "    \"\"\"Thread-safe prompt and response caching\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100):\n",
    "        self.max_size = max_size\n",
    "        self._response_cache = {}\n",
    "        self._cache_times = {}\n",
    "        self._lock = threading.RLock()\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.ttl_seconds = 3600  # 1 hour TTL\n",
    "        \n",
    "    def _hash_inputs(self, template: str, **kwargs) -> str:\n",
    "        \"\"\"Create cache key from inputs\"\"\"\n",
    "        # TODO: Implement hashing\n",
    "        # content = f\"{template}{sorted(kwargs.items())}\"\n",
    "        # return hashlib.md5(content.encode()).hexdigest()\n",
    "        \n",
    "        # Simple implementation for now\n",
    "        content = f\"{template}{sorted(kwargs.items())}\"\n",
    "        return str(hash(content))\n",
    "    \n",
    "    @lru_cache(maxsize=100)\n",
    "    def render_prompt(self, template_str: str, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Cache rendered prompts\n",
    "        TODO: Thread-safe implementation\n",
    "        \"\"\"\n",
    "        # TODO: Create PromptTemplate and render\n",
    "        # template = PromptTemplate.from_template(template_str)\n",
    "        # return template.format(**kwargs)\n",
    "        \n",
    "        # Simple string formatting for now\n",
    "        try:\n",
    "            return template_str.format(**kwargs)\n",
    "        except KeyError as e:\n",
    "            print(f\"TODO: Fix template rendering - missing variable {e}\")\n",
    "            return template_str\n",
    "    \n",
    "    def get_cached_response(self, prompt_hash: str) -> Optional[str]:\n",
    "        \"\"\"Retrieve cached LLM response if available\"\"\"\n",
    "        with self._lock:\n",
    "            # TODO: Implement cache retrieval with TTL check\n",
    "            if prompt_hash in self._response_cache:\n",
    "                cache_time = self._cache_times.get(prompt_hash, 0)\n",
    "                if time.time() - cache_time < self.ttl_seconds:\n",
    "                    self.hits += 1\n",
    "                    return self._response_cache[prompt_hash]\n",
    "                else:\n",
    "                    # Expired cache entry\n",
    "                    del self._response_cache[prompt_hash]\n",
    "                    del self._cache_times[prompt_hash]\n",
    "            \n",
    "            self.misses += 1\n",
    "            return None\n",
    "    \n",
    "    def cache_response(self, prompt_hash: str, response: str):\n",
    "        \"\"\"Store LLM response in cache\"\"\"\n",
    "        with self._lock:\n",
    "            # TODO: Implement cache storage with TTL\n",
    "            # Check cache size and evict if necessary\n",
    "            if len(self._response_cache) >= self.max_size:\n",
    "                # Remove oldest entry\n",
    "                oldest_key = min(self._cache_times.keys(), \n",
    "                               key=lambda k: self._cache_times[k])\n",
    "                del self._response_cache[oldest_key]\n",
    "                del self._cache_times[oldest_key]\n",
    "            \n",
    "            self._response_cache[prompt_hash] = response\n",
    "            self._cache_times[prompt_hash] = time.time()\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        with self._lock:\n",
    "            total_requests = self.hits + self.misses\n",
    "            hit_rate = self.hits / total_requests if total_requests > 0 else 0\n",
    "            \n",
    "            return {\n",
    "                'hits': self.hits,\n",
    "                'misses': self.misses,\n",
    "                'hit_rate': hit_rate,\n",
    "                'cache_size': len(self._response_cache),\n",
    "                'max_size': self.max_size\n",
    "            }\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear all cached data\"\"\"\n",
    "        with self._lock:\n",
    "            self._response_cache.clear()\n",
    "            self._cache_times.clear()\n",
    "            self.hits = 0\n",
    "            self.misses = 0\n",
    "\n",
    "# Enhanced extraction function with caching\n",
    "def extract_with_caching(text: str, cache: PromptCache) -> InvoiceExtraction:\n",
    "    \"\"\"Extract with prompt and response caching\"\"\"\n",
    "    \n",
    "    # Generate cache key\n",
    "    prompt_template = prompt_selector.select_prompt(text)\n",
    "    cache_key = cache._hash_inputs(prompt_template.template, invoice_text=text)\n",
    "    \n",
    "    print(f\"üîç Checking cache for key: {cache_key[:16]}...\")\n",
    "    \n",
    "    # Check for cached response\n",
    "    cached_response = cache.get_cached_response(cache_key)\n",
    "    \n",
    "    if cached_response:\n",
    "        print(f\"   üíæ Cache hit! Using cached response\")\n",
    "        # TODO: Parse cached response to InvoiceExtraction\n",
    "        # For now, return mock data\n",
    "        return InvoiceExtraction(\n",
    "            invoice_number=\"CACHED-001\",\n",
    "            vendor_name=\"Cached Vendor\",\n",
    "            invoice_date=datetime.now(),\n",
    "            line_items=[],\n",
    "            subtotal=100.0,\n",
    "            tax_rate=0.08,\n",
    "            tax_amount=8.0,\n",
    "            total_amount=108.0,\n",
    "            confidence_score=0.90\n",
    "        )\n",
    "    else:\n",
    "        print(f\"   üö´ Cache miss - calling LLM\")\n",
    "        \n",
    "        # Render prompt (with caching)\n",
    "        prompt = cache.render_prompt(prompt_template.template, invoice_text=text)\n",
    "        \n",
    "        # Call LLM\n",
    "        response = call_llm(prompt)\n",
    "        \n",
    "        # Cache the response\n",
    "        cache.cache_response(cache_key, response)\n",
    "        \n",
    "        # TODO: Parse response to InvoiceExtraction\n",
    "        return InvoiceExtraction(\n",
    "            invoice_number=\"FRESH-001\",\n",
    "            vendor_name=\"Fresh Vendor\",\n",
    "            invoice_date=datetime.now(),\n",
    "            line_items=[],\n",
    "            subtotal=200.0,\n",
    "            tax_rate=0.08,\n",
    "            tax_amount=16.0,\n",
    "            total_amount=216.0,\n",
    "            confidence_score=0.85\n",
    "        )\n",
    "\n",
    "# Test caching system\n",
    "print(\"üß™ Testing prompt caching system...\")\n",
    "\n",
    "cache = PromptCache(max_size=50)\n",
    "\n",
    "test_texts = [\n",
    "    \"Invoice #TEST-001 from Company A, Total: $500\",\n",
    "    \"Invoice #TEST-002 from Company B, Total: $750\",\n",
    "    \"Invoice #TEST-001 from Company A, Total: $500\",  # Duplicate for cache test\n",
    "]\n",
    "\n",
    "print(f\"\\nüìÑ Processing {len(test_texts)} invoices with caching:\")\n",
    "\n",
    "for i, text in enumerate(test_texts):\n",
    "    print(f\"\\n   Invoice {i+1}: {text[:30]}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = extract_with_caching(text, cache)\n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"      ‚úÖ Processed in {processing_time:.3f}s - {result.vendor_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Failed: {str(e)[:50]}...\")\n",
    "\n",
    "# Show cache statistics\n",
    "stats = cache.get_stats()\n",
    "print(f\"\\nüìä Cache Performance:\")\n",
    "print(f\"   Cache hits: {stats['hits']}\")\n",
    "print(f\"   Cache misses: {stats['misses']}\")\n",
    "print(f\"   Hit rate: {stats['hit_rate']:.1%}\")\n",
    "print(f\"   Cache utilization: {stats['cache_size']}/{stats['max_size']}\")\n",
    "\n",
    "if stats['hit_rate'] > 0:\n",
    "    print(f\"   ‚úÖ Cache is working - {stats['hit_rate']:.1%} of requests served from cache\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è No cache hits yet - need more duplicate requests to see benefits\")\n",
    "\n",
    "print(\"\\nüìù TODO Summary for Task 5:\")\n",
    "print(\"1. Implement proper JSON response parsing for cached responses\")\n",
    "print(\"2. Add cache warming strategies for common invoice types\")\n",
    "print(\"3. Implement cache persistence across sessions\")\n",
    "print(\"4. Add cache invalidation strategies\")\n",
    "print(\"5. Optimize cache key generation for better hit rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task6_title",
   "metadata": {},
   "source": [
    "## Task 6: Build Testing Framework (5 minutes)\n",
    "\n",
    "### Test and Track Prompt Performance\n",
    "\n",
    "Create framework to measure prompt effectiveness\n",
    "and track metrics over time.\n",
    "\n",
    "**TODO Instructions:**\n",
    "1. Implement test case storage and management\n",
    "2. Create prompt testing logic with accuracy metrics\n",
    "3. Build A/B testing comparison framework\n",
    "4. Add performance tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task6_testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTester:\n",
    "    \"\"\"Test prompts against known invoice samples\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.test_cases = []\n",
    "        self.metrics = {\n",
    "            'accuracy': [],\n",
    "            'token_usage': [],\n",
    "            'extraction_time': [],\n",
    "            'retry_count': []\n",
    "        }\n",
    "    \n",
    "    def add_test_case(self, invoice_text: str, expected: InvoiceExtraction):\n",
    "        \"\"\"Add test case with ground truth\"\"\"\n",
    "        # TODO: Store test case\n",
    "        test_case = {\n",
    "            'id': len(self.test_cases),\n",
    "            'text': invoice_text,\n",
    "            'expected': expected,\n",
    "            'created_at': datetime.now()\n",
    "        }\n",
    "        self.test_cases.append(test_case)\n",
    "        print(f\"‚úÖ Added test case {test_case['id']}: {expected.vendor_name}\")\n",
    "    \n",
    "    def calculate_accuracy(self, extracted: InvoiceExtraction, \n",
    "                          expected: InvoiceExtraction) -> float:\n",
    "        \"\"\"Calculate accuracy score between extracted and expected data\"\"\"\n",
    "        # TODO: Implement sophisticated accuracy calculation\n",
    "        scores = []\n",
    "        \n",
    "        # Check key fields\n",
    "        # if extracted.invoice_number == expected.invoice_number:\n",
    "        #     scores.append(1.0)\n",
    "        # else:\n",
    "        #     scores.append(0.0)\n",
    "        # \n",
    "        # if extracted.vendor_name == expected.vendor_name:\n",
    "        #     scores.append(1.0)\n",
    "        # else:\n",
    "        #     scores.append(0.0)\n",
    "        # \n",
    "        # # Check total amount (allow small differences)\n",
    "        # if abs(extracted.total_amount - expected.total_amount) < 0.01:\n",
    "        #     scores.append(1.0)\n",
    "        # else:\n",
    "        #     scores.append(0.0)\n",
    "        # \n",
    "        # return sum(scores) / len(scores) if scores else 0.0\n",
    "        \n",
    "        # Mock accuracy calculation for testing\n",
    "        import random\n",
    "        return random.uniform(0.7, 0.95)\n",
    "    \n",
    "    def test_prompt(self, prompt: PromptTemplate) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Test prompt against all test cases\n",
    "        TODO: Implement testing logic\n",
    "        - Run extraction\n",
    "        - Compare with expected\n",
    "        - Calculate accuracy metrics\n",
    "        - Track token usage\n",
    "        \"\"\"\n",
    "        if not self.test_cases:\n",
    "            print(\"‚ö†Ô∏è No test cases available. Add test cases first.\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"üß™ Testing prompt against {len(self.test_cases)} test cases...\")\n",
    "        \n",
    "        results = {\n",
    "            'prompt_type': type(prompt).__name__,\n",
    "            'test_cases_count': len(self.test_cases),\n",
    "            'accuracy_scores': [],\n",
    "            'token_usage': [],\n",
    "            'processing_times': [],\n",
    "            'success_count': 0\n",
    "        }\n",
    "        \n",
    "        for i, test_case in enumerate(self.test_cases):\n",
    "            print(f\"   Test {i+1}/{len(self.test_cases)}: \", end=\"\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # TODO: Run actual extraction\n",
    "                # extracted = extract_with_retry(test_case['text'])\n",
    "                # accuracy = self.calculate_accuracy(extracted, test_case['expected'])\n",
    "                \n",
    "                # Mock extraction for testing\n",
    "                extracted = InvoiceExtraction(\n",
    "                    invoice_number=\"TEST-001\",\n",
    "                    vendor_name=\"Test Vendor\",\n",
    "                    invoice_date=datetime.now(),\n",
    "                    line_items=[],\n",
    "                    subtotal=100.0,\n",
    "                    tax_rate=0.08,\n",
    "                    tax_amount=8.0,\n",
    "                    total_amount=108.0,\n",
    "                    confidence_score=0.85\n",
    "                )\n",
    "                \n",
    "                accuracy = self.calculate_accuracy(extracted, test_case['expected'])\n",
    "                processing_time = time.time() - start_time\n",
    "                \n",
    "                results['accuracy_scores'].append(accuracy)\n",
    "                results['token_usage'].append(300)  # Mock token count\n",
    "                results['processing_times'].append(processing_time)\n",
    "                results['success_count'] += 1\n",
    "                \n",
    "                print(f\"‚úÖ {accuracy:.1%}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed\")\n",
    "                results['accuracy_scores'].append(0.0)\n",
    "                results['token_usage'].append(200)  # Mock token count for failed attempts\n",
    "                results['processing_times'].append(time.time() - start_time)\n",
    "        \n",
    "        # Calculate summary metrics\n",
    "        results['avg_accuracy'] = sum(results['accuracy_scores']) / len(results['accuracy_scores'])\n",
    "        results['avg_tokens'] = sum(results['token_usage']) / len(results['token_usage'])\n",
    "        results['avg_time'] = sum(results['processing_times']) / len(results['processing_times'])\n",
    "        results['success_rate'] = results['success_count'] / len(self.test_cases)\n",
    "        \n",
    "        print(f\"\\n   üìä Results: {results['avg_accuracy']:.1%} accuracy, {results['avg_tokens']:.0f} tokens avg\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compare_prompts(self, prompts: List[PromptTemplate]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        A/B test multiple prompts\n",
    "        TODO: Create comparison matrix\n",
    "        \"\"\"\n",
    "        print(f\"üèÅ A/B testing {len(prompts)} prompt variants...\")\n",
    "        \n",
    "        results = []\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            print(f\"\\nüîç Testing prompt variant {i+1}/{len(prompts)}\")\n",
    "            metrics = self.test_prompt(prompt)\n",
    "            \n",
    "            if metrics:  # Only add if we got results\n",
    "                results.append({\n",
    "                    'Prompt': f\"Variant_{i+1}\",\n",
    "                    'Accuracy': f\"{metrics['avg_accuracy']:.1%}\",\n",
    "                    'Success_Rate': f\"{metrics['success_rate']:.1%}\",\n",
    "                    'Avg_Tokens': f\"{metrics['avg_tokens']:.0f}\",\n",
    "                    'Avg_Time': f\"{metrics['avg_time']:.2f}s\",\n",
    "                    'Test_Cases': metrics['test_cases_count']\n",
    "                })\n",
    "        \n",
    "        if results:\n",
    "            df = pd.DataFrame(results)\n",
    "            print(f\"\\nüìà A/B Test Comparison:\")\n",
    "            print(df.to_string(index=False))\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"‚ùå No valid results to compare\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def add_sample_test_cases(self):\n",
    "        \"\"\"Add some sample test cases for demonstration\"\"\"\n",
    "        sample_cases = [\n",
    "            {\n",
    "                'text': \"Invoice #INV-001 from TechCorp, Date: 2024-01-15, Total: $1500.00\",\n",
    "                'expected': InvoiceExtraction(\n",
    "                    invoice_number=\"INV-001\",\n",
    "                    vendor_name=\"TechCorp\",\n",
    "                    invoice_date=datetime(2024, 1, 15),\n",
    "                    line_items=[],\n",
    "                    subtotal=1388.89,\n",
    "                    tax_rate=0.08,\n",
    "                    tax_amount=111.11,\n",
    "                    total_amount=1500.00,\n",
    "                    confidence_score=0.95\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                'text': \"Invoice ABC-123 from Supplies Inc, Total: $750\",\n",
    "                'expected': InvoiceExtraction(\n",
    "                    invoice_number=\"ABC-123\",\n",
    "                    vendor_name=\"Supplies Inc\",\n",
    "                    invoice_date=datetime.now(),\n",
    "                    line_items=[],\n",
    "                    subtotal=694.44,\n",
    "                    tax_rate=0.08,\n",
    "                    tax_amount=55.56,\n",
    "                    total_amount=750.00,\n",
    "                    confidence_score=0.90\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for case in sample_cases:\n",
    "            self.add_test_case(case['text'], case['expected'])\n",
    "\n",
    "# Test the testing framework\n",
    "print(\"üß™ Testing the prompt testing framework...\")\n",
    "\n",
    "tester = PromptTester()\n",
    "\n",
    "# Add sample test cases\n",
    "print(f\"\\nüìã Adding sample test cases:\")\n",
    "tester.add_sample_test_cases()\n",
    "\n",
    "# Test individual prompts\n",
    "print(f\"\\nüîç Testing individual prompts:\")\n",
    "basic_results = tester.test_prompt(basic_extraction_prompt)\n",
    "\n",
    "# Compare multiple prompts\n",
    "print(f\"\\nüèÅ Comparing multiple prompts:\")\n",
    "prompts_to_test = [\n",
    "    basic_extraction_prompt,\n",
    "    few_shot_prompt,\n",
    "    cot_extraction_prompt\n",
    "]\n",
    "\n",
    "comparison_df = tester.compare_prompts(prompts_to_test)\n",
    "\n",
    "if not comparison_df.empty:\n",
    "    print(f\"\\nüèÜ Best performing prompt characteristics:\")\n",
    "    # Find best accuracy\n",
    "    best_accuracy_idx = comparison_df['Accuracy'].str.rstrip('%').astype(float).idxmax()\n",
    "    best_prompt = comparison_df.loc[best_accuracy_idx]\n",
    "    print(f\"   Highest accuracy: {best_prompt['Prompt']} ({best_prompt['Accuracy']})\")\n",
    "\n",
    "print(\"\\nüìù TODO Summary for Task 6:\")\n",
    "print(\"1. Implement real accuracy calculation comparing extracted vs expected\")\n",
    "print(\"2. Add more sophisticated metrics (F1 score, field-level accuracy)\")\n",
    "print(\"3. Create automated test case generation from real invoices\")\n",
    "print(\"4. Add regression testing for prompt changes\")\n",
    "print(\"5. Implement continuous performance monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task7_title",
   "metadata": {},
   "source": [
    "## Task 7: Add Versioning System (4 minutes)\n",
    "\n",
    "### Version Control for Prompts\n",
    "\n",
    "Track prompt evolution and enable rollback.\n",
    "\n",
    "**TODO Instructions:**\n",
    "1. Implement prompt version storage and metadata\n",
    "2. Create prompt registry with active version management\n",
    "3. Add rollback capabilities\n",
    "4. Test versioning workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task7_versioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptVersion:\n",
    "    \"\"\"Versioned prompt with metadata\"\"\"\n",
    "    \n",
    "    def __init__(self, template: PromptTemplate, version: str, metadata: Dict[str, Any]):\n",
    "        self.template = template\n",
    "        self.version = version\n",
    "        self.created_at = datetime.now()\n",
    "        self.metadata = metadata\n",
    "        self.performance_stats = {\n",
    "            'success_rate': 0.0,\n",
    "            'avg_tokens': 0,\n",
    "            'avg_accuracy': 0.0,\n",
    "            'usage_count': 0\n",
    "        }\n",
    "    \n",
    "    def update_stats(self, success_rate: float, avg_tokens: int, accuracy: float = 0.0):\n",
    "        \"\"\"Track performance over time\"\"\"\n",
    "        # TODO: Update statistics with weighted averaging\n",
    "        self.performance_stats['usage_count'] += 1\n",
    "        \n",
    "        # Simple moving average for now\n",
    "        count = self.performance_stats['usage_count']\n",
    "        \n",
    "        self.performance_stats['success_rate'] = (\n",
    "            (self.performance_stats['success_rate'] * (count - 1) + success_rate) / count\n",
    "        )\n",
    "        \n",
    "        self.performance_stats['avg_tokens'] = int(\n",
    "            (self.performance_stats['avg_tokens'] * (count - 1) + avg_tokens) / count\n",
    "        )\n",
    "        \n",
    "        if accuracy > 0:\n",
    "            self.performance_stats['avg_accuracy'] = (\n",
    "                (self.performance_stats['avg_accuracy'] * (count - 1) + accuracy) / count\n",
    "            )\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get version summary\"\"\"\n",
    "        return {\n",
    "            'version': self.version,\n",
    "            'created_at': self.created_at.isoformat(),\n",
    "            'metadata': self.metadata,\n",
    "            'performance': self.performance_stats,\n",
    "            'template_length': len(self.template.template)\n",
    "        }\n",
    "\n",
    "class PromptRegistry:\n",
    "    \"\"\"Manage prompt versions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.versions = {}  # name -> {version -> PromptVersion}\n",
    "        self.active_versions = {}  # name -> version\n",
    "    \n",
    "    def register(self, name: str, prompt: PromptTemplate, \n",
    "                version: str = \"1.0.0\", metadata: Dict[str, Any] = None):\n",
    "        \"\"\"Register new prompt version\"\"\"\n",
    "        # TODO: Store versioned prompt\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        \n",
    "        # Initialize name if not exists\n",
    "        if name not in self.versions:\n",
    "            self.versions[name] = {}\n",
    "        \n",
    "        # Create version\n",
    "        prompt_version = PromptVersion(prompt, version, metadata)\n",
    "        self.versions[name][version] = prompt_version\n",
    "        \n",
    "        # Set as active if first version or explicitly requested\n",
    "        if name not in self.active_versions or metadata.get('set_active', False):\n",
    "            self.active_versions[name] = version\n",
    "        \n",
    "        print(f\"‚úÖ Registered {name} v{version} (active: {self.active_versions[name]})\")\n",
    "    \n",
    "    def get_active(self, name: str) -> Optional[PromptTemplate]:\n",
    "        \"\"\"Get current active version\"\"\"\n",
    "        # TODO: Return active prompt\n",
    "        if name in self.active_versions:\n",
    "            active_version = self.active_versions[name]\n",
    "            if name in self.versions and active_version in self.versions[name]:\n",
    "                return self.versions[name][active_version].template\n",
    "        \n",
    "        print(f\"‚ö†Ô∏è No active version found for {name}\")\n",
    "        return None\n",
    "    \n",
    "    def get_version(self, name: str, version: str) -> Optional[PromptTemplate]:\n",
    "        \"\"\"Get specific version\"\"\"\n",
    "        if name in self.versions and version in self.versions[name]:\n",
    "            return self.versions[name][version].template\n",
    "        return None\n",
    "    \n",
    "    def rollback(self, name: str, version: str):\n",
    "        \"\"\"Rollback to previous version\"\"\"\n",
    "        # TODO: Implement rollback\n",
    "        if name in self.versions and version in self.versions[name]:\n",
    "            old_version = self.active_versions.get(name, 'none')\n",
    "            self.active_versions[name] = version\n",
    "            print(f\"üîÑ Rolled back {name} from v{old_version} to v{version}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Cannot rollback {name} to v{version} - version not found\")\n",
    "            return False\n",
    "    \n",
    "    def list_versions(self, name: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"List all versions for a prompt\"\"\"\n",
    "        if name not in self.versions:\n",
    "            return []\n",
    "        \n",
    "        versions_info = []\n",
    "        for version, prompt_version in self.versions[name].items():\n",
    "            info = prompt_version.get_summary()\n",
    "            info['is_active'] = (version == self.active_versions.get(name))\n",
    "            versions_info.append(info)\n",
    "        \n",
    "        # Sort by creation time\n",
    "        versions_info.sort(key=lambda x: x['created_at'], reverse=True)\n",
    "        return versions_info\n",
    "    \n",
    "    def update_performance(self, name: str, version: str, \n",
    "                          success_rate: float, avg_tokens: int, accuracy: float = 0.0):\n",
    "        \"\"\"Update performance stats for a version\"\"\"\n",
    "        if name in self.versions and version in self.versions[name]:\n",
    "            self.versions[name][version].update_stats(success_rate, avg_tokens, accuracy)\n",
    "    \n",
    "    def get_best_performing_version(self, name: str, metric: str = 'avg_accuracy') -> Optional[str]:\n",
    "        \"\"\"Find best performing version by metric\"\"\"\n",
    "        if name not in self.versions:\n",
    "            return None\n",
    "        \n",
    "        best_version = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for version, prompt_version in self.versions[name].items():\n",
    "            if prompt_version.performance_stats['usage_count'] > 0:\n",
    "                score = prompt_version.performance_stats.get(metric, 0)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_version = version\n",
    "        \n",
    "        return best_version\n",
    "\n",
    "# Test the versioning system\n",
    "print(\"üß™ Testing prompt versioning system...\")\n",
    "\n",
    "registry = PromptRegistry()\n",
    "\n",
    "# Register initial versions\n",
    "print(f\"\\nüìù Registering prompt versions:\")\n",
    "\n",
    "registry.register(\n",
    "    name=\"invoice_extraction\",\n",
    "    prompt=basic_extraction_prompt,\n",
    "    version=\"1.0.0\",\n",
    "    metadata={\"description\": \"Basic extraction prompt\", \"author\": \"system\"}\n",
    ")\n",
    "\n",
    "registry.register(\n",
    "    name=\"invoice_extraction\",\n",
    "    prompt=few_shot_prompt,\n",
    "    version=\"1.1.0\",\n",
    "    metadata={\"description\": \"Added few-shot examples\", \"author\": \"developer\"}\n",
    ")\n",
    "\n",
    "registry.register(\n",
    "    name=\"invoice_extraction\",\n",
    "    prompt=cot_extraction_prompt,\n",
    "    version=\"2.0.0\",\n",
    "    metadata={\"description\": \"Chain of thought reasoning\", \"author\": \"researcher\", \"set_active\": True}\n",
    ")\n",
    "\n",
    "# Test getting active version\n",
    "print(f\"\\nüéØ Testing active version retrieval:\")\n",
    "active_prompt = registry.get_active(\"invoice_extraction\")\n",
    "if active_prompt:\n",
    "    print(f\"‚úÖ Got active prompt ({len(active_prompt.template)} chars)\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed to get active prompt\")\n",
    "\n",
    "# Simulate performance updates\n",
    "print(f\"\\nüìä Simulating performance data:\")\n",
    "registry.update_performance(\"invoice_extraction\", \"1.0.0\", 0.75, 200, 0.80)\n",
    "registry.update_performance(\"invoice_extraction\", \"1.1.0\", 0.85, 250, 0.88)\n",
    "registry.update_performance(\"invoice_extraction\", \"2.0.0\", 0.80, 300, 0.85)\n",
    "\n",
    "# List all versions\n",
    "print(f\"\\nüìã All versions of 'invoice_extraction':\")\n",
    "versions = registry.list_versions(\"invoice_extraction\")\n",
    "for version_info in versions:\n",
    "    active_indicator = \"üü¢\" if version_info['is_active'] else \"‚ö™\"\n",
    "    print(f\"   {active_indicator} v{version_info['version']} - {version_info['metadata']['description']}\")\n",
    "    print(f\"      Performance: {version_info['performance']['avg_accuracy']:.1%} accuracy, {version_info['performance']['avg_tokens']} tokens\")\n",
    "\n",
    "# Find best performing version\n",
    "print(f\"\\nüèÜ Finding best performing version:\")\n",
    "best_version = registry.get_best_performing_version(\"invoice_extraction\", \"avg_accuracy\")\n",
    "if best_version:\n",
    "    print(f\"   Best performing: v{best_version}\")\n",
    "    \n",
    "    # Rollback to best version if it's not active\n",
    "    current_active = registry.active_versions[\"invoice_extraction\"]\n",
    "    if best_version != current_active:\n",
    "        print(f\"   üîÑ Rolling back to best performing version...\")\n",
    "        registry.rollback(\"invoice_extraction\", best_version)\n",
    "\n",
    "# Test rollback functionality\n",
    "print(f\"\\nüîÑ Testing rollback functionality:\")\n",
    "success = registry.rollback(\"invoice_extraction\", \"1.0.0\")\n",
    "if success:\n",
    "    current_active = registry.get_active(\"invoice_extraction\")\n",
    "    print(f\"   ‚úÖ Successfully rolled back to v1.0.0\")\n",
    "    # Roll back to the best version\n",
    "    registry.rollback(\"invoice_extraction\", best_version)\n",
    "\n",
    "print(\"\\nüìù TODO Summary for Task 7:\")\n",
    "print(\"1. Add automatic version numbering (semantic versioning)\")\n",
    "print(\"2. Implement version diffing to see changes between versions\")\n",
    "print(\"3. Add version tags and branch management\")\n",
    "print(\"4. Implement automated performance-based version promotion\")\n",
    "print(\"5. Add version backup and restore from external storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lab_summary",
   "metadata": {},
   "source": [
    "## Lab Summary and Assessment\n",
    "\n",
    "### What You've Built\n",
    "\n",
    "Congratulations! You've created a comprehensive prompt management system with:\n",
    "\n",
    "1. **Structured Output Models** - Pydantic schemas with validation\n",
    "2. **Template System** - Reusable prompts with variables and examples\n",
    "3. **Conditional Logic** - Smart prompt selection based on document characteristics\n",
    "4. **Validation & Retry** - Instructor integration with automatic retries\n",
    "5. **Performance Caching** - Thread-safe caching for efficiency\n",
    "6. **Testing Framework** - A/B testing and performance metrics\n",
    "7. **Version Control** - Prompt versioning with rollback capabilities\n",
    "\n",
    "### Assessment Criteria\n",
    "\n",
    "**Students succeed if they:**\n",
    "- ‚úÖ Create working Pydantic models with validation\n",
    "- ‚úÖ Implement multiple prompt templates with variables\n",
    "- ‚úÖ Build conditional prompt selection system\n",
    "- ‚úÖ Integrate Instructor for structured outputs\n",
    "- ‚úÖ Add functional caching mechanism\n",
    "- ‚úÖ Create testing framework with metrics\n",
    "\n",
    "### Self-Assessment Questions\n",
    "\n",
    "1. **Model Validation**: Do your Pydantic models catch invalid data effectively?\n",
    "2. **Template Flexibility**: Can you easily add new prompt variants?\n",
    "3. **Smart Selection**: Does the system choose appropriate prompts for different documents?\n",
    "4. **Performance**: Is caching improving response times?\n",
    "5. **Quality Assurance**: Can you measure and compare prompt effectiveness?\n",
    "6. **Maintainability**: Can you safely deploy new prompt versions?\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "**Issue: Pydantic validation too strict**\n",
    "- Solution: Add Optional fields and reasonable defaults\n",
    "\n",
    "**Issue: Prompt templates not rendering**\n",
    "- Solution: Check variable names match exactly\n",
    "\n",
    "**Issue: Cache growing too large**\n",
    "- Solution: Implement LRU eviction or TTL\n",
    "\n",
    "**Issue: Instructor retry loops forever**\n",
    "- Solution: Set max_attempts and fallback logic\n",
    "\n",
    "### Key Learning Points\n",
    "\n",
    "- **Structured outputs prevent downstream errors**\n",
    "- **Templates make prompts reusable and testable**\n",
    "- **Caching dramatically improves performance**\n",
    "- **Validation should happen at extraction time**\n",
    "- **Versioning enables safe experimentation**\n",
    "- **Testing reveals which prompts work best**\n",
    "\n",
    "### Production Deployment Checklist\n",
    "\n",
    "Before deploying this system:\n",
    "\n",
    "1. **Complete TODOs**: Implement all the placeholder logic\n",
    "2. **Add Monitoring**: Track prompt performance in production\n",
    "3. **Error Handling**: Add comprehensive error handling and logging\n",
    "4. **Security**: Validate all inputs and sanitize outputs\n",
    "5. **Scaling**: Test with concurrent users and large document volumes\n",
    "6. **Backup**: Implement prompt version backup and recovery\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To advance this system:\n",
    "\n",
    "1. **AI-Powered Enhancement**: Use ML to automatically improve prompts based on failures\n",
    "2. **Multi-Modal Support**: Add support for image and table extraction\n",
    "3. **Domain Adaptation**: Create industry-specific prompt libraries\n",
    "4. **Real-Time Learning**: Implement online learning from user corrections\n",
    "5. **Integration**: Connect with document management and ERP systems\n",
    "\n",
    "**Excellent work!** You now have a production-ready prompt management system that will make your invoice processing reliable, maintainable, and continuously improving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}