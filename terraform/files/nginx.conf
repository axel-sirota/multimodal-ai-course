server {
    listen 80;
    server_name _;
    client_max_body_size 0;
    proxy_read_timeout 600s;
    proxy_connect_timeout 600s;
    proxy_send_timeout 600s;

    # Custom endpoints (Python app on 8080)
    location ~ ^/(health|startup|pull_model|serve_model|metrics|think)$ {
        proxy_pass http://localhost:8080;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header Authorization $http_authorization;
        proxy_buffering off;
    }

    # Ollama native endpoints - proxy to Ollama on 11434
    location / {
        proxy_pass http://localhost:11434;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_buffering off;
    }
}